# Awesome-Reasearch-Work

---
Awesome-Reasearch-Work

(Sort A-Z)

---

# Table of Contents

  <ol>
    <li><a href=#MultiModal-Models>MultiModal-Models</a></li>
    <li><a href=#Language-Models>Language-Models</a></li>
    <li><a href=#Vision-Models>Vision-Models</a></li>
    	<ol>
            <li><a href=#Detection>Detection</a></li>
            <li><a href=#Open-Vocabulary-Object-Detection>Open-Vocabulary-Object-Detection</a></li>
        	  <ol>
              <li><a href=#on-MSCOCO-Benchmark>on-MSCOCO-Benchmark</a></li>
              <li><a href=#on-LVIS-v1.0-Benchmark>on-LVIS-v1.0-Benchmark</a></li>
              <li><a href=#on-OpenImages-v4-Benchmark>on-OpenImages-v4-Benchmark</a></li>
              <li><a href=#on-Objects365-Benchmark>on-Objects365-Benchmark</a></li>
            </ol>
            <li><a href=#Open-Vocabulary-Segmentation> Open-Vocabulary-Segmentation</a></li>
      </ol>
    <li><a href=#Audio-Models>Audio-Models</a></li>
    <li><a href=#Video-Models>Video-Models</a></li>
    <li><a href=#VLM-Pre-training-Methods>VLM-Pre-training-Methods</a></li>
    <li><a href=#VLM-Transfer-Learning-Methods>VLM-Transfer-Learning-Methods</a></li>
    <li><a href=#VLM-Knowledge-Distillation-for-Detection>VLM-Knowledge-Distillation-for-Detection</a></li>
    <li><a href=#VLM-Knowledge-Distillation-for-Segmentation>VLM-Knowledge-Distillation-for-Segmentation</a></li>
    <li><a href=#VLM-Knowledge-Distillation-for-Other-Vision-Tasks>VLM-Knowledge-Distillation-for-Other-Vision-Tasks</a></li>
    <li><a href=#Prompt-Learning>Prompt-Learning</a></li>
  </ol>

---

# MultiModal-Models

| Model / Methods   | Title                                                        | Paper Link                                                   | Code Link                                                    | Published | Keywords         | Venue                                            |
| ----------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | --------- | ---------------- | ------------------------------------------------ |
| ALIGN             | Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2102.05918) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)]() | 2021      |                  |                                                  |
| AltCLIP           | AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2211.06679v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)]() | 2022      |                  |                                                  |
| BLIP              | BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2201.12086) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/salesforce/BLIP)![Star](https://img.shields.io/github/stars/salesforce/BLIP.svg?style=social&label=Star) | 2022      | salesforce       |                                                  |
| BLIP-2            | BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2301.12597) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/salesforce/BLIP)![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star) | 2023      | salesforce       |                                                  |
| BridgeTower       | BridgeTower: Building Bridges Between Encoders in Vision-Language Representative Learning | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2206.08657) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/BridgeTower)![Star](https://img.shields.io/github/stars/microsoft/BridgeTower.svg?style=social&label=Star) | 2023      | microsoft        | [AAAI’23](https://aaai.org/Conferences/AAAI-23/) |
| BROS              | BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2108.04539) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/BridgeTower)![Star](https://img.shields.io/github/stars/clovaai/bros.svg?style=social&label=Star) | 2021      | clovaai          |                                                  |
| Chameleon         | Chameleon: Mixed-Modal Early-Fusion Foundation Models        | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2405.09818v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/chameleon)![Star](https://img.shields.io/github/stars/facebookresearch/chameleon.svg?style=social&label=Star) | 2024      | facebookresearch |                                                  |
| Chinese-CLIP      | Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2211.01335) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://huggingface.co/OFA-Sys) | 2023      | OFA-Sys          |                                                  |
| CLIP              | Learning Transferable Visual Models From Natural Language Supervision | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2103.00020) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/openai/CLIP)![Star](https://img.shields.io/github/stars/openai/CLIP.svg?style=social&label=Star) | 2021      | openai           |                                                  |
| CLIPSeg           | Image Segmentation Using Text and Image Prompts              | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2112.10003) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/timojl/clipseg)![Star](https://img.shields.io/github/stars/timojl/clipseg.svg?style=social&label=Star) | 2021      |                  | CVPR 2022                                        |
| CLVP              | Better speech synthesis through scaling                      | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2305.07243) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/neonbjb/tortoise-tts)![Star](https://img.shields.io/github/stars/neonbjb/tortoise-tts.svg?style=social&label=Star) | 2023      |                  |                                                  |
| Data2Vec          | data 2 vec：A General Framework for Self-supervised Learning in Speech，Vision and Language | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/pdf/2202.03555) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/data2vec_vision)![Star](https://img.shields.io/github/stars/facebookresearch/data2vec_vision.svg?style=social&label=Star) | 2022      |                  |                                                  |
| DePlot            | DePlot: One-shot visual language reasoning by plot-to-table translation | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2212.10505) | ![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge) | 2022      |                  |                                                  |
| Donut             | OCR-free Document Understanding Transformer                  | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2111.15664) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/clovaai/donut)![Star](https://img.shields.io/github/stars/clovaai/donut.svg?style=social&label=Star) | 2021      | clovaai          |                                                  |
| FLAVA             | FLAVA: A Foundational Language And Vision Alignment Model    | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2112.04482) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/multimodal)![Star](https://img.shields.io/github/stars/facebookresearch/multimodal.svg?style=social&label=Star) | 2021      | facebookresearch |                                                  |
| GIT               | GIT: A Generative Image-to-text Transformer for Vision and Language | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2205.14100) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/GenerativeImage2Text)![Star](https://img.shields.io/github/stars/microsoft/GenerativeImage2Text.svg?style=social&label=Star) | 2022      | microsoft        |                                                  |
| Grounding DINO    | Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2303.05499) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/IDEA-Research/GroundingDINO)![Star](https://img.shields.io/github/stars/IDEA-Research/GroundingDINO.svg?style=social&label=Star) | 2023      | IDEA-Research    |                                                  |
| GroupViT          | GroupViT: Semantic Segmentation Emerges from Text Supervision | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2202.11094) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/NVlabs/GroupViT)![Star](https://img.shields.io/github/stars/NVlabs/GroupViT.svg?style=social&label=Star) | 2022      | NVlabs           |                                                  |
| IDEFICS           | OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://huggingface.co/papers/2306.16527) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://huggingface.co/docs/transformers/model_doc/INSERT%20LINK%20TO%20GITHUB%20REPO%20HERE) | 2023      | NVlabs           |                                                  |
| IDEFICS-2         | What matters when building vision-language models?           | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2405.02246) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/NVlabs/GroupViT) | 2024      | NVlabs           |                                                  |
| InstructBLIP      | InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2305.06500) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star) | 2023      | salesforce       |                                                  |
| InstructBlipVideo | InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2305.06500) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star) | 2023      | salesforce       |                                                  |
| KOSMOS-2          | Kosmos-2: Grounding Multimodal Large Language Models to the World | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2306.14824) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/unilm/tree/master/kosmos-2)![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star) | 2023      | microsoft        |                                                  |
| LayoutLM          | LayoutLM: Pre-training of Text and Layout for Document Image Understanding | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1912.13318) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)]() | 2019      | microsoft        |                                                  |
| LayoutLMV2        | LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2012.14740) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/NielsRogge/Transformers-Tutorials)![Star](https://img.shields.io/github/stars/NielsRogge/Transformers-Tutorials.svg?style=social&label=Star) | 2020      | microsoft        |                                                  |
| LayoutLMv3        | LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2204.08387) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/NielsRogge/Transformers-Tutorials)![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star) | 2022      | microsoft        |                                                  |
| LayoutXLM         | LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2104.08836) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/NielsRogge/microsoft/unilm)![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star) | 2021      | microsoft        |                                                  |
| LiLT              | LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2202.13669) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/jpwang/lilt)![Star](https://img.shields.io/github/stars/jpwang/lilt.svg?style=social&label=Star) | 2022      |                  |                                                  |
| LLaVa             | Visual Instruction Tuning                                    | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2304.08485) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/haotian-liu/LLaVA)![Star](https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&label=Star) | 2023      |                  |                                                  |
| LLaVa-VL          | Improved Baselines with Visual Instruction Tuning            | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/pdf/2310.03744) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/haotian-liu/LLaVA)![Star](https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&label=Star) | 2024      |                  |                                                  |
| LLaVA-NeXT        | LLaVA-NeXT: Improved reasoning, OCR, and world knowledge     | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2310.03744) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/haotian-liu/LLaVA)![Star](https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&label=Star) | 2024      |                  |                                                  |
| LLaVa-NeXT-Video  | LLaVA-NeXT: A Strong Zero-shot Video Understanding Model     | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://llava-vl.github.io/blog/2024-04-30-llava-next-video/) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/LLaVA-VL/LLaVA-NeXT)![Star](https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT.svg?style=social&label=Star) | 2024      |                  |                                                  |
| Video-LLaVA       | Video-LLaVA: Learning United Visual Representation by Alignment Before Projection | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2311.10122) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/LLaVA-VL/LLaVA-NeXT)![Star](https://img.shields.io/github/stars/PKU-YuanGroup/Video-LLaVA.svg?style=social&label=Star) | 2023      |                  |                                                  |
| LXMERT            | LXMERT: Learning Cross-Modality Encoder Representations from Transformers | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1908.07490) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/airsplay/lxmert)![Star](https://img.shields.io/github/stars/airsplay/lxmert.svg?style=social&label=Star) | 2019      |                  |                                                  |
| MatCha            | MatCha：Enhancing Visual Language Pretraining with Math Reasoning and Chart Derrendering | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2212.09662) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb) | 2022      | google           |                                                  |
| MGP-STR           | Multi-Granularity Prediction for Scene Text Recognition      | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2209.03592) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/MGP-STR)![Star](https://img.shields.io/github/stars/AlibabaResearch/AdvancedLiterateMachinery.svg?style=social&label=Star) | 2022      | AlibabaResearch  |                                                  |
| Nougat            | Nougat: Neural Optical Understanding for Academic Documents  | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2308.13418) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/nougat)![Star](https://img.shields.io/github/stars/facebookresearch/nougat.svg?style=social&label=Star) | 2023      | facebookresearch |                                                  |
| OneFormer         | OneFormer: One Transformer to Rule Universal Image Segmentation | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2211.06220) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/SHI-Labs/OneFormer)![Star](https://img.shields.io/github/stars/SHI-Labs/OneFormer.svg?style=social&label=Star) | 2022      | SHI-Labs         |                                                  |
| OWL-ViT           | Simple Open-Vocabulary Object Detection with Vision Transformers | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2205.06230) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit)![Star](https://img.shields.io/github/stars/google-research/scenic.svg?style=social&label=Star) | 2022      | google           |                                                  |
| OWLv2             | Scaling Open-Vocabulary Object Detection                     | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2306.09683) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit)![Star](https://img.shields.io/github/stars/google-research/scenic.svg?style=social&label=Star) | 2023      | google           |                                                  |
| PaliGemma         | PaliGemma – Google’s Cutting-Edge Open Vision Language Model | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2306.09683) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://huggingface.co/google/paligemma-3b-pt-224) | 2024      |                  |                                                  |
| Perceiver         | Perceiver IO: A General Architecture for Structured Inputs & Outputs | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2107.14795) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/deepmind/deepmind-research/tree/master/perceiver)![Star](https://img.shields.io/github/stars/deepmind/deepmind-research.svg?style=social&label=Star) | 2021      |                  |                                                  |
| Pix2Struct        | Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2210.03347) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/deepmind/deepmind-research/tree/master/perceiver)![Star](https://img.shields.io/github/stars/deepmind/deepmind-research.svg?style=social&label=Star) | 2022      |                  |                                                  |
| SAM               | Segment Anything                                             | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/pdf/2304.02643v1.pdf) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/segment-anything)![Star](https://img.shields.io/github/stars/facebookresearch/segment-anything.svg?style=social&label=Star) | 2023      | meta             |                                                  |
| SAM v2            | SAM 2: Segment Anything in Images and Videos                 | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/segment-anything-2)![Star](https://img.shields.io/github/stars/facebookresearch/segment-anything-2.svg?style=social&label=Star) | 2024      | meta             |                                                  |
| SigLIP            | Sigmoid Loss for Language Image Pre-Training                 | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2303.15343) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/big_vision)![Star](https://img.shields.io/github/stars/google-research/big_vision.svg?style=social&label=Star) | 2023      |                  |                                                  |
| TAPAS             | TAPAS: Weakly Supervised Table Parsing via Pre-training      | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://www.aclweb.org/anthology/2020.acl-main.398) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/big_vision)![Star](https://img.shields.io/github/stars/google-research/tapas.svg?style=social&label=Star) | 2020      |                  |                                                  |
| TrOCR             | TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2109.10282) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/unilm/tree/6f60612e7cc86a2a1ae85c47231507a587ab4e01/trocr)![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star) | 2021      |                  |                                                  |
| TVLT              | TVLT: Textless Vision-Language Transformer                   | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2209.14156) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/zinengtang/TVLT)![Star](https://img.shields.io/github/stars/zinengtang/TVLT.svg?style=social&label=Star) | 2022      |                  |                                                  |
| TVP               | Text-Visual Prompting for Efficient 2D Temporal Video Grounding | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2303.04995) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/intel/TVP)![Star](https://img.shields.io/github/stars/intel/TVP.svg?style=social&label=Star) | 2023      | Intel            |                                                  |
| UDOP              | Unifying Vision, Text, and Layout for Universal Document Processing | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2212.02623) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)]() | 2022      |                  |                                                  |
| ViLT              | ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2102.03334) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/dandelin/ViLT)![Star](https://img.shields.io/github/stars/dandelin/ViLT.svg?style=social&label=Star) | 2021      |                  |                                                  |
| VipLlava          | Making Large Multimodal Models Understand Arbitrary Visual Prompts | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2312.00784) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mu-cai/ViP-LLaVA)![Star](https://img.shields.io/github/stars/mu-cai/ViP-LLaVA.svg?style=social&label=Star) | 2023      |                  |                                                  |
| VisualBERT        | VisualBERT: A Simple and Performant Baseline for Vision and Language | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/pdf/1908.03557) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/uclanlp/visualbert)![Star](https://img.shields.io/github/stars/uclanlp/visualbert.svg?style=social&label=Star) | 2019      |                  |                                                  |
| X-CLIP            | Expanding Language-Image Pretrained Models for General Video Recognition | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2208.02816) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/VideoX)![Star](https://img.shields.io/github/stars/microsoft/VideoX.svg?style=social&label=Star) | 2022      |                  |                                                  |
|                   |                                                              |                                                              |                                                              |           |                  |                                                  |

- Vision Encoder Decoder Models

  - Vision Models

    - [ViT](https://huggingface.co/docs/transformers/model_doc/vit)
    - [BEiT](https://huggingface.co/docs/transformers/model_doc/beit)
    - [Swin](https://huggingface.co/docs/transformers/model_doc/swin)


  - Language Models

    - [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)
    - [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)
    - [BERT](https://huggingface.co/docs/transformers/model_doc/bert)
    - [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)



- Vision TextDual Encoder
  
- Speech Encoder Decoder Models



# Language-Models


| Model / Methods      | Title                                                        | Paper Link                                                   | Code Link                                                    | Published | Keywords         | Venue                                                        |
| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | --------- | ---------------- | ------------------------------------------------------------ |
| ALBERT               | ALBERT: A Lite BERT for Self-supervised Learning of Language Representations | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1909.11942) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/ALBERT)![Star](https://img.shields.io/github/stars/google-research/ALBERT.svg?style=social&label=Star) | 2019      | google-research  |                                                              |
| BART                 | BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1910.13461) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/fairseq/tree/main/examples/bart)![Star](https://img.shields.io/github/stars/facebookresearch/fairseq.svg?style=social&label=Star) | 2019      | facebookresearch |                                                              |
| BARThez              | BARThez: a Skilled Pretrained French Sequence-to-Sequence Model | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2010.12321) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/moussaKam/BARThez)![Star](https://img.shields.io/github/stars/moussaKam/BARThez.svg?style=social&label=Star) | 2020      | moussaKam        |                                                              |
| BARTpho              | BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2109.09701) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/VinAIResearch/BARTpho)![Star](https://img.shields.io/github/stars/VinAIResearch/BARTpho.svg?style=social&label=Star) | 2022      | VinAIResearch    | INTERSPEECH 2022                                             |
| BERT                 | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1810.04805) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/bert)![Star](https://img.shields.io/github/stars/google-research/bert.svg?style=social&label=Star) | 2018      |                  |                                                              |
| BertGeneration       | Leveraging Pre-trained Checkpoints for Sequence Generation Tasks | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1907.12461) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)]() | 2019      |                  |                                                              |
| BERTweet             | BERTweet: A pre-trained language model for English Tweets    | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://www.aclweb.org/anthology/2020.emnlp-demos.2.pdf) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/VinAIResearch/BERTweet)![Star](https://img.shields.io/github/stars/VinAIResearch/BERTweet.svg?style=social&label=Star) | 2020      |                  | EMNLP-2020                                                   |
| BigBird              | Big Bird: Transformers for Longer Sequences                  | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2007.14062) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/bigbird)![Star](https://img.shields.io/github/stars/google-research/bigbird.svg?style=social&label=Star) | 2020      |                  | [NeurIPS 2020](https://papers.nips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html) |
| BioGPT               | BioGPT: generative pre-trained transformer for biomedical text generation and mining | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/BioGPT)![Star](https://img.shields.io/github/stars/microsoft/BioGPT.svg?style=social&label=Star) | 2022      |                  |                                                              |
| Blenderbot           | Recipes for building an open-domain chatbot                  | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/pdf/2004.13637.pdf) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/ParlAI)![Star](https://img.shields.io/github/stars/facebookresearch/ParlAI.svg?style=social&label=Star) | 2020      |                  |                                                              |
| BLOOM                | Introducing The World’s Largest Open Multilingual Language Model: BLOOM | [![Paper](https://img.shields.io/badge/Blog-ydd7e6?style=for-the-badge)](https://bigscience.huggingface.co/blog/bloom) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)]() | 2022      |                  |                                                              |
| BORT                 | Optimal Subarchitecture Extraction for BERT                  | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2010.10499) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/alexa/bort)![Star](https://img.shields.io/github/stars/alexa/bort.svg?style=social&label=Star) | 2020      |                  |                                                              |
| ByT5                 | ByT5: Towards a token-free future with pre-trained byte-to-byte models | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2105.13626) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/byt5)![Star](https://img.shields.io/github/stars/google-research/byt5.svg?style=social&label=Star) | 2021      |                  |                                                              |
| CamemBERT            | CamemBERT: a Tasty French Language Model                     | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1911.03894) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://camembert-model.fr/) | 2019      |                  |                                                              |
| CANINE               | CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2103.06874) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/language/tree/master/language/canine)![Star](https://img.shields.io/github/stars/google-research/language.svg?style=social&label=Star) | 2021      | Google-research  |                                                              |
| CodeGen              | A Conversational Paradigm for Program Synthesis              | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2203.13474) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/salesforce/codegen)![Star](https://img.shields.io/github/stars/salesforce/codegen.svg?style=social&label=Star) | 2022      | salesforce       |                                                              |
| CodeLlama            | Code Llama: Open Foundation Models for Code                  | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/llama)![Star](https://img.shields.io/github/stars/facebookresearch/llama.svg?style=social&label=Star) | 2023      |                  |                                                              |
| Cohere               | Command-R: Retrieval Augmented Generation at Production Scale | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://cohere.com/blog/command-r) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/EleutherAI/gpt-neox)![Star](https://img.shields.io/github/stars/facebookresearch/llama.svg?style=social&label=Star) | 2024      |                  |                                                              |
| ConvBERT             | ConvBERT: Improving BERT with Span-based Dynamic Convolution | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2008.02496) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/TsinghuaAI/CPM-Generate)![Star](https://img.shields.io/github/stars/yitu-opensource/ConvBert.svg?style=social&label=Star) | 2020      |                  |                                                              |
| CPM                  | CPM: A Large-scale Generative Chinese Pre-trained Language Model | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2012.00413) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/TsinghuaAI/CPM-Generate)![Star](https://img.shields.io/github/stars/TsinghuaAI/CPM-Generate.svg?style=social&label=Star) | 2020      |                  |                                                              |
| CPMAnt               | CPM-Ant is an open-source Chinese pre-trained language model (PLM) with 10B parameters. | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live)![Star](https://img.shields.io/github/stars/OpenBMB/CPM-Live.svg?style=social&label=Star) | 2020      |                  |                                                              |
| CTRL                 | CTRL: A Conditional Transformer Language Model for Controllable Generation | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1909.05858) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/salesforce/ctrl)![Star](https://img.shields.io/github/stars/salesforce/ctrl.svg?style=social&label=Star) | 2019      |                  |                                                              |
| DBRX                 | DBRX is a [transformer-based](https://www.isattentionallyouneed.com/) decoder-only large language model (LLM) that was trained using next-token prediction. | [![Paper](https://img.shields.io/badge/Blog-ydd7e6?style=for-the-badge)](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/databricks/dbrx-instruct)![Star](https://img.shields.io/github/stars/databricks/dbrx-instruct.svg?style=social&label=Star) | 2024      |                  |                                                              |
| DeBERTa              | DeBERTa：Decoding-enhanced BERT with Disentangled Attention  | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2006.03654) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/DeBERTa)![Star](https://img.shields.io/github/stars/microsoft/DeBERTa.svg?style=social&label=Star) | 2020      |                  |                                                              |
| DeBERTa-v2           | DeBERTa：Decoding-enhanced BERT with Disentangled Attention  | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2006.03654) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/DeBERTa)![Star](https://img.shields.io/github/stars/microsoft/DeBERTa.svg?style=social&label=Star) | 2021      |                  |                                                              |
| DialoGPT             | DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1911.00536) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/DialoGPT)![Star](https://img.shields.io/github/stars/microsoft/DialoGPT.svg?style=social&label=Star) | 2019      |                  |                                                              |
| DistilBERT           | DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1910.01108) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) | 2019      |                  |                                                              |
| DPR                  | Dense Passage Retrieval for Open-Domain Question Answering   | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2004.04906) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/DPR)![Star](https://img.shields.io/github/stars/facebookresearch/DPR.svg?style=social&label=Star) | 2020      |                  |                                                              |
| ELECTRA              | ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://openreview.net/pdf?id=r1xMH1BtvB) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/electra)![Star](https://img.shields.io/github/stars/google-research/electra.svg?style=social&label=Star) | 2020      |                  |                                                              |
| ERNIE 1.0            | ERNIE: Enhanced Representation through Knowledge Integration | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1904.09223) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/PaddlePaddle/ERNIE)![Star](https://img.shields.io/github/stars/PaddlePaddle/ERNIE.svg?style=social&label=Star) | 2019      |                  |                                                              |
| ERNIE 2.0            | ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://ojs.aaai.org/index.php/AAAI/article/view/6428) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/PaddlePaddle/ERNIE)![Star](https://img.shields.io/github/stars/PaddlePaddle/ERNIE.svg?style=social&label=Star) | 2020      |                  | AAAI 2020                                                    |
| ERNIE 3.0            | ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2107.02137) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/PaddlePaddle/ERNIE)![Star](https://img.shields.io/github/stars/PaddlePaddle/ERNIE.svg?style=social&label=Star) | 2021      |                  |                                                              |
| ERNIE-Gram           | ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2010.12148) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/PaddlePaddle/ERNIE)![Star](https://img.shields.io/github/stars/PaddlePaddle/ERNIE.svg?style=social&label=Star) | 2020      |                  |                                                              |
| ERNIE-health         | Building Chinese Biomedical Language Models via Multi-Level Text Discrimination | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2110.07244) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/PaddlePaddle/ERNIE)![Star](https://img.shields.io/github/stars/PaddlePaddle/ERNIE.svg?style=social&label=Star) | 2022      |                  |                                                              |
| ErnieM               | ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2012.15674) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/PaddlePaddle/ERNIE)![Star](https://img.shields.io/github/stars/PaddlePaddle/ERNIE.svg?style=social&label=Star) | 2020      |                  |                                                              |
| ESM                  | Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://www.pnas.org/content/118/15/e2016239118) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/esm)![Star](https://img.shields.io/github/stars/facebookresearch/esm.svg?style=social&label=Star) | 2022      |                  |                                                              |
| Falcon               | The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2306.01116) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/huggingface/transformers/tree/main/src/transformers/models/falcon)![Star](https://img.shields.io/github/stars/huggingface/transformers.svg?style=social&label=Star) | 2023      |                  |                                                              |
| FastSpeech2Conformer | Recent Developments On Espnet Toolkit Boosted By Conformer   | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2010.13956) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/espnet/espnet)![Star](https://img.shields.io/github/stars/espnet/espnet.svg?style=social&label=Star) | 2020      |                  |                                                              |
| FLAN-T5              | Scaling Instruction-Finetuned Language Models                | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/pdf/2210.11416.pdf) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints)![Star](https://img.shields.io/github/stars/google-research/t5x.svg?style=social&label=Star) | 2022      |                  |                                                              |
| FLAN-UL2             | UL2: Unifying Language Learning Paradigms                    | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/pdf/2210.11416.pdf) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints)![Star](https://img.shields.io/github/stars/google-research/t5x.svg?style=social&label=Star) | 2022      |                  |                                                              |
| FlauBERT             | FlauBERT：Unsupervised Language Model Pre-training for French | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1912.05372) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/getalp/Flaubert)![Star](https://img.shields.io/github/stars/getalp/Flaubert.svg?style=social&label=Star) | 2019      |                  |                                                              |
| FNet                 | FNet: Mixing Tokens with Fourier Transforms                  | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2105.03824) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/google-research/tree/master/f_net)![Star](https://img.shields.io/github/stars/google-research/google-research.svg?style=social&label=Star) | 2021      |                  |                                                              |
| FSMT                 | Facebook FAIR’s WMT19 News Translation Task Submission       | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1907.06616) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/pytorch/fairseq/tree/master/examples/wmt19)![Star](https://img.shields.io/github/stars/pytorch/fairseq.svg?style=social&label=Star) | 2019      |                  |                                                              |
| Funnel Transformer   | Funnel-Transformer：Filtering out Sequential Redemption for Efficient Language Processing | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2006.03236) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com//laiguokun/Funnel-Transformer)![Star](https://img.shields.io/github/stars/laiguokun/Funnel-Transformer.svg?style=social&label=Star) | 2020      |                  |                                                              |
| Fuyu                 | Fuyu-8B: A Multimodal Architecture for AI Agents             | [![Paper](https://img.shields.io/badge/Blog-ydd7e6?style=for-the-badge)](https://www.adept.ai/blog/fuyu-8b) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/persimmon-ai-labs/adept-inference)![Star](https://img.shields.io/github/stars/persimmon-ai-labs/adept-inference.svg?style=social&label=Star) | 2023      |                  |                                                              |
| Gemma                | Gemma：Open Models Based on Gemini Technology and Research   | [![Paper](https://img.shields.io/badge/Blog-ydd7e6?style=for-the-badge)](https://blog.google/technology/developers/gemma-open-models/) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://www.kaggle.com/models/google/gemma) | 2023      |                  |                                                              |
| Gemma2               | Gemma2: Open Models Based on Gemini Technology and Research  | [![Paper](https://img.shields.io/badge/Blog-ydd7e6?style=for-the-badge)](https://blog.google/technology/developers/google-gemma-2/) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://www.kaggle.com/models/google/gemma) | 2023      |                  |                                                              |
| OpenAI GPT           | Improving Language Understanding by Generative Pre-Training  | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/openai/finetune-transformer-lm)![Star](https://img.shields.io/github/stars/openai/finetune-transformer-lm.svg?style=social&label=Star) | 2018      | OpenAI           |                                                              |
| GPT Neo              | The Pile: An 800GB Dataset of Diverse Text for Language Modeling | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2101.00027) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/EleutherAI/gpt-neo)![Star](https://img.shields.io/github/stars/EleutherAI/gpt-neo.svg?style=social&label=Star) | 2020      |                  |                                                              |
| GPTBigCode           | SantaCoder: don't reach for the stars!                       | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2301.03988) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/EleutherAI/gpt-neo) | 2023      |                  |                                                              |
| OpenAI GPT2          | Language Models are Unsupervised Multitask Learners          | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://openai.com/index/better-language-models/) | 2019      | 1.5B OpenAI      |                                                              |
| GPT-Sw3              | Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.376.pdf) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)]() | 2022      |                  |                                                              |
| HerBERT              | KLEJ: Comprehensive Benchmark for Polish Language Understanding | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://www.aclweb.org/anthology/2020.acl-main.111.pdf) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/allegro/HerBERT)![Star](https://img.shields.io/github/stars/allegro/HerBERT.svg?style=social&label=Star) | 2020      |                  |                                                              |
| I-BERT               | I-BERT: Integer-only BERT Quantization                       | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2101.01321) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/kssteven418/I-BERT)![Star](https://img.shields.io/github/stars/kssteven418/I-BERT.svg?style=social&label=Star) | 2021      |                  |                                                              |
| Jamba                | Introducing Jamba: AI21's Groundbreaking SSM-Transformer Model | [![Paper](https://img.shields.io/badge/Blog-ydd7e6?style=for-the-badge)](https://www.ai21.com/blog/announcing-jamba) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)]() | 2024      |                  |                                                              |
| Jukebox              | Jukebox: A generative model for music                        | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/pdf/2005.00341.pdf) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/openai/jukebox)![Star](https://img.shields.io/github/stars/openai/jukebox.svg?style=social&label=Star) | 2020      |                  |                                                              |
| LED                  | Longformer: The Long-Document Transformer                    | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2004.05150) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://colab.research.google.com/drive/12INTTR6n64TzS4RrXZxMSXfrOd9Xzamo?usp=sharing) | 2020      |                  |                                                              |
| LLaMA                | LLaMA: Open and Efficient Foundation Language Models         | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2302.13971) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/meta-llama/llama)![Star](https://img.shields.io/github/stars/meta-llama/llama.svg?style=social&label=Star) | 2023      |                  |                                                              |
| Llama2               | LLaMA: Open Foundation and Fine-Tuned Chat Models            | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/meta-llama/llama)![Star](https://img.shields.io/github/stars/meta-llama/llama.svg?style=social&label=Star) | 2023      |                  |                                                              |
| Llama3               | Introducing Meta Llama 3: The most capable openly available LLM to date | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://ai.meta.com/blog/meta-llama-3/) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/meta-llama/llama3)![Star](https://img.shields.io/github/stars/meta-llama/llama3.svg?style=social&label=Star) | 2024      |                  |                                                              |
| Longformer           | Longformer: The Long-Document Transformer                    | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/pdf/2004.05150.pdf) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/allenai/longformer)![Star](https://img.shields.io/github/stars/allenai/longformer.svg?style=social&label=Star) | 2020      |                  |                                                              |
| LongT5               | LongT5: Efficient Text-To-Text Transformer for Long Sequences | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2112.07916) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/allenai/longformer)![Star](https://img.shields.io/github/stars/allenai/longformer.svg?style=social&label=Star) | 2021      |                  |                                                              |
| LUKE                 | LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2112.07916) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/allenai/longformer)![Star](https://img.shields.io/github/stars/allenai/longformer.svg?style=social&label=Star) | 2020      |                  |                                                              |
| M2M100               | Beyond English-Centric Multilingual Machine Translation      | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2010.11125) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)]() | 2020      |                  |                                                              |
| MADLAD-400           | MADLAD-400：A Multilingual And Document-Level Large Audited Dataset]（MADLAD-400：A Multilingual And Document-Level Large Audited Dataset | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2309.04662) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/google-research/tree/master/madlad_400)![Star](https://img.shields.io/github/stars/google-research/google-research.svg?style=social&label=Star) | 2023      |                  |                                                              |
| Mamba                | Mamba：Linear-Time Sequence Modeling with Selective State Spaces | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2312.00752) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/state-spaces/mamba)![Star](https://img.shields.io/github/stars/state-spaces/mamba.svg?style=social&label=Star) | 2024      |                  |                                                              |
| MarianMT             | A framework for translation models, using the same models as BART. | [![Paper](https://img.shields.io/badge/Blog-ydd7e6?style=for-the-badge)](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/marian.md) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/marian.md) | 2024      |                  |                                                              |
| MarkupLM             | MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2110.08518) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/unilm/tree/master/markuplm)![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star) | 2021      |                  |                                                              |
| MBart and MBart-50   | Multilingual Denoising Pre-training for Neural Machine Translation | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2001.08210) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/pytorch/fairseq/tree/master/examples/mbart)![Star](https://img.shields.io/github/stars/pytorch/fairseq.svg?style=social&label=Star) | 2020      |                  |                                                              |
| Mega                 | Mega: Moving Average Equipped Gated Attention                | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2209.10655) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/mega)![Star](https://img.shields.io/github/stars/facebookresearch/mega.svg?style=social&label=Star) | 2022      |                  |                                                              |
| MegatronBERT         | Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1909.08053) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/NVIDIA/Megatron-LM)![Star](https://img.shields.io/github/stars/NVIDIA/Megatron-LM.svg?style=social&label=Star) | 2019      |                  |                                                              |
| MegatronGPT2         | Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1909.08053) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/NVIDIA/Megatron-LM)![Star](https://img.shields.io/github/stars/NVIDIA/Megatron-LM.svg?style=social&label=Star) | 2019      |                  |                                                              |
| Mistral              | Mistral-7B is a decoder-only Transformer                     | [![Paper](https://img.shields.io/badge/Blog-ydd7e6?style=for-the-badge)](https://mistral.ai/news/announcing-mistral-7b/) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/persimmon-ai-labs/adept-inference) | 2023      |                  |                                                              |
| Mixtral              | a high-quality sparse mixture of experts models (SMoE) with open weights. | [![Paper](https://img.shields.io/badge/Blog-ydd7e6?style=for-the-badge)](https://mistral.ai/news/mixtral-of-experts/) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/NVIDIA/Megatron-LM) | 2023      |                  |                                                              |
| mLUKE                | mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2110.08151) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/studio-ousia/luke)![Star](https://img.shields.io/github/stars/studio-ousia/luke.svg?style=social&label=Star) | 2021      |                  |                                                              |
| MobileBERT           | MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2004.02984) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/google-research/tree/master/mobilebert)![Star](https://img.shields.io/github/stars/google-research/google-research.svg?style=social&label=Star) | 2020      |                  |                                                              |
| MPNet                | MPNet：Masked and Permuted Pre-training for Language Understanding | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2004.09297) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/MPNet)![Star](https://img.shields.io/github/stars/microsoft/MPNet.svg?style=social&label=Star) | 2020      |                  |                                                              |
| MPT                  | MPT models are GPT-style decoder-only transformers with several improvements | [![Paper](https://img.shields.io/badge/Blog-ydd7e6?style=for-the-badge)](https://www.mosaicml.com/blog/mpt-7b) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mosaicml/llm-foundry)![Star](https://img.shields.io/github/stars/mosaicml/llm-foundry.svg?style=social&label=Star) | 2023      |                  |                                                              |
| MRA                  | Multi Resolution Analysis (MRA) for Approximate Self-Attention | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2207.10284) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mlpen/mra-attention)![Star](https://img.shields.io/github/stars/mlpen/mra-attention.svg?style=social&label=Star) | 2022      |                  |                                                              |
| MT5                  | mT5: A massively multilingual pre-trained text-to-text transformer | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2010.11934) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/multilingual-t5)![Star](https://img.shields.io/github/stars/google-research/multilingual-t5.svg?style=social&label=Star) | 2020      |                  |                                                              |
| MVP                  | MVP: Multi-task Supervised Pre-training for Natural Language Generation | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2206.12131) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/RUCAIBox/MVP)![Star](https://img.shields.io/github/stars/RUCAIBox/MVP.svg?style=social&label=Star) | 2022      |                  |                                                              |
| Nezha                | NEZHA: Neural Contextualized Representation for Chinese Language Understanding | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1909.00204) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/huawei-noah/Pretrained-Language-Model)![Star](https://img.shields.io/github/stars/huawei-noah/Pretrained-Language-Model.svg?style=social&label=Star) | 2019      |                  |                                                              |
| NLLB                 | No Language Left Behind: Scaling Human-Centered Machine Translation | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2207.04672) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/fairseq/tree/nllb)![Star](https://img.shields.io/github/stars/facebookresearch/fairseq.svg?style=social&label=Star) | 2022      |                  |                                                              |
| NLLB-MOE             | No Language Left Behind: Scaling Human-Centered Machine Translation | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2207.04672) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/fairseq/tree/nllb)![Star](https://img.shields.io/github/stars/facebookresearch/fairseq.svg?style=social&label=Star) | 2022      |                  |                                                              |
| Nyströmformer        | Nyströmformer：A Nyström-Based Algorithm for Approximating Self-Attention | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2102.03902) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mlpen/Nystromformer)![Star](https://img.shields.io/github/stars/mlpen/Nystromformer.svg?style=social&label=Star) | 2021      |                  |                                                              |
| OLMo                 | OLMo: Accelerating the Science of Language Models            | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2402.00838) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/allenai/OLMo/tree/main/olmo)![Star](https://img.shields.io/github/stars/allenai/OLMo.svg?style=social&label=Star) | 2024      |                  |                                                              |
| Open-Llama           | The Open-Llama model was proposed in the open source Open-Llama project by community developer s-JoL. | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)]() | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://huggingface.co/models?search=openllama) | 2023      |                  |                                                              |
| OPT                  | Open Pre-trained Transformer Language Models                 | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/pdf/2205.01068) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/metaseq)![Star](https://img.shields.io/github/stars/facebookresearch/metaseq.svg?style=social&label=Star) | 2022      |                  |                                                              |
| Pegasus              | PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/pdf/1912.08777.pdf) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/pegasus)![Star](https://img.shields.io/github/stars/google-research/pegasus.svg?style=social&label=Star) | 2019      |                  |                                                              |
| PEGASUS-X            | Investigating Efficiently Extending Transformers for Long Input Summarization | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2208.04347) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/pegasus)![Star](https://img.shields.io/github/stars/google-research/pegasus.svg?style=social&label=Star) | 2022      |                  |                                                              |
| Persimmon            | Persimmon-8B is a fully permissively-licensed model with approximately 8 billion parameters | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)]() | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/persimmon-ai-labs/adept-inference)![Star](https://img.shields.io/github/stars/persimmon-ai-labs/adept-inference.svg?style=social&label=Star) | 2022      |                  |                                                              |
| Phi                  | Textbooks Are All You Need                                   | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2309.05463) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](ttps://huggingface.co/microsoft/phi-1) | 2023      |                  |                                                              |
| Phi-3                | Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2404.14219) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) | 2024      |                  |                                                              |
| PhoBERT              | PhoBERT: Pre-trained language models for Vietnamese          | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://www.aclweb.org/anthology/2020.findings-emnlp.92.pdf) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/VinAIResearch/PhoBERT)![Star](https://img.shields.io/github/stars/VinAIResearch/PhoBERT.svg?style=social&label=Star) | 2022      |                  |                                                              |
| PLBart               | Unified Pre-training for Program Understanding and Generation | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2103.06333) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/wasiahmad/PLBART)![Star](https://img.shields.io/github/stars/wasiahmad/PLBART.svg?style=social&label=Star) | 2021      |                  |                                                              |
| ProphetNet           | ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2001.04063) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)]() | 2020      |                  |                                                              |
| QDQBERT              | Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2004.09602) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/wasiahmad/PLBART) | 2020      |                  |                                                              |
| Qwen                 | Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2309.16609) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/QwenLM/Qwen)![Star](https://img.shields.io/github/stars/QwenLM/Qwen.svg?style=social&label=Star) | 2023      |                  |                                                              |
| Qwen2                | Qwen2 is the new model series of large language models from the Qwen team. | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2407.10671) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/QwenLM/Qwen2)![Star](https://img.shields.io/github/stars/QwenLM/Qwen2.svg?style=social&label=Star) | 2024      |                  |                                                              |
| Qwen-VL              | Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2308.12966) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/QwenLM/Qwen-VL)![Star](https://img.shields.io/github/stars/QwenLM/Qwen-VL.svg?style=social&label=Star) | 2023      |                  |                                                              |
| Qwen2MoE             | Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters | [![Paper](https://img.shields.io/badge/BLog-ydd7e6?style=for-the-badge)](https://qwenlm.github.io/blog/qwen-moe/) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://huggingface.co/Qwen) | 2024      |                  |                                                              |
| REALM                | REALM: Retrieval-Augmented Language Model Pre-Training       | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2002.08909) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/language/tree/master/language/realm)![Star](https://img.shields.io/github/stars/google-research/language.svg?style=social&label=Star) | 2020      |                  |                                                              |
| RecurrentGemma       | RecurrentGemma: Moving Past Transformers for Efficient Open Language Models | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-deepmind/recurrentgemma)![Star](https://img.shields.io/github/stars/google-deepmind/recurrentgemma.svg?style=social&label=Star) | 2024      |                  |                                                              |
| Reformer             | Reformer: The Efficient Transformer                          | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2001.04451.pdf) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google/trax/tree/master/trax/models/reformer)![Star](https://img.shields.io/github/stars/google/trax.svg?style=social&label=Star) | 2020      |                  |                                                              |
| RemBERT              | Rethinking Embedding Coupling in Pre-trained Language Models | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2010.12821) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)]() | 2020      |                  |                                                              |
| RetriBERT            | Explain Anything Like I’m Five: A Model for Open Domain Long Form Question Answering | [![Paper](https://img.shields.io/badge/BLOG-ydd7e6?style=for-the-badge)](https://yjernite.github.io/lfqa.html) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/huggingface/transformers/tree/main/examples/research-projects/distillation) | 2020      |                  |                                                              |
| RoBERTa              | RoBERTa: A Robustly Optimized BERT Pretraining Approach      | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1907.11692) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/pytorch/fairseq/tree/master/examples/roberta)![Star](https://img.shields.io/github/stars/pytorch/fairseq.svg?style=social&label=Star) | 2019      |                  |                                                              |
| RoBERTa-PreLayerNorm | fairseq: A Fast, Extensible Toolkit for Sequence Modeling    | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1904.01038) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/princeton-nlp/DinkyTrain)![Star](https://img.shields.io/github/stars/princeton-nlp/DinkyTrain.svg?style=social&label=Star) | 2022      |                  |                                                              |
| RoCBert              | RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://aclanthology.org/2022.acl-long.65.pdf) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)]() | 2022      |                  |                                                              |
| RoFormer             | RoFormer: Enhanced Transformer with Rotary Position Embedding | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://aclanthology.org/2022.acl-long.65.pdf) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/ZhuiyiTechnology/roformer)![Star](https://img.shields.io/github/stars/ZhuiyiTechnology/roformer.svg?style=social&label=Star) | 2021      |                  |                                                              |
| RWKV-LM              | RWKV is an RNN with transformer-level LLM performance        | -                                                            | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/BlinkDL/RWKV-LM)![Star](https://img.shields.io/github/stars/BlinkDL/RWKV-LM.svg?style=social&label=Star) | 2022      |                  |                                                              |
| RWKV-4.0             | RWKV: Reinventing RNNs for the Transformer Era               | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2305.13048) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/BlinkDL/RWKV-LM)![Star](https://img.shields.io/github/stars/BlinkDL/RWKV-LM.svg?style=social&label=Star) | 2023      |                  |                                                              |
| RWKV-5/6 Eagle/Finch | Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2404.05892) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/BlinkDL/RWKV-LM)![Star](https://img.shields.io/github/stars/BlinkDL/RWKV-LM.svg?style=social&label=Star) | 2024      |                  |                                                              |
| Splinter             | Few-Shot Question Answering by Pretraining Span Selection    | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2101.00438) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/oriram/splinter)![Star](https://img.shields.io/github/stars/oriram/splinter.svg?style=social&label=Star) | 2021      |                  |                                                              |
| SqueezeBERT          | SqueezeBERT: What can computer vision teach NLP about efficient neural networks? | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2006.11316) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)]() | 2020      |                  |                                                              |
| StableLM             | StableLM-3B-4E1T                                             | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://stability.wandb.io/stability-llm/stable-lm/reports/StableLM-3B-4E1T--VmlldzoyMjU4?accessToken=u3zujipenkx5g7rtcj9qojjgxpconyjktjkli2po09nffrffdhhchq045vp0wyfo) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)]() | 2024      |                  |                                                              |
| Starcoder2           | StarCoder 2 and The Stack v2: The Next Generation            | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2402.19173) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)]() | 2024      |                  |                                                              |
| SwitchTransformers   | Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2101.03961) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google/flaxformer/tree/main/flaxformer/architectures/moe)![Star](https://img.shields.io/github/stars/google/flaxformer.svg?style=social&label=Star) | 2021      |                  |                                                              |
| T5                   | Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/pdf/1910.10683.pdf) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/text-to-text-transfer-transformer)![Star](https://img.shields.io/github/stars/google-research/text-to-text-transfer-transformer.svg?style=social&label=Star) | 2023      |                  |                                                              |
| T5v1.1               | Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/pdf/1910.10683.pdf) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/text-to-text-transfer-transformer)![Star](https://img.shields.io/github/stars/google-research/text-to-text-transfer-transformer.svg?style=social&label=Star) | 2023      |                  |                                                              |
| TAPEX                | TAPEX: Table Pre-training via Learning a Neural SQL Executor | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2107.07653) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)]() | 2021      |                  |                                                              |
| Transformer XL       | Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1901.02860) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/kimiyoung/transformer-xl) | 2019      |                  |                                                              |
| UL2                  | Unifying Language Learning Paradigms                         | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/pdf/2205.05131v1.pdf) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/google-research/tree/master/ul2)![Star](https://img.shields.io/github/stars/google-research/google-research.svg?style=social&label=Star) | 2022      |                  |                                                              |
| UMT5                 | UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://openreview.net/forum?id=kXwdL1cWOAi) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/t5x)![Star](https://img.shields.io/github/stars/google-research/t5x.svg?style=social&label=Star) | 2024      |                  |                                                              |
| X-MOD                | Lifting the Curse of Multilinguality by Pre-training Modular Transformers | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](http://dx.doi.org/10.18653/v1/2022.naacl-main.255) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/fairseq/tree/58cc6cca18f15e6d56e3f60c959fe4f878960a60/fairseq/models/xmod)![Star](https://img.shields.io/github/stars/facebookresearch/fairseq.svg?style=social&label=Star) | 2022      |                  |                                                              |
| XGLM                 | Few-shot Learning with Multilingual Language Models          | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2112.10668) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/pytorch/fairseq/tree/main/examples/xglm)![Star](https://img.shields.io/github/stars/facebookresearch/fairseq.svg?style=social&label=Star) | 2021      |                  |                                                              |
| XLM                  | Cross-lingual Language Model Pretraining                     | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1901.07291) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/XLM/)![Star](https://img.shields.io/github/stars/facebookresearch/XLM.svg?style=social&label=Star) | 2019      |                  |                                                              |
| XLM-ProphetNet       | ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2001.04063) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/XLM/)![Star](https://img.shields.io/github/stars/facebookresearch/XLM.svg?style=social&label=Star) | 2020      |                  |                                                              |
| XLM-RoBERTa          | Unsupervised Cross-lingual Representation Learning at Scale  | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1911.02116) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/pytorch/fairseq/tree/master/examples/xlmr)![Star](https://img.shields.io/github/stars/pytorch/fairseq.svg?style=social&label=Star) | 2019      |                  |                                                              |
| XLM-RoBERTa-XL       | Larger-Scale Transformers for Multilingual Masked Language Modeling | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2105.00572) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/pytorch/fairseq/tree/master/examples/xlmr)![Star](https://img.shields.io/github/stars/pytorch/fairseq.svg?style=social&label=Star) | 2021      |                  |                                                              |
| XLM-V                | XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2301.10472) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/stefan-it/xlm-v-experiments)![Star](https://img.shields.io/github/stars/stefan-it/xlm-v-experiments.svg?style=social&label=Star) | 2023      |                  |                                                              |
| XLNet                | XLNet: Generalized Autoregressive Pretraining for Language Understanding | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1906.08237) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/zihangdai/xlnet/)![Star](https://img.shields.io/github/stars/zihangdai/xlnet.svg?style=social&label=Star) | 2019      |                  |                                                              |
| YOSO                 | You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2111.09714) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mlpen/YOSO)![Star](https://img.shields.io/github/stars/mlpen/YOSO.svg?style=social&label=Star) | 2021      |                  |                                                              |
|                      |                                                              |                                                              |                                                              |           |                  |                                                              |

- Encoder Decoder Models
- RAG



# Vision-Models

## On-Hugging-face


| Model / Methods     | Title                                                        | Paper Link                                                   | Code Link                                                    | Published | Keywords         | Venue        |
| ------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | --------- | ---------------- | ------------ |
| BEiT                | BEiT: BERT Pre-Training of Image Transformers                | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2111.09714) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/unilm/tree/master/beit)![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star) | 2022      | microsoft        |              |
| BiT                 | Big Transfer (BiT): General Visual Representation Learning   | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1912.11370) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/big_transfer)![Star](https://img.shields.io/github/stars/google-research/big_transfer.svg?style=social&label=Star) | 2020      | google-research  |              |
| Conditional DETR    | Conditional DETR for Fast Training Convergence               | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2108.06152) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/Atten4Vis/ConditionalDETR)![Star](https://img.shields.io/github/stars/Atten4Vis/ConditionalDETR.svg?style=social&label=Star) | 2023      |                  |              |
| ConvNeXT            | A ConvNet for the 2020s                                      | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2201.03545) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/ConvNeXt)![Star](https://img.shields.io/github/stars/facebookresearch/ConvNeXt.svg?style=social&label=Star) | 2022      |                  |              |
| ConvNeXt V2         | ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2301.00808) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/ConvNeXt-V2)![Star](https://img.shields.io/github/stars/facebookresearch/ConvNeXt-V2.svg?style=social&label=Star) | 2023      |                  |              |
| CvT                 | CvT: Introducing Convolutions to Vision Transformers         | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2103.15808) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/CvT)![Star](https://img.shields.io/github/stars/microsoft/CvT.svg?style=social&label=Star) | 2021      |                  |              |
| Deformable DETR     | Deformable DETR: Deformable Transformers for End-to-End Object Detection | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2010.04159) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/fundamentalvision/Deformable-DETR)![Star](https://img.shields.io/github/stars/fundamentalvision/Deformable-DETR.svg?style=social&label=Star) | 2020      |                  |              |
| DeiT                | Training data-efficient image transformers & distillation through attention | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2012.12877) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/deit)![Star](https://img.shields.io/github/stars/facebookresearch/deit.svg?style=social&label=Star) | 2021      |                  |              |
| Depth Anything      | Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2401.10891) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/LiheYoung/Depth-Anything)![Star](https://img.shields.io/github/stars/LiheYoung/Depth-Anything.svg?style=social&label=Star) | 2024      |                  |              |
| Depth Anything V2   | Depth Anything V2                                            | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2406.09414) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/DepthAnything/Depth-Anything-V2)![Star](https://img.shields.io/github/stars/DepthAnything/Depth-Anything-V2.svg?style=social&label=Star) | 2024      |                  |              |
| DETA                | NMS Strikes Back                                             | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2212.06137) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/jozhang97/DETA)![Star](https://img.shields.io/github/stars/jozhang97/DETA.svg?style=social&label=Star) | 2021      |                  |              |
| DETR                | End-to-End Object Detection with Transformers                | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2005.12872) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/detr)![Star](https://img.shields.io/github/stars/facebookresearch/detr.svg?style=social&label=Star) | 2020      | facebookresearch |              |
| DiNAT               | Dilated Neighborhood Attention Transformer                   | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2209.15001) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer)![Star](https://img.shields.io/github/stars/SHI-Labs/Neighborhood-Attention-Transformer.svg?style=social&label=Star) | 2022      | SHI-Labs         |              |
| DINOv2              | DINOv2: Learning Robust Visual Features without Supervision  | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2304.07193) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/dinov2)![Star](https://img.shields.io/github/stars/facebookresearch/dinov2.svg?style=social&label=Star) | 2023      | facebookresearch |              |
| DiT                 | DiT: Self-supervised Pre-training for Document Image Transformer | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2203.02378) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/unilm/tree/master/dit)![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star) | 2022      |                  |              |
| DPT                 | Vision Transformers for Dense Prediction                     | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2103.13413) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/isl-org/DPT)![Star](https://img.shields.io/github/stars/isl-org/DPT.svg?style=social&label=Star) | 2021      |                  |              |
| EfficientFormer     | EfficientFormer: Vision Transformers at MobileNet Speed      | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2206.01191) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/snap-research/EfficientFormer)![Star](https://img.shields.io/github/stars/snap-research/EfficientFormer.svg?style=social&label=Star) | 2022      | snap-research    | NeurIPs 2022 |
| EfficientFormerV2   | Rethinking Vision Transformers for MobileNet Size and Speed  | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2212.08059) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/snap-research/EfficientFormer)![Star](https://img.shields.io/github/stars/snap-research/EfficientFormer.svg?style=social&label=Star) | 2023      | snap-research    | ICCV 2023    |
| EfficientNet        | EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1905.11946) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet)![Star](https://img.shields.io/github/stars/tensorflow/tpu.svg?style=social&label=Star) | 2019      | tensorflow       |              |
| FocalNet            | Focal Modulation Networks                                    | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2203.11926) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/FocalNet)![Star](https://img.shields.io/github/stars/microsoft/FocalNet.svg?style=social&label=Star) | 2022      |                  |              |
| GLPN                | Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2201.07436) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/vinvino02/GLPDepth)![Star](https://img.shields.io/github/stars/vinvino02/GLPDepth.svg?style=social&label=Star) | 2022      |                  |              |
| Hiera               | Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2306.00989) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/hiera)![Star](https://img.shields.io/github/stars/facebookresearch/hiera.svg?style=social&label=Star) | 2023      |                  |              |
| OpenAI ImageGPT     | Generative Pretraining from Pixels                           | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://openai.com/blog/image-gpt) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/openai/image-gpt)![Star](https://img.shields.io/github/stars/openai/image-gpt.svg?style=social&label=Star) | 2020      | openai           | ICML 2020    |
| LeViT               | LeViT: Introducing Convolutions to Vision Transformers       | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2104.01136) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/LeViT)![Star](https://img.shields.io/github/stars/facebookresearch/LeViT.svg?style=social&label=Star) | 2021      | facebookresearch |              |
| Mask2Former         | Masked-attention Mask Transformer for Universal Image Segmentation | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2112.01527) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/Mask2Former)![Star](https://img.shields.io/github/stars/facebookresearch/Mask2Former.svg?style=social&label=Star) | 2022      |                  |              |
| MaskFormer          | Per-Pixel Classification is Not All You Need for Semantic Segmentation | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2107.06278) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/MaskFormer)![Star](https://img.shields.io/github/stars/facebookresearch/MaskFormer.svg?style=social&label=Star) | 2021      | facebookresearch | NeurIPS 2021 |
| MobileNet V1        | MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1704.04861) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md)![Star](https://img.shields.io/github/stars/tensorflow/models.svg?style=social&label=Star) | 2017      |                  |              |
| MobileNet V2        | MobileNetV2: Inverted Residuals and Linear Bottlenecks       | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1801.04381) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md)![Star](https://img.shields.io/github/stars/tensorflow/models.svg?style=social&label=Star) | 2018      |                  |              |
| MobileViT           | MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2110.02178) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/apple/ml-cvnets)![Star](https://img.shields.io/github/stars/apple/ml-cvnets.svg?style=social&label=Star) | 2021      | apple            |              |
| MobileViT V2        | Separable Self-attention for Mobile Vision Transformers      | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2206.02680) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/apple/ml-cvnets)![Star](https://img.shields.io/github/stars/apple/ml-cvnets.svg?style=social&label=Star) | 2021      | apple            |              |
| NAT                 | Neighborhood Attention Transformer                           | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2204.07143) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer)![Star](https://img.shields.io/github/stars/SHI-Labs/Neighborhood-Attention-Transformer.svg?style=social&label=Star) | 2022      | SHI-Labs         |              |
| PoolFormer          | MetaFormer is Actually What You Need for Vision              | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2111.11418) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/sail-sg/poolformer)![Star](https://img.shields.io/github/stars/sail-sg/poolformer.svg?style=social&label=Star) | 2021      |                  |              |
| PVT                 | Pyramid Vision Transformer (PVT)                             | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2102.12122) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/whai362/PVT)![Star](https://img.shields.io/github/stars/whai362/PVT.svg?style=social&label=Star) | 2021      |                  |              |
| PVTv2               | PVT v2: Improved Baselines with Pyramid Vision Transformer   | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2106.13797) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/whai362/PVT)![Star](https://img.shields.io/github/stars/whai362/PVT.svg?style=social&label=Star) | 2021      |                  |              |
| RegNet              | Designing Network Design Spaces                              | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2003.13678) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/pycls)![Star](https://img.shields.io/github/stars/facebookresearch/pycls.svg?style=social&label=Star) | 2020      | facebookresearch |              |
| ResNet              | Deep Residual Learning for Image Recognition                 | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1512.03385) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/KaimingHe/deep-residual-networks)![Star](https://img.shields.io/github/stars/KaimingHe/deep-residual-networks.svg?style=social&label=Star) | 2015      |                  |              |
| RT-DETR             | DETRs Beat YOLOs on Real-time Object Detection               | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2304.08069) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/lyuwenyu/RT-DETR/)![Star](https://img.shields.io/github/stars/lyuwenyu/RT-DETR.svg?style=social&label=Star) | 2023      |                  |              |
| SegFormer           | SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2105.15203) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/NVlabs/SegFormer)![Star](https://img.shields.io/github/stars/NVlabs/SegFormer.svg?style=social&label=Star) | 2021      |                  |              |
| SegGPT              | SegGPT: Segmenting Everything In Context                     | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2304.03284) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/baaivision/Painter)![Star](https://img.shields.io/github/stars/baaivision/Painter.svg?style=social&label=Star) | 2023      |                  |              |
| SuperPoint          | SuperPoint: Self-Supervised Interest Point Detection and Description | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1712.07629) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/magicleap/SuperPointPretrainedNetwork)![Star](https://img.shields.io/github/stars/magicleap/SuperPointPretrainedNetwork.svg?style=social&label=Star) | 2017      |                  |              |
| SwiftFormer         | SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2303.15446) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/Amshaker/SwiftFormer)![Star](https://img.shields.io/github/stars/Amshaker/SwiftFormer.svg?style=social&label=Star) | 2023      |                  |              |
| Swin Transformer    | Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2103.14030) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/Swin-Transformer)![Star](https://img.shields.io/github/stars/microsoft/Swin-Transformer.svg?style=social&label=Star) | 2021      | microsoft        |              |
| Swin Transformer V2 | Swin Transformer V2: Scaling Up Capacity and Resolution      | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2111.09883) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/Swin-Transformer)![Star](https://img.shields.io/github/stars/microsoft/Swin-Transformer.svg?style=social&label=Star) | 2021      | microsoft        |              |
| Swin2SR             | Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2209.11345) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mv-lab/swin2sr)![Star](https://img.shields.io/github/stars/mv-lab/swin2sr.svg?style=social&label=Star) | 2022      |                  | ECCV 2022    |
| Table Transformer   | PubTables-1M: Towards comprehensive table extraction from unstructured documents | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2110.00061) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/table-transformer)![Star](https://img.shields.io/github/stars/microsoft/table-transformer.svg?style=social&label=Star) | 2021      |                  |              |
| UPerNet             | Unified Perceptual Parsing for Scene Understanding           | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1807.10221) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/uper_head.py)![Star](https://img.shields.io/github/stars/open-mmlab/mmsegmentation.svg?style=social&label=Star) | 2018      |                  |              |
| VAN                 | Visual Attention Network                                     | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2202.09741) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/Visual-Attention-Network/VAN-Classification)![Star](https://img.shields.io/github/stars/Visual-Attention-Network/VAN-Classification.svg?style=social&label=Star) | 2022      |                  |              |
| ViT                 | An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2010.11929) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/vision_transformer)![Star](https://img.shields.io/github/stars/google-research/vision_transformer.svg?style=social&label=Star) | 2020      | google-research  |              |
| ViT Hybrid          | An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2010.11929) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/vision_transformer)![Star](https://img.shields.io/github/stars/google-research/vision_transformer.svg?style=social&label=Star) | 2020      |                  |              |
| ViTDet              | Exploring Plain Vision Transformer Backbones for Object Detection | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2203.16527) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/detectron2/tree/main/projects/ViTDet)![Star](https://img.shields.io/github/stars/facebookresearch/detectron2.svg?style=social&label=Star) | 2022      |                  |              |
| ViTMAE              | Masked Autoencoders Are Scalable Vision Learners             | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2111.06377v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/mae)![Star](https://img.shields.io/github/stars/facebookresearch/mae.svg?style=social&label=Star) | 2021      |                  |              |
| ViTMatte            | Boosting Image Matting with Pretrained Plain Vision Transformers | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2305.15272) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/hustvl/ViTMatte)![Star](https://img.shields.io/github/stars/hustvl/ViTMatte.svg?style=social&label=Star) | 2023      |                  |              |
| ViTMSN              | Masked Siamese Networks for Label-Efficient Learning         | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2204.07141) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/msn)![Star](https://img.shields.io/github/stars/facebookresearch/msn.svg?style=social&label=Star) | 2022      | facebookresearch |              |
| YOLOS               | You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2106.00666) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/hustvl/YOLOS)![Star](https://img.shields.io/github/stars/hustvl/YOLOS.svg?style=social&label=Star) | 2021      |                  |              |
| ZoeDepth            | ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2302.12288) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/Caojunxu/AC-FPN)![Star](https://img.shields.io/github/stars/Caojunxu/AC-FPN.svg?style=social&label=Star) | 2023      |                  |              |
|                     |                                                              |                                                              |                                                              |           |                  |              |
|                     |                                                              |                                                              |                                                              |           |                  |              |

## Detection

### On-coco2017-Benchmark

| Model / Methods                                              | Title                                                        | Paper Link                                                   | Code Link                                                    | Published  | Keywords | Venue        |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ---------- | -------- | ------------ |
| Fast-RCNN                                                    | Fast R-CNN                                                   | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1504.08083v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/rbgirshick/fast-rcnn)![Star](https://img.shields.io/github/stars/rbgirshick/fast-rcnn.svg?style=social&label=Star) | 2015.04.30 |          | ICCV 2015    |
| SSD512                                                       | SSD: Single Shot MultiBox Detector                           | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1512.02325v5) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/weiliu89/caffe/tree/ssd)![Star](https://img.shields.io/github/stars/weiliu89/caffe.svg?style=social&label=Star) | 2015.12.08 |          |              |
| Faster R-CNN (box refinement, context, multi-scale testing)  | Deep Residual Learning for Image Recognition                 | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1512.03385v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/tensorflow/models)![Star](https://img.shields.io/github/stars/tensorflow/models.svg?style=social&label=Star) | 2015.12.10 |          | CVPR 2016    |
| ION                                                          | Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2311.12068v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)]() | 2015.12.14 |          | CVPR 2016    |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
| MultiPath Network                                            | A MultiPath Network for Object Detection                     | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1604.02135v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/multipathnet)![Star](https://img.shields.io/github/stars/facebookresearch/multipathnet.svg?style=social&label=Star) | 2016.04.07 |          |              |
| Faster R-CNN                                                 | Speed/accuracy trade-offs for modern convolutional object detectors | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1611.10012v3) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/tensorflow/models)![Star](https://img.shields.io/github/stars/tensorflow/models.svg?style=social&label=Star) | 2016.11.30 |          | CVPR 2017    |
| Faster R-CNN + FPN                                           | Feature Pyramid Networks for Object Detection                | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1612.06851v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/detectron)![Star](https://img.shields.io/github/stars/facebookresearch/detectron.svg?style=social&label=Star) | 2016.12.09 |          | CVPR 2017    |
| Faster R-CNN + TDM(Deconvolutional SSD)                      | Beyond Skip Connections: Top-Down Modulation for Object Detection | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1612.06851v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/MTCloudVision/mxnet-dssd)![Star](https://img.shields.io/github/stars/MTCloudVision/mxnet-dssd.svg?style=social&label=Star) | 2016.12.10 |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
| Mask R-CNN                                                   | Mask R-CNN                                                   | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1708.02002v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/detectron)![Star](https://img.shields.io/github/stars/zhaoweicai/cascade-rcnn.svg?style=social&label=Star) | 2017.03.20 |          | ICCV 2017    |
| RefineDet                                                    | Single-Shot Refinement Neural Network for Object Detection   | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1711.06897v3) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/sfzhang15/RefineDet)![Star](https://img.shields.io/github/stars/sfzhang15/RefineDet.svg?style=social&label=Star) | 2017.08.07 |          | CVPR 2018    |
| RetinaNet                                                    | Focal Loss for Dense Object Detection                        | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1708.02002v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/detectron)![Star](https://img.shields.io/github/stars/zhaoweicai/cascade-rcnn.svg?style=social&label=Star) | 2017.08.07 |          |              |
| FPN                                                          | ChainerCV: a Library for Deep Learning in Computer Vision    | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1708.08169v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/pfnet/chainercv)![Star](https://img.shields.io/github/stars/pfnet/chainercv.svg?style=social&label=Star) | 2017.08.28 |          |              |
| DeformConv-R-FCN                                             | Deformable Convolutional Networks                            | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1703.06211v3) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/msracver/Deformable-ConvNets)![Star](https://img.shields.io/github/stars/msracver/Deformable-ConvNets.svg?style=social&label=Star) | 2017.10.17 |          | ICCV 2017    |
| D-RFCN + SNIP                                                | An Analysis of Scale Invariance in Object Detection - SNIP   | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1711.08189v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/bharatsingh430/snip)![Star](https://img.shields.io/github/stars/bharatsingh430/snip.svg?style=social&label=Star) | 2017.11.22 |          |              |
| Cascade R-CNN                                                | Cascade R-CNN: Delving into High Quality Object Detection    | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1712.00726v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/zhaoweicai/cascade-rcnn)![Star](https://img.shields.io/github/stars/zhaoweicai/cascade-rcnn.svg?style=social&label=Star) | 2017.12.03 |          | CVPR 2018    |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
| PANet                                                        | Path Aggregation Network for Instance Segmentation           | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1803.01534v4) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/ShuLiu1993/PANet)![Star](https://img.shields.io/github/stars/ShuLiu1993/PANet.svg?style=social&label=Star) | 2018.03.05 |          | CVPR 2018    |
| YOLOv3 + Darknet-53                                          | YOLOv3: An Incremental Improvement                           | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1811.12030v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/pjreddie/darknet)![Star](https://img.shields.io/github/stars/pjreddie/darknet.svg?style=social&label=Star) | 2018.04.08 |          |              |
| SNIPER                                                       | SNIPER: Efficient Multi-Scale Training                       | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1805.09300v3) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/MahyarNajibi/SNIPER)![Star](https://img.shields.io/github/stars/MahyarNajibi/SNIPER.svg?style=social&label=Star) | 2018.05.23 |          | CVPR 2018    |
| IoU-Net                                                      | Acquisition of Localization Confidence for Accurate Object Detection | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1807.11590v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/vacancy/PreciseRoIPooling)![Star](https://img.shields.io/github/stars/vacancy/PreciseRoIPooling.svg?style=social&label=Star) | 2018.07.30 |          | ECCV 2018    |
| CornerNet                                                    | CornerNet: Detecting Objects as Paired Keypoints             | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1811.12030v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/STVIR/Grid-R-CNN)![Star](https://img.shields.io/github/stars/STVIR/Grid-R-CNN.svg?style=social&label=Star) | 2018.08.03 |          | ECCV 2018    |
| ResNet-50-FPN Mask R-CNN + KL Loss + var voting + soft-NMS   | Bounding Box Regression with Uncertainty for Accurate Object Detection | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1809.08545v3) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/yihui-he/KL-Loss)![Star](https://img.shields.io/github/stars/yihui-he/KL-Loss.svg?style=social&label=Star) | 2018.09.23 |          | CVPR 2019    |
| M2Det                                                        | M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1811.04533v3) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/qijiezhao/M2Det)![Star](https://img.shields.io/github/stars/qijiezhao/M2Det.svg?style=social&label=Star) | 2018.11.12 |          |              |
| GHM-C + GHM-R                                                | Gradient Harmonized Single-stage Detector                    | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1811.05181v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/libuyu/GHM_Detection)![Star](https://img.shields.io/github/stars/libuyu/GHM_Detection.svg?style=social&label=Star) | 2018.11.13 |          |              |
| DCNv2                                                        | Deformable ConvNets v2: More Deformable, Better Results      | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1811.11168v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch)![Star](https://img.shields.io/github/stars/chengdazhi/Deformable-Convolution-V2-PyTorch.svg?style=social&label=Star) | 2018.11.27 |          | CVPR 2019    |
| Grid R-CNN                                                   | Grid R-CNN                                                   | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1811.12030v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/STVIR/Grid-R-CNN)![Star](https://img.shields.io/github/stars/STVIR/Grid-R-CNN.svg?style=social&label=Star) | 2018.11.29 |          | CVPR 2019    |
| ESPNetv2                                                     | ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1811.11431v3) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/sacmehta/ESPNetv2)![Star](https://img.shields.io/github/stars/sacmehta/ESPNetv2.svg?style=social&label=Star) | 2018.11.28 |          | CVPR 2019    |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
| TridentNet<br/>(ResNet-101-Deformable, Image Pyramid)        | Scale-Aware Trident Networks for Object Detection            | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1901.01892v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/tusen-ai/simpledet/tree/master/models/tridentnet)![Star](https://img.shields.io/github/stars/tusen-ai/simpledet.svg?style=social&label=Star) | 2019.01.07 |          | ICCV 2019    |
| RetinaMask (ResNeXt-101-FPN-GN)                              | RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1901.03353v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/chengyangfu/retinamask)![Star](https://img.shields.io/github/stars/chengyangfu/retinamask.svg?style=social&label=Star) | 2019.01.10 |          |              |
| HTC<br/>(ResNeXt-101-FPN)                                    | Hybrid Task Cascade for Instance Segmentation                | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)]([ Abstract](https://arxiv.org/abs/1901.07518v2)) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/open-mmlab/mmdetection)![Star](https://img.shields.io/github/stars/open-mmlab/mmdetection.svg?style=social&label=Star) | 2019.01.22 |          | CVPR 2019    |
| ExtremeNet (Hourglass-104, multi-scale)                      | Bottom-up Object Detection by Grouping Extreme and Center Points | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1901.08043v3) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/xingyizhou/ExtremeNet)![Star](https://img.shields.io/github/stars/xingyizhou/ExtremeNet.svg?style=social&label=Star) | 2019.01.23 |          | CVPR 2019    |
| FSAF<br/>(ResNeXt-101, multi-scale)                          | Feature Selective Anchor-Free Module for Single-Shot Object Detection | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1903.00621v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/xuannianz/FSAF)![Star](https://img.shields.io/github/stars/xuannianz/FSAF.svg?style=social&label=Star) | 2019.03.02 |          | CVPR 2019    |
| InterNet<br/>(ResNet-101-FPN, multi-scale)                   | Feature Intertwiner for Object Detection                     | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1911.09070v7) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/hli2020/feature_intertwiner)![Star](https://img.shields.io/github/stars/hli2020/feature_intertwiner.svg?style=social&label=Star) | 2019.03.28 |          | ICLR 2019    |
| FCOS<br/>(ResNeXt-64x4d-101-FPN 4 + improvements)            | FCOS: Fully Convolutional One-Stage Object Detection         | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1904.01355v5) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/tianzhi0549/FCOS)![Star](https://img.shields.io/github/stars/tianzhi0549/FCOS.svg?style=social&label=Star) | 2019.04.02 |          | ICCV 2019    |
| Libra R-CNN<br/>(ResNeXt-101-FPN)                            | Libra R-CNN: Towards Balanced Learning for Object Detection  | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1904.02701v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/OceanPang/Libra_R-CNN)![Star](https://img.shields.io/github/stars/OceanPang/Libra_R-CNN.svg?style=social&label=Star) | 2019.04.04 |          | CVPR 2019    |
| GCNet<br/>(ResNeXt-101 + DCN + cascade + GC r4)              | GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1904.11492v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/xvjiarui/GCNet)![Star](https://img.shields.io/github/stars/xvjiarui/GCNet.svg?style=social&label=Star) | 2019.04.05 |          |              |
| FoveaBox<br/>(ResNeXt-101)                                   | FoveaBox: Beyond Anchor-based Object Detector                | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1904.03797v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/taokong/FoveaBox )![Star](https://img.shields.io/github/stars/taokong/FoveaBox.svg?style=social&label=Star) | 2019.04.08 |          |              |
| CenterNet-DLA<br/>(DLA-34, multi-scale)                      | Objects as Points                                            | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1904.07850v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/xingyizhou/CenterNet)![Star](https://img.shields.io/github/stars/xingyizhou/CenterNet.svg?style=social&label=Star) | 2019.04.16 |          |              |
| CenterNet511<br/>(Hourglass-104, multi-scale)                | CenterNet: Keypoint Triplets for Object Detection            | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1904.08189v3) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/Duankaiwen/CenterNet)![Star](https://img.shields.io/github/stars/Duankaiwen/CenterNet.svg?style=social&label=Star) | 2019.04.17 |          | ICCV 2019    |
| CornerNet-Saccade (Hourglass-104, multi-scale)               | CornerNet-Lite: Efficient Keypoint Based Object Detection    | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1904.08900v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/princeton-vl/CornerNet-Lite)![Star](https://img.shields.io/github/stars/princeton-vl/CornerNet-Lite.svg?style=social&label=Star) | 2019.04.18 |          |              |
| AA-ResNet-10 + RetinaNet]                                    | Attention Augmented Convolutional Networks                   | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1904.09925v5) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/leaderj1001/Attention-Augmented-Conv2d)![Star](https://img.shields.io/github/stars/leaderj1001/Attention-Augmented-Conv2d.svg?style=social&label=Star) | 2019.04.22 |          | ICCV 2023    |
| RPDet<br/>(ResNet-101-DCN, multi-scale)                      | RepPoints: Point Set Representation for Object Detection     | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1904.11490v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/RepPoints)![Star](https://img.shields.io/github/stars/microsoft/RepPoints.svg?style=social&label=Star) | 2019.04.25 |          | ICCV 2019    |
| WSMA-Seg                                                     | Segmentation is All You Need                                 | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1904.13300v3) | -                                                            | 2019.04.30 |          |              |
| HSD<br/>(Rest101, 768x768, single-scale test)                | Hierarchical Shot Detector                                   | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](http://openaccess.thecvf.com/content_ICCV_2019/html/Cao_Hierarchical_Shot_Detector_ICCV_2019_paper.html) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/JialeCao001/HSD)![Star](https://img.shields.io/github/stars/JialeCao001/HSD.svg?style=social&label=Star) | 2019.      |          | ICCV 2019    |
| FreeAnchor<br/>(ResNeXt-101)                                 | FreeAnchor: Learning to Match Anchors for Visual Object Detection | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1909.02466v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/zhangxiaosong18/FreeAnchor)![Star](https://img.shields.io/github/stars/zhangxiaosong18/FreeAnchor.svg?style=social&label=Star) | 2019.05.19 |          | NeurIPS 2019 |
| ResNeXt-64x4d-101 NAS-FCOS @128-256 w/improvements           | NAS-FCOS: Fast Neural Architecture Search for Object Detection | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1906.04423v4) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/Lausannen/NAS-FCOS)![Star](https://img.shields.io/github/stars/Lausannen/NAS-FCOS.svg?style=social&label=Star) | 2019.06.11 |          | CVPR 2020    |
| Cascade R-CNN                                                | Cascade R-CNN: High Quality Object Detection and Instance Segmentation | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1906.09756v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/zhaoweicai/cascade-rcnn)![Star](https://img.shields.io/github/stars/zhaoweicai/cascade-rcnn.svg?style=social&label=Star) | 2019.06.24 |          |              |
| NAS-FPN<br/>(AmoebaNet-D, learned aug)                       | Learning Data Augmentation Strategies for Object Detection   | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1906.11172v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/tensorflow/tpu/tree/master/models/official/detection)![Star](https://img.shields.io/github/stars/tensorflow/tpu.svg?style=social&label=Star) | 2019.06.26 |          | ECCV 2020    |
| Faster R-CNN + FPN + CGD                                     | Compact Global Descriptor for Neural Networks                | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1907.09665v10) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/HolmesShuan/Compact-Global-Descriptor)![Star](https://img.shields.io/github/stars/HolmesShuan/Compact-Global-Descriptor.svg?style=social&label=Star) | 2019.07.23 |          |              |
| Faster R-CNN<br/>(LIP-ResNet-101-MD w FPN)                   | LIP: Local Importance-based Pooling                          | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1908.04156v3) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/sebgao/LIP)![Star](https://img.shields.io/github/stars/sebgao/LIP.svg?style=social&label=Star) | 2019.08.12 |          | ICCV 2019    |
| MatrixNet Corners<br/>(ResNet-152, multi-scale)              | Matrix Nets: A New Deep Architecture for Object Detection    | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1908.04646v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/arashwan/matrixnet)![Star](https://img.shields.io/github/stars/arashwan/matrixnet.svg?style=social&label=Star) | 2019.08.13 |          |              |
| HTC<br/>(HRNetV2p-W48)                                       | Deep High-Resolution Representation Learning for Visual Recognition | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1908.07919v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/HRNet) | 2019.08.20 |          |              |
| Cascade R-CNN-FPN<br/>(ResNet-101, map-guided)               | InstaBoost: Boosting Instance Segmentation via Probability Map Guided Copy-Pasting | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1908.07801v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/GothicAi/InstaBoost)![Star](https://img.shields.io/github/stars/GothicAi/InstaBoost.svg?style=social&label=Star) | 2019.08.21 |          | ICCV 2019    |
| Cascade Mask R-CNN                                           | CBNet: A Novel Composite Backbone Network Architecture for Object Detection | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1909.03625v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/PKUbahuangliuhe/CBNet)![Star](https://img.shields.io/github/stars/PKUbahuangliuhe/CBNet.svg?style=social&label=Star) | 2019.09.09 |          |              |
| Faster R-CNN<br/>(Cascade RPN)                               | Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1909.06720v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/thangvubk/Cascade-RPN)![Star](https://img.shields.io/github/stars/thangvubk/Cascade-RPN.svg?style=social&label=Star) | 2019.09.15 |          | NeurIPS 2019 |
| IoU-Net+EnergyRegression                                     | Energy-Based Models for Deep Probabilistic Regression        | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1909.12297v4) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/fregu856/ebms_regression)![Star](https://img.shields.io/github/stars/fregu856/ebms_regression.svg?style=social&label=Star) | 2019.09.26 |          | ECCV 2020    |
| ResNet-50-DW-DPN<br/>(Deformable Kernels)                    | Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1910.02940v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/hangg7/deformable-kernels/)![Star](https://img.shields.io/github/stars/hangg7/deformable-kernels.svg?style=social&label=Star) | 2019.10.07 |          | ICLR 2020    |
| CenterMask+VoVNetV2-99<br/>(single-scale)                    | CenterMask : Real-Time Anchor-Free Instance Segmentation     | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1911.09070v7) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/youngwanLEE/CenterMask)![Star](https://img.shields.io/github/stars/youngwanLEE/CenterMask.svg?style=social&label=Star) | 2019.11.15 |          |              |
| EfficientDet                                                 | EfficientDet: Scalable and Efficient Object Detection        | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1911.09070v7) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google/automl/tree/master/efficientdet)![Star](https://img.shields.io/github/stars/google/automl.svg?style=social&label=Star) | 2019.11.20 |          | CVPR 2020    |
| YOLOv3 @800 + ASFF*<br/>(Darknet-53)                         | Learning Spatial Fusion for Single-Shot Object Detection]    | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1911.09516v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/ruinmessi/ASFF)![Star](https://img.shields.io/github/stars/ruinmessi/ASFF.svg?style=social&label=Star) | 2019.11.21 |          |              |
| SAPD (ResNeXt-101, single-scale)                             | Soft Anchor-Point Object Detection                           | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1911.12448v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/xuannianz/FSAF)![Star](https://img.shields.io/github/stars/xuannianz/FSAF.svg?style=social&label=Star) | 2019.11.27 |          |              |
| MnasFPN<br/>(MobileNetV2)                                    | MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1912.01106v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/tensorflow/models)![Star](https://img.shields.io/github/stars/tensorflow/models.svg?style=social&label=Star) | 2019.12.02 |          | CVPR 2020    |
| MAL<br/>(ResNeXt101, multi-scale)                            | Multiple Anchor Learning for Visual Object Detection         | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1912.02252v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/KevinKecc/MAL)![Star](https://img.shields.io/github/stars/KevinKecc/MAL.svg?style=social&label=Star) | 2019.12.04 |          |              |
| ATSS<br/>(ResNetXt-64x4d-101+DCN,multi-scale)                | Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1912.02424v4) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/sfzhang15/ATSS)![Star](https://img.shields.io/github/stars/sfzhang15/ATSS.svg?style=social&label=Star) | 2019.12.05 |          | CVPR 2020    |
| RetinaNet<br/>(SpineNet-190, 1280x1280)                      | SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1911.09070v7) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/tensorflow/tpu/tree/master/models/official/detection)![Star](https://img.shields.io/github/stars/tensorflow/tpu.svg?style=social&label=Star) | 2019.12.10 |          | CVPR 2020    |
| RDSNet<br/>(ResNet-101, RetinaNet, mask, MBRM)               | RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1912.05070v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/wangsr126/RDSNet)![Star](https://img.shields.io/github/stars/wangsr126/RDSNet.svg?style=social&label=Star) | 2019.12.11 |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
| TSD (SENet154-DCN,multi-scale)                               | Revisiting the Sibling Head in Object Detector               | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2003.07540v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/Sense-X/TSD)![Star](https://img.shields.io/github/stars/Sense-X/TSD.svg?style=social&label=Star) | 2020.03.17 |          | CVPR 2020    |
| ResNeSt-200<br/>(multi-scale)                                | ResNeSt: Split-Attention Networks                            | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2004.08955v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/zhanghang1989/ResNeSt)![Star](https://img.shields.io/github/stars/zhanghang1989/ResNeSt.svg?style=social&label=Star) | 2020.04.19 |          |              |
| FreeAnchor + SEPC<br/>(DCN, ResNext-101-64x4d)               | Scale-Equalizing Pyramid Convolution for Object Detection    | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2005.03101v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/jshilong/SEPC)![Star](https://img.shields.io/github/stars/jshilong/SEPC.svg?style=social&label=Star) | 2020.05.06 |          | CVPR 2020    |
| AC-FPN Cascade R-CNN (ResNet-101, single scale)              | Attention-guided Context Feature Pyramid Network for Object Detection | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2005.11475v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/tensorflow/tpu/tree/master/models/official/detection)![Star](https://img.shields.io/github/stars/tensorflow/tpu.svg?style=social&label=Star) | 2020.05.23 |          |              |
| DetectoRS<br/>(ResNeXt-101-64x4d, multi-scale)               | DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2006.02334v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/joe-siyuan-qiao/DetectoRS)![Star](https://img.shields.io/github/stars/joe-siyuan-qiao/DetectoRS.svg?style=social&label=Star) | 2020.06.03 |          | CVPR 2021    |
| SpineNet-190<br/>(1280, with Self-training on OpenImages, single-scale) | Rethinking Pre-training and Self-training                    | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2006.06882v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/self_training)![Star](https://img.shields.io/github/stars/tensorflow/tpu.svg?style=social&label=Star) | 2020.06.11 |          | NeurIPS 2020 |
| PAA<br/>(ResNext-152-32x8d + DCN, multi-scale)               | Probabilistic Anchor Assignment with IoU Prediction for Object Detection | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)]([ 抽象的](https://arxiv.org/abs/2007.08103v2)) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/kkhoot/PAA)![Star](https://img.shields.io/github/stars/kkhoot/PAA.svg?style=social&label=Star) | 2020.07.16 |          | ECCV 2020    |
| RepPoints v2<br/>(ResNeXt-101, DCN, multi-scale)             | RepPoints V2: Verification Meets Regression for Object Detection | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2007.08508v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/Scalsol/RepPointsV2)![Star](https://img.shields.io/github/stars/Scalsol/RepPointsV2.svg?style=social&label=Star) | 2020.07.16 |          | NeurIPS 2020 |
| aLRP Loss<br/>(ResNext-101-64x4d, DCN, multiscale test)      | A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2009.13592v4) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/kemaloksuz/aLRPLoss)![Star](https://img.shields.io/github/stars/kemaloksuz/aLRPLoss.svg?style=social&label=Star) | 2020.09.28 |          | NeurIPS 2020 |
| Deformable DETR                                              | Deformable DETR: Deformable Transformers for End-to-End Object Detection | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2010.04159v4) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/fundamentalvision/Deformable-DETR)![Star](https://img.shields.io/github/stars/fundamentalvision/Deformable-DETR.svg?style=social&label=Star) | 2020.10.08 |          | ICLR 2021    |
| RelationNet++ (ResNeXt-64x4d-101-DCN)                        | RelationNet++: Bridging Visual Representations for Object Detection via Transformer Decoder | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2010.15831v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/RelationNet2)![Star](https://img.shields.io/github/stars/microsoft/RelationNet2.svg?style=social&label=Star) | 2020.10.29 |          | NeurIPS 2020 |
| Cascade Eff-B7 NAS-FPN<br/>(1280, self-training Copy Paste, single-scale) | Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2012.07177v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/conradry/copy-paste-aug)![Star](https://img.shields.io/github/stars/conradry/copy-paste-aug.svg?style=social&label=Star) | 2020.12.13 |          | CVPR 2021    |
| YOLOv4-P7 with TTA                                           | Scaled-YOLOv4: Scaling Cross Stage Partial Network           | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2011.08036v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/WongKinYiu/ScaledYOLOv4)![Star](https://img.shields.io/github/stars/WongKinYiu/ScaledYOLOv4.svg?style=social&label=Star) | 2020.11.16 |          | CVPR 2021    |
| GFLV2<br/>(Res2Net-101, DCN, multiscale)                     | Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2011.12885v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/implus/GFocalV2)![Star](https://img.shields.io/github/stars/implus/GFocalV2.svg?style=social&label=Star) | 2020.11.25 |          | CVPR 2021    |
| GCNet<br/>(ResNeXt-101 + DCN + cascade + GC r4)              | Global Context Networks                                      | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2012.13375v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/xvjiarui/GCNet)![Star](https://img.shields.io/github/stars/xvjiarui/GCNet.svg?style=social&label=Star) | 2020.12.24 |          |              |
|                                                              | D2Det: Towards High Quality Object Detection and Instance Segmentation |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
| PAFNet (ResNet50-vd)                                         | PAFNet: An Efficient Anchor-Free Object Detector Guidance    | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2104.13534v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/PaddlePaddle/PaddleDetection)![Star](https://img.shields.io/github/stars/PaddlePaddle/PaddleDetection.svg?style=social&label=Star) | 2021.04.28 |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |
|                                                              |                                                              |                                                              |                                                              |            |          |              |

## Open-Vocabulary-Object-Detection

### on-MSCOCO-Benchmark

| Model / Methods                  | Title                                                        | Paper Link                                                   | Code Link                                                    | Published | AP(0.5⬆️)  coco | Keywords  | Venue     |
| -------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | --------- | -------------- | --------- | --------- |
| Cooperative Foundational Models  | Enhancing Novel Object Detection via Cooperative Foundational Models | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2311.12068v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/rohit901/cooperative-foundational-models)![Star](https://img.shields.io/github/stars/rohit901/cooperative-foundational-models.svg?style=social&label=Star) | 2023      | 50.3           |           |           |
| DE-ViT                           | Detect Everything with Few Examples                          | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2309.12969v3) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mlzxy/devit)![Star](https://img.shields.io/github/stars/mlzxy/devit.svg?style=social&label=Star) | 2023      | 50             |           |           |
| DITO                             | Region-centric Image-Language Pretraining for Open-Vocabulary Detection | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2310.00161v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/google-research/tree/master/fvlm/dito)![Star](https://img.shields.io/github/stars/google-research/google-research.svg?style=social&label=Star) | 2023      | 46.1           |           |           |
| OV-DQUO<br />(RN50x4)            | OV-DQUO: Open-Vocabulary DETR with Denoising Text Query Training and Open-World Unknown Objects Supervision | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2405.17913v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/xiaomoguhz/ov-dquo)![Star](https://img.shields.io/github/stars/xiaomoguhz/ov-dquo.svg?style=social&label=Star) | 2024      | 45.6           |           |           |
| LP-OVOD<br />(OWL-ViT Proposals) | LP-OVOD: Open-Vocabulary Object Detection by Linear Probing  | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2310.17109v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/vinairesearch/lp-ovod)![Star](https://img.shields.io/github/stars/vinairesearch/lp-ovod.svg?style=social&label=Star) | 2023      | 44.9           |           |           |
| CLIPSelf                         | CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2310.01403v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/wusize/clipself)![Star](https://img.shields.io/github/stars/wusize/clipself.svg?style=social&label=Star) | 2023      | 44.3           |           |           |
| CORA+                            | CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2303.13076v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/tgxs002/cora)![Star](https://img.shields.io/github/stars/tgxs002/cora.svg?style=social&label=Star) | 2023      | 43.1           |           | CVPR 2023 |
| BARON                            | Aligning Bag of Regions for Open-Vocabulary Object Detection | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2302.13996v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/wusize/ovdet)![Star](https://img.shields.io/github/stars/wusize/ovdet.svg?style=social&label=Star) | 2023      | 42.7           |           | CVPR 2023 |
| CORA                             | CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2303.13076v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/tgxs002/cora)![Star](https://img.shields.io/github/stars/tgxs002/cora.svg?style=social&label=Star) | 2023      | 41.7           |           | CVPR 2023 |
| RALF                             | Retrieval-Augmented Open-Vocabulary Object Detection         | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](http://openaccess.thecvf.com//content/CVPR2024/papers/Kim_Retrieval-Augmented_Open-Vocabulary_Object_Detection_CVPR_2024_paper.pdf) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mlvlab/RALF)![Star](https://img.shields.io/github/stars/mlvlab/RALF.svg?style=social&label=Star) | 2024      | 41.3           |           | CVPR 2024 |
| LP-OVOD                          | LP-OVOD: Open-Vocabulary Object Detection by Linear Probing  | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2310.17109v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/vinairesearch/lp-ovod)![Star](https://img.shields.io/github/stars/vinairesearch/lp-ovod.svg?style=social&label=Star) | 2023      | 40.5           |           |           |
| Region-CLIP<br />(RN50x4-C4)     | RegionCLIP: Region-based Language-Image Pretraining          | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](http://openaccess.thecvf.com//content/CVPR2022/html/Zhong_RegionCLIP_Region-Based_Language-Image_Pretraining_CVPR_2022_paper.html) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/regionclip)![Star](https://img.shields.io/github/stars/microsoft/regionclip.svg?style=social&label=Star) | 2021      | 39.3           | microsoft | CVPR 2022 |
| OV-DQUO<br/>(R50)                | OV-DQUO: Open-Vocabulary DETR with Denoising Text Query Training and Open-World Unknown Objects Supervision | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2405.17913v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/xiaomoguhz/ov-dquo)![Star](https://img.shields.io/github/stars/xiaomoguhz/ov-dquo.svg?style=social&label=Star) | 2024      | 39.2           |           |           |
| Object-Centric-OVD               | Bridging the Gap between Object and Image-level Representations for Open-Vocabulary Detection | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2207.03482v3) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mmaaz60/mvits_for_class_agnostic_od)![Star](https://img.shields.io/github/stars/mmaaz60/mvits_for_class_agnostic_od.svg?style=social&label=Star) | 2022      | 36.9           |           | ECCV 2022 |
| CLIM<br/>(RN50)                  | CLIM: Contrastive Language-Image Mosaic for Region Representation | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2312.11376v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/wusize/clim)![Star](https://img.shields.io/github/stars/wusize/clim.svg?style=social&label=Star) | 2023      | 36.9           |           | AAAI 2024 |
| OADP<br/>(G-OVD)]                | Object-Aware Distillation Pyramid for Open-Vocabulary Object Detection | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](http://openaccess.thecvf.com//content/CVPR2023/html/Wang_Object-Aware_Distillation_Pyramid_for_Open-Vocabulary_Object_Detection_CVPR_2023_paper.html) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/lutingwang/oadp)![Star](https://img.shields.io/github/stars/lutingwang/oadp.svg?style=social&label=Star) | 2023      | 35.6           |           | CVPR 2023 |
| VL-PLM<br/>(RN50)                | Exploiting Unlabeled Data with Vision and Language Models for Object Detection | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2207.08954v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/xiaofeng94/vl-plm)![Star](https://img.shields.io/github/stars/xiaofeng94/vl-plm.svg?style=social&label=Star) | 2022      | 34.4           |           |           |
| CFM-ViT                          | Contrastive Feature Masking Open-Vocabulary Vision Transformer | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://paperswithcode.com/paper/contrastive-feature-masking-open-vocabulary) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)]() | 2023      | 34.1           |           | ICCV 2023 |
| MEDet<br/>(RN50)                 | Open Vocabulary Object Detection with Proposal Mining and Prediction Equalization | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2206.11134v4) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/peixianchen/medet)![Star](https://img.shields.io/github/stars/peixianchen/medet.svg?style=social&label=Star) | 2022      | 32.6           |           |           |
| Region-CLIP<br/>(RN50-C4)        | RegionCLIP: Region-based Language-Image Pretraining          | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](http://openaccess.thecvf.com//content/CVPR2022/html/Zhong_RegionCLIP_Region-Based_Language-Image_Pretraining_CVPR_2022_paper.html) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/regionclip)![Star](https://img.shields.io/github/stars/microsoft/regionclip.svg?style=social&label=Star) | 2022      | 31.4           |           | CVPR 2022 |
| OVAD-Baseline                    | Open-vocabulary Attribute Detection                          | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](http://openaccess.thecvf.com//content/CVPR2023/html/Bravo_Open-Vocabulary_Attribute_Detection_CVPR_2023_paper.html) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/OVAD-Benchmark/ovad-bechmark-code)![Star](https://img.shields.io/github/stars/OVAD-Benchmark/ovad-bechmark-code.svg?style=social&label=Star) | 2022      | 30.0           |           | CVPR 2023 |
| OADP                             | Object-Aware Distillation Pyramid for Open-Vocabulary Object Detection | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](http://openaccess.thecvf.com//content/CVPR2023/html/Wang_Object-Aware_Distillation_Pyramid_for_Open-Vocabulary_Object_Detection_CVPR_2023_paper.html) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/lutingwang/oadp)![Star](https://img.shields.io/github/stars/lutingwang/oadp.svg?style=social&label=Star) | 2023      | 30.0           |           | CVPR 2023 |
| OV-DERT                          | Open-Vocabulary DETR with Conditional Matching               | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2203.11876v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/yuhangzang/ov-detr)![Star](https://img.shields.io/github/stars/yuhangzang/ov-detr.svg?style=social&label=Star) | 2022      | 29.4           |           |           |
| LocOv<br/>(RN50-C4)]             | Localized Vision-Language Matching for Open-vocabulary Object Detection | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2205.06160v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/lmb-freiburg/locov)![Star](https://img.shields.io/github/stars/lmb-freiburg/locov.svg?style=social&label=Star) | 2022      | 28.6           |           |           |
| Detic                            | Detecting Twenty-thousand Classes using Image-level Supervision | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2201.02605v3) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/Detic)![Star](https://img.shields.io/github/stars/facebookresearch/Detic.svg?style=social&label=Star) | 2022      | 27.8           |           |           |
| ViLD                             | Open-vocabulary Object Detection via Vision and Language Knowledge Distillation | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://openreview.net/forum?id=lL3lnMbR4WU) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://colab.research.google.com/drive/19LBqQg0cS36rTLL_TaXZ7Ka9KJGkxiSe?usp=sharing) | 2021      | 27.6           |           | ICIR 2022 |
| OVR-CNN                          | Open-Vocabulary Object Detection Using Captions              | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](http://openaccess.thecvf.com//content/CVPR2021/html/Zareian_Open-Vocabulary_Object_Detection_Using_Captions_CVPR_2021_paper.html) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/alirezazareian/ovr-cnn)![Star](https://img.shields.io/github/stars/alirezazareian/ovr-cnn.svg?style=social&label=Star) | 2021      | 22.8           |           | CVPR 2021 |
| HierKD                           | Open-Vocabulary One-Stage Detection with Hierarchical Visual-Language Knowledge Distillation | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](http://openaccess.thecvf.com//content/CVPR2022/html/Ma_Open-Vocabulary_One-Stage_Detection_With_Hierarchical_Visual-Language_Knowledge_Distillation_CVPR_2022_paper.html) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mengqidyangge/hierkd)![Star](https://img.shields.io/github/stars/mengqidyangge/hierkd.svg?style=social&label=Star) | 2022      | 20.3           |           | CVPR 2022 |
|                                  |                                                              |                                                              |                                                              |           |                |           |           |

### on-LVIS-v1.0-Benchmark

| Model / Methods                      | Title                                                        | Paper Link                                                   | Code Link                                                    | Published | AP(0.5⬆️)lvis | Keywords | Venue                                                        |
| ------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | --------- | ------------ | -------- | ------------------------------------------------------------ |
| LaMI-DETR                            | LaMI-DETR: Open-Vocabulary Detection with Language Model Instruction | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2407.11335v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/eternaldolphin/lami-detr)![Star](https://img.shields.io/github/stars/eternaldolphin/lami-detr.svg?style=social&label=Star) | 2024      | 43.4         |          | ECCV2024                                                     |
| DITO                                 | Region-centric Image-Language Pretraining for Open-Vocabulary Detection | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2310.00161v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/google-research)![Star](https://img.shields.io/github/stars/google-research/google-research.svg?style=social&label=Star) | 2023      | 40.4         |          |                                                              |
| OV-DQUO<br/>(ViT-L/14)               | OV-DQUO: Open-Vocabulary DETR with Denoising Text Query Training and Open-World Unknown Objects Supervision | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2405.17913v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/xiaomoguhz/ov-dquo)![Star](https://img.shields.io/github/stars/xiaomoguhz/ov-dquo.svg?style=social&label=Star) | 2024      | 39.3         |          |                                                              |
| CoDet<br/>(EVA02-L)                  | CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2310.16667v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/cvmi-lab/codet)![Star](https://img.shields.io/github/stars/cvmi-lab/codet.svg?style=social&label=Star) | 2023      | 37.0         |          |                                                              |
| CLIPSelf                             | CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2310.01403v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/wusize/clipself)![Star](https://img.shields.io/github/stars/wusize/clipself.svg?style=social&label=Star) | 2023      | 34.9         |          |                                                              |
| DE-ViT                               | Detect Everything with Few Examples                          | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2309.12969v3) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mlzxy/devit)![Star](https://img.shields.io/github/stars/mlzxy/devit.svg?style=social&label=Star) | 2023      | 34.3         |          |                                                              |
| CFM-ViT                              | Contrastive Feature Masking Open-Vocabulary Vision Transformer | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2309.00775v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)]() | 2023      | 33.9         |          | [ICCV 2023 ](https://paperswithcode.com/conference/iccv-2023-1) |
| CLIM<br/>(RN50x64)                   | CLIM: Contrastive Language-Image Mosaic for Region Representation | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2312.11376v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/wusize/clim)![Star](https://img.shields.io/github/stars/wusize/clim.svg?style=social&label=Star) | 2023      | 32.3         |          |                                                              |
| RO-ViT                               | Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2305.07011v4) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mcahny/rovit)![Star](https://img.shields.io/github/stars/mcahny/rovit.svg?style=social&label=Star) | 2023      | 32.1         |          |                                                              |
| RTGen                                | Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2405.19854v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/seermer/RTGen)![Star](https://img.shields.io/github/stars/seermer/RTGen.svg?style=social&label=Star) | 2024      | 30.2         |          |                                                              |
| OV-DQUO<br/>(ViT-B/16)               | OV-DQUO: Open-Vocabulary DETR with Denoising Text Query Training and Open-World Unknown Objects Supervision | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2405.17913v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/xiaomoguhz/ov-dquo)![Star](https://img.shields.io/github/stars/xiaomoguhz/ov-dquo.svg?style=social&label=Star) | 2024      | 29.7         |          |                                                              |
| ViLD-ensemble w/ ALIGN<br/>(Eb7-FPN) | Open-vocabulary Object Detection via Vision and Language Knowledge Distillation | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2104.13921v3) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild)![Star](https://img.shields.io/github/stars/tensorflow/tpu.svg?style=social&label=Star) | 2021      | 26.3         |          |                                                              |
| OWL-ViT<br/>(CLIP-L/14)              | Simple Open-Vocabulary Object Detection with Vision Transformers | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2205.06230v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit)![Star](https://img.shields.io/github/stars/google-research/scenic.svg?style=social&label=Star) | 2022      | 25.6         |          |                                                              |
| POMP                                 | Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2205.06230v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit)![Star](https://img.shields.io/github/stars/google-research/scenic.svg?style=social&label=Star) | 2023      | 25.2         |          |                                                              |
| BARON                                | Aligning Bag of Regions for Open-Vocabulary Object Detection | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2302.13996v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/wusize/ovdet)![Star](https://img.shields.io/github/stars/wusize/ovdet.svg?style=social&label=Star) | 2023      | 22.6         |          | CVPR 2023                                                    |
| MEDet                                | Open Vocabulary Object Detection with Proposal Mining and Prediction Equalization | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2206.11134v4) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/peixianchen/medet)![Star](https://img.shields.io/github/stars/peixianchen/medet.svg?style=social&label=Star) | 2022      | 22.4         |          |                                                              |
| Region-CLIP<br/>(RN50x4-C4)          | RegionCLIP: Region-based Language-Image Pretraining          | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](http://openaccess.thecvf.com//content/CVPR2022/html/Zhong_RegionCLIP_Region-Based_Language-Image_Pretraining_CVPR_2022_paper.html) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/regionclip)![Star](https://img.shields.io/github/stars/microsoft/regionclip.svg?style=social&label=Star) | 2021      | 22.0         |          | CVPR 2022                                                    |
| RALF                                 | Retrieval-Augmented Open-Vocabulary Object Detection         | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](http://openaccess.thecvf.com//content/CVPR2024/html/Kim_Retrieval-Augmented_Open-Vocabulary_Object_Detection_CVPR_2024_paper.html) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mlvlab/RALF)![Star](https://img.shields.io/github/stars/mlvlab/RALF.svg?style=social&label=Star) | 2024      | 21.9         |          | CVPR 2024                                                    |
| OADP                                 | Object-Aware Distillation Pyramid for Open-Vocabulary Object Detection | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](http://openaccess.thecvf.com//content/CVPR2023/html/Wang_Object-Aware_Distillation_Pyramid_for_Open-Vocabulary_Object_Detection_CVPR_2023_paper.html) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/lutingwang/oadp)![Star](https://img.shields.io/github/stars/lutingwang/oadp.svg?style=social&label=Star) | 2023      | 21.7         |          | CVPR 2023                                                    |
| X-Paste                              | X-Paste: Revisiting Scalable Copy-Paste for Instance Segmentation using CLIP and StableDiffusion | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2212.03863v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/yoctta/xpaste)![Star](https://img.shields.io/github/stars/yoctta/xpaste.svg?style=social&label=Star) | 2022      | 21.4         |          |                                                              |
| Object-Centric-OVD                   | Bridging the Gap between Object and Image-level Representations for Open-Vocabulary Detection | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2207.03482v3) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mmaaz60/mvits_for_class_agnostic_od)![Star](https://img.shields.io/github/stars/mmaaz60/mvits_for_class_agnostic_od.svg?style=social&label=Star) | 2022      | 21.1         |          |                                                              |
| ViLD-ensemble<br/>(R152-FPN)         | Open-vocabulary Object Detection via Vision and Language Knowledge Distillation | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2207.03482v3) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mmaaz60/mvits_for_class_agnostic_od)![Star](https://img.shields.io/github/stars/mmaaz60/mvits_for_class_agnostic_od.svg?style=social&label=Star) | 2021      | 18.7         |          |                                                              |
| Detic                                | Detecting Twenty-thousand Classes using Image-level Supervision | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2201.02605v3) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/Detic)![Star](https://img.shields.io/github/stars/facebookresearch/Detic.svg?style=social&label=Star) | 2022      | 17.8         |          |                                                              |
| Region-CLIP<br/>(RN50-C4)            | Detecting Twenty-thousand Classes using Image-level Supervision | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](http://openaccess.thecvf.com//content/CVPR2022/html/Zhong_RegionCLIP_Region-Based_Language-Image_Pretraining_CVPR_2022_paper.html) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/regionclip)![Star](https://img.shields.io/github/stars/microsoft/regionclip.svg?style=social&label=Star) | 2021      | 17.1         |          |                                                              |
| ViLD-ensemble<br/>(R50-FPN)          | Open-vocabulary Object Detection via Vision and Language Knowledge Distillation | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://openreview.net/forum?id=lL3lnMbR4WU) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/tensorflow/tpu)![Star](https://img.shields.io/github/stars/tensorflow/tpu.svg?style=social&label=Star) | 2021      | 16.6         |          | ICLR  2023                                                   |
| ViLD<br/>(R50-FPN)                   | Open-vocabulary Object Detection via Vision and Language Knowledge Distillation | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://openreview.net/forum?id=lL3lnMbR4WU) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/tensorflow/tpu)![Star](https://img.shields.io/github/stars/tensorflow/tpu.svg?style=social&label=Star) | 2021      | 16.1         |          |                                                              |
|                                      |                                                              |                                                              |                                                              |           |              |          |                                                              |
|                                      |                                                              |                                                              |                                                              |           |              |          |                                                              |

### on-OpenImages-v4-Benchmark

| Model / Methods    | Title                                                        | Paper Link                                                   | Code Link                                                    | Published | AP(0.5⬆️) | Keywords | Venue |
| ------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | --------- | -------- | -------- | ----- |
| Object-Centric-OVD | Bridging the Gap between Object and Image-level Representations for Open-Vocabulary Detection | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2207.03482v3) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mmaaz60/mvits_for_class_agnostic_od)![Star](https://img.shields.io/github/stars/mmaaz60/mvits_for_class_agnostic_od.svg?style=social&label=Star) | 2022      | 42.9     |          |       |
| Detic              | Detecting Twenty-thousand Classes using Image-level Supervision | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2201.02605v3) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/Detic)![Star](https://img.shields.io/github/stars/facebookresearch/Detic.svg?style=social&label=Star) | 2022      | 42.2     |          |       |
|                    |                                                              |                                                              |                                                              |           |          |          |       |
|                    |                                                              |                                                              |                                                              |           |          |          |       |

### on-Objects365-Benchmark

| Model / Methods    | Title                                                        | Paper Link                                                   | Code Link                                                    | Published | AP(0.5⬆️) | Keywords | Venue     |
| ------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | --------- | -------- | -------- | --------- |
| Object-Centric-OVD | Bridging the Gap between Object and Image-level Representations for Open-Vocabulary Detection | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2207.03482v3) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mmaaz60/mvits_for_class_agnostic_od)![Star](https://img.shields.io/github/stars/mmaaz60/mvits_for_class_agnostic_od.svg?style=social&label=Star) | 2022      | 22.3     |          |           |
| ViLD               | Open-vocabulary Object Detection via Vision and Language Knowledge Distillation | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://openreview.net/forum?id=lL3lnMbR4WU) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/hanoonaR/object-centric-ovd)![Star](https://img.shields.io/github/stars/hanoonaR/object-centric-ovd.svg?style=social&label=Star) | 2021      | 18.2     |          | ICLR 2022 |
|                    |                                                              |                                                              |                                                              |           |          |          |           |
|                    |                                                              |                                                              |                                                              |           |          |          |           |



## Open-Vocabulary-Segmentation

| Model / Methods | Title | Paper Link | Code Link | Published | Keywords | Venue |
| --------------- | ----- | ---------- | --------- | --------- | -------- | ----- |
|                 |       |            |           |           |          |       |
|                 |       |            |           |           |          |       |
|                 |       |            |           |           |          |       |
|                 |       |            |           |           |          |       |







# Audio-Models

| Model / Methods | Title | Paper Link | Code Link | Published | Keywords | Venue |
| --------------- | ----- | ---------- | --------- | --------- | -------- | ----- |
|                 |       |            |           |           |          |       |
|                 |       |            |           |           |          |       |
|                 |       |            |           |           |          |       |
|                 |       |            |           |           |          |       |



# Video-Models

| Model / Methods | Title | Paper Link | Code Link | Published | Keywords | Venue |
| --------------- | ----- | ---------- | --------- | --------- | -------- | ----- |
|                 |       |            |           |           |          |       |
|                 |       |            |           |           |          |       |
|                 |       |            |           |           |          |       |
|                 |       |            |           |           |          |       |


# VLM-Pre-training-Methods

---

| Model / Methods | Title                                                        | Paper Link                                                 | Code Link                                                    | Published | Keywords | Venue     |
| --------------- | ------------------------------------------------------------ | ---------------------------------------------------------- | ------------------------------------------------------------ | --------- | -------- | --------- |
|                 | Efficient Vision-Language Pre-training by Cluster Masking    | [Paper](https://arxiv.org/pdf/2405.08815)                  | [Code](https://github.com/Zi-hao-Wei/Efficient-Vision-Language-Pre-training-by-Cluster-Masking) | 2024      |          | CVPR 2024 |
|                 | Towards Better Vision-Inspired Vision-Language Models        | [Paper](https://www.lamda.nju.edu.cn/caoyh/files/VIVL.pdf) | Code                                                         | 2024      |          | CVPR 2024 |
|                 | Non-autoregressive Sequence-to-Sequence Vision-Language Models | [Paper](https://arxiv.org/abs/2403.02249v1)                | Code                                                         | 2024      |          | CVPR 2024 |
| ViTamin         | ViTamin: Designing Scalable Vision Models in the Vision-Language Era | [Paper](https://arxiv.org/abs/2404.02132v1)                | [Code](https://github.com/Beckschen/ViTamin)                 | 2024      |          | CVPR 2024 |
|                 | Iterated Learning Improves Compositionality in Large Vision-Language Models | [Paper](https://arxiv.org/abs/2404.02145v1)                | Code                                                         | 2024      |          | CVPR 2024 |
| FairCLIP        | FairCLIP: Harnessing Fairness in Vision-Language Learning    | [Paper](https://arxiv.org/abs/2403.19949v1)                | [Code](https://ophai.hms.harvard.edu/datasets/fairvlmed10k)  | 2024      |          | CVPR 2024 |
| InternVL        | InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks | [Paper](https://arxiv.org/abs/2312.14238)                  | [Code](https://github.com/OpenGVLab/InternVL)                | 2024      |          | CVPR 2024 |
| VILA            | VILA: On Pre-training for Visual Language Models             | [Paper](https://arxiv.org/abs/2312.07533)                  | Code                                                         | 2024      |          | CVPR 2024 |
|                 | Generative Region-Language Pretraining for Open-Ended Object Detection | [Paper](https://arxiv.org/pdf/2403.10191v1.pdf)            | [Code](https://github.com/FoundationVision/GenerateU)        | 2024      |          | CVPR 2024 |
|                 | Enhancing Vision-Language Pre-training with Rich Supervisions | [Paper](https://arxiv.org/pdf/2403.03346v1.pdf)            | Code                                                         | 2024      |          | CVPR 2024 |
|                 | Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization | [Paper](https://arxiv.org/abs/2309.04669)                  | [Code](https://github.com/jy0205/LaVIT)                      | 2024      |          | ICLR 2024 |
| MMICL           | MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning | [Paper](https://arxiv.org/abs/2309.07915)                  | [Code](https://github.com/PKUnlp-icler/MIC)                  | 2024      |          | ICLR 2024 |
|                 | Retrieval-Enhanced Contrastive Vision-Text Models            | [Paper](https://arxiv.org/abs/2306.07196)                  | Code                                                         | 2024      |          | ICLR 2024 |
|                 |                                                              |                                                            |                                                              |           |          |           |



---

# VLM-Transfer-Learning-Methods

| -                           | Title                                                        | Paper Link                                        | Code Link                                                    | Published | Keywords | Venue      |
| --------------------------- | ------------------------------------------------------------ | ------------------------------------------------- | ------------------------------------------------------------ | --------- | -------- | ---------- |
| CLAP                        | CLAP: Isolating Content from Style through Contrastive Learning with Augmented Prompts | [Paper](https://arxiv.org/abs/2311.16445)         | [Code](https://github.com/YichaoCai1/CLAP)                   | 2024      |          | ECCV 2024  |
| FALIP                       | FALIP: Visual Prompt as Foveal Attention Boosts CLIP Zero-Shot Performance | [Paper](https://arxiv.org/abs/2407.05578v1)       | [Code](https://pumpkin805.github.io/FALIP/)                  | 2024      |          | ECCV 2024  |
| GalLoP                      | GalLoP: Learning Global and Local Prompts for Vision-Language Models | [Paper](https://arxiv.org/pdf/2407.01400)         | Code                                                         | 2024      |          | ECCV 2024  |
| Mind the Interference       | Mind the Interference: Retaining Pre-trained Knowledge in Parameter Efficient Continual Learning of Vision-Language Models | [Paper](https://arxiv.org/pdf/2407.05342v1)       | [Code](https://github.com/lloongx/DIKI)                      | 2024      |          | ECCV 2024  |
|                             | One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models | [Paper](https://arxiv.org/abs/2403.01849v1)       | [Code](https://github.com/TreeLLi/APT)                       | 2024      |          | CVPR 2024  |
|                             | Any-Shift Prompting for Generalization over Distributions    | [Paper](https://arxiv.org/abs/2402.10099)         | Code                                                         | 2024      |          | CVPR 2024  |
| CLAP                        | A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models | [Paper](https://arxiv.org/abs/2312.12730)         | [Code](https://github.com/jusiro/CLAP)                       | 2024      |          | CVPR 2024  |
|                             | Anchor-based Robust Finetuning of Vision-Language Models     | [Paper](https://arxiv.org/abs/2404.06244)         | Code                                                         | 2024      |          | CVPR 2024  |
|                             | Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners | Paper                                             | Code                                                         | 2024      |          | CVPR 2024  |
|                             | Visual In-Context Prompting                                  | [Paper](https://arxiv.org/abs/2311.13601)         | [Code](https://github.com/UX-Decoder/DINOv)                  | 2024      |          | CVPR 2024  |
| TCP                         | TCP:Textual-based Class-aware Prompt tuning for Visual-Language Model | [Paper](https://arxiv.org/abs/2311.18231)         | [Code](https://github.com/htyao89/Textual-based_Class-aware_prompt_tuning/) | 2024      |          | CVPR 2024  |
|                             | Efficient Test-Time Adaptation of Vision-Language Models     | [Paper](https://arxiv.org/abs/2403.18293v1)       | [Code](https://kdiaaa.github.io/tda/)                        | 2024      |          | CVPR 2024  |
| Dual Memory Networks        | Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models | [Paper](https://arxiv.org/abs/2403.17589v1)       | [Code](https://github.com/YBZh/DMN)                          | 2024      |          | CVPR 2024  |
| DePT                        | DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning | [Paper](https://arxiv.org/abs/2309.05173)         | [Code](https://github.com/ZhengxiangShi/DePT)                | 2024      |          | ICLR 2024  |
| Nemesis                     | Nemesis: Normalizing the soft-prompt vectors of vision-language models | [Paper](https://openreview.net/pdf?id=zmJDzPh1Dm) | Code                                                         | 2024      |          | ICLR 2024  |
|                             | Prompt Gradient Projection for Continual Learning            | [Paper](https://openreview.net/pdf?id=EH2O3h7sBI) | Code                                                         | 2024      |          | ICLR 2024  |
| An Image Is Worth 1000 Lies | An Image Is Worth 1000 Lies: Transferability of Adversarial Images across Prompts on Vision-Language Models | [Paper](https://openreview.net/pdf?id=nc5GgFAvtk) | Code                                                         | 2024      |          | ICLR 2024  |
| Matcher                     | Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching | [Paper](https://arxiv.org/abs/2305.13310)         | Code                                                         | 2024      |          | ICLR 2024  |
|                             | Text-driven Prompt Generation for Vision-Language Models in Federated Learning | [Paper](https://arxiv.org/abs/2310.06123)         | Code                                                         | 2024      |          | ICLR 2024  |
|                             | Consistency-guided Prompt Learning for Vision-Language Models | [Paper](https://arxiv.org/abs/2306.01195)         | Code                                                         | 2024      |          | ICLR 2024  |
| C-TPT                       | C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion | [Paper](https://openreview.net/pdf?id=jzzEHTBFOT) | Code                                                         | 2024      |          | ICLR 2024  |
|                             | Learning to Prompt Segment Anything Models                   | [Paper](https://arxiv.org/pdf/2401.04651.pdf)     | Code                                                         | 2024      |          | arXiv 2024 |
|                             |                                                              |                                                   |                                                              |           |          |            |



# VLM-Knowledge-Distillation-for-Detection

|             | Title                                                        | Paper Link                                        | Code Link                                                 | Published | Keywords | Venue     |
| ----------- | ------------------------------------------------------------ | ------------------------------------------------- | --------------------------------------------------------- | --------- | -------- | --------- |
| RegionGPT   | RegionGPT: Towards Region Understanding Vision Language Model | [Paper](https://arxiv.org/pdf/2403.02330v1.pdf)   | [Code](https://guoqiushan.github.io/regiongpt.github.io/) |           |          | CVPR 2024 |
|             | LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors | [Paper](https://arxiv.org/pdf/2402.04630.pdf)     | Code                                                      |           |          | ICLR 2024 |
| Ins-DetCLIP | Ins-DetCLIP: Aligning Detection Model to Follow Human-Language Instruction | [Paper](https://openreview.net/pdf?id=M0MF4t3hE9) | Code                                                      |           |          | ICLR 2024 |

 

# VLM-Knowledge-Distillation-for-Segmentation

|          | Title                                                        | Paper Link                                | Code Link | Published | Keywords | Venue     |
| -------- | ------------------------------------------------------------ | ----------------------------------------- | --------- | --------- | -------- | --------- |
| CLIPSelf | CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction | [Paper](https://arxiv.org/abs/2310.01403) | Code      | 2024      |          | ICLR 2024 |
|          |                                                              |                                           |           |           |          |           |



# VLM-Knowledge-Distillation-for-Other-Vision-Tasks

|             | Title                                                        | Paper Link                                    | Code Link                                     | Published | Keywords | Venue     |
| ----------- | ------------------------------------------------------------ | --------------------------------------------- | --------------------------------------------- | --------- | -------- | --------- |
| FROSTER     | FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition | [Paper](https://arxiv.org/pdf/2402.03241.pdf) | Code                                          | 2024      |          | ICLR 2024 |
| AnomalyCLIP | AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection | [Paper](https://arxiv.org/pdf/2310.18961.pdf) | [Code](https://github.com/zqhang/AnomalyCLIP) | 2024      |          | ICLR 2024 |
|             |                                                              |                                               |                                               |           |          |           |



---

# Prompt-Learning

| -          | Title                                                        | Paper Link                                                   | Code Link                                                    | Published | Keywords                                   | Venue        |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | --------- | ------------------------------------------ | ------------ |
| CoOp       | Learning to Prompt for Vision-Language Models                | [Paper](https://arxiv.org/abs/2203.05557)                    | [Code](https://github.com/KaiyangZhou/CoOp)                  | 2022      | Use text-based learnable prompts/adapters  | IJCV 2022    |
| CoCoOp     | Conditional Prompt Learning for Vision-Language Models       | [Paper](https://arxiv.org/abs/2203.05557)                    | [Code](https://github.com/KaiyangZhou/CoOp)                  | 2022      | Use text-based learnable prompts/adapters  | CVPR 2022    |
| ProDA      | Prompt Distribution Learning                                 | [Paper](https://arxiv.org/abs/2205.03340)                    | [Code](https://github.com/bbbdylan/proda)                    | 2022      | Use text-based learnable prompts/adapters  | CVPR 2022    |
| VPT        | Visual Prompt Tuning                                         | [Paper](https://arxiv.org/abs/2203.12119)                    | [Code](https://github.com/kmnp/vpt)                          | 2022      | Use image-based learnable prompts/adapters | ECCV 2022    |
| MaPLe      | MaPLe: Multi-modal Prompt Learning                           | [Paper](https://arxiv.org/abs/2210.03117)                    | [Code](https://github.com/muzairkhattak/multimodal-prompt-learning) | 2023      |                                            | CVPR 2023    |
| KgCoOp     | Visual-Language Prompt Tuningx with Knowledge-guided Context Optimization | [Paper](https://arxiv.org/abs/2303.13283)                    | [Code](https://github.com/htyao89/KgCoOp)                    | 2023      |                                            | CVPR 2023    |
| LASP       | LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models | [Paper](https://arxiv.org/abs/2210.01115)                    | -                                                            | 2023      |                                            | CVPR 2023    |
| DAM-VP     | Diversity-Aware Meta Visual Prompting                        | [Paper](https://arxiv.org/abs/2303.08138)                    | [Code](https://github.com/shikiw/DAM-VP)                     | 2023      |                                            | CVPR 2023    |
| TaskRes    | Task Residual for Tuning Vision-Language Models              | [Paper](https://arxiv.org/abs/2211.10277)                    | [Code](https://github.com/geekyutao/TaskRes)                 | 2023      |                                            | CVPR 2023    |
| RPO        | Read-only Prompt Optimization for Vision-Language Few-shot Learning | [Paper](https://arxiv.org/abs/2308.14960)                    | [Code](https://github.com/mlvlab/rpo)                        | 2023      |                                            | ICCV 2023    |
| KAPT       | Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models | [Paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Kan_Knowledge-Aware_Prompt_Tuning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.pdf) | -                                                            | 2023      |                                            | ICCV 2023    |
| ProGrad    | Prompt-aligned Gradient for Prompt Tuning                    | [Paper](https://arxiv.org/abs/2205.14865)                    | [Code](https://github.com/BeierZhu/Prompt-align)             | 2023      |                                            | ICCV 2023    |
| PromptSRC  | Self-regulating Prompts: Foundational Model Adaptation without Forgetting | [Paper](https://openaccess.thecvf.com//content/ICCV2023/papers/Khattak_Self-regulating_Prompts_Foundational_Model_Adaptation_without_Forgetting_ICCV_2023_paper.pdf) | [Code](https://github.com/muzairkhattak/PromptSRC)           | 2023      |                                            | ICCV 2023    |
| DeFo       | Learning to Decompose Visual Features with Latent Textual Prompts | [Paper](https://arxiv.org/abs/2210.04287)                    | -                                                            | 2023      |                                            | ICLR 2023    |
| POMP       | Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition | [Paper](https://arxiv.org/abs/2304.04704)                    | [Code](https://github.com/amazon-science/prompt-pretraining) | 2023      |                                            | NeurIPS 2023 |
| MetaPrompt | Learning Domain Invariant Prompt for Vision-Language Models  | [Paper](https://arxiv.org/abs/2212.04196)                    | -                                                            | 2024      |                                            | TIP 2024     |
| SA2VP      | SA2VP: Spatially Aligned-and-Adapted Visual Prompt           | [Paper](https://arxiv.org/abs/2312.10376)                    | [Code](https://github.com/tommy-xq/SA2VP)                    | 2024      |                                            |              |
| HPT        | Learning Hierarchical Prompt with Structured Linguistic Knowledge for Vision-Language Models | [Paper](https://arxiv.org/abs/2312.06323)                    | [Code](https://github.com/Vill-Lab/2024-AAAI-HPT)            | 2024      |                                            |              |
| LaViP      | LaViP: Language-Grounded Visual Prompts                      | [Paper](https://arxiv.org/abs/2312.10945)                    | -                                                            | 2024      |                                            |              |
| CoPrompt   | Consistency-guided Prompt Learning for Vision-Language Models | [Paper](https://arxiv.org/abs/2306.01195)                    | [Code](https://github.com/ShuvenduRoy/CoPrompt)              | 2024      |                                            |              |
| ProText    | Learning to Prompt with Text Only Supervision for Vision-Language Models | [Paper](https://arxiv.org/abs/2401.02418)                    | [Code](https://github.com/muzairkhattak/ProText)             | 2024      |                                            |              |
| PromptKD   | Unsupervised Prompt Distillation for Vision Language Models  | [Paper](https://arxiv.org/abs/2403.02781)                    | [Code](https://github.com/zhengli97/PromptKD)                | 2024      |                                            |              |
| DePT       | DePT: Decoupled Prompt Tuning                                | [Paper](https://arxiv.org/abs/2309.07439)                    | [Code](https://github.com/Koorye/DePT)                       | 2024      |                                            |              |
| ArGue      | ArGue: Attribute-Guided Prompt Tuning for Vision-Language Models | [Paper](https://arxiv.org/abs/2311.16494)                    | -                                                            | 2024      |                                            |              |
| TCP        | TCP:Textual-based Class-aware Prompt tuning for Visual-Language Model | [Paper](https://arxiv.org/abs/2311.18231)                    | [Code](https://github.com/htyao89/Textual-based_Class-aware_prompt_tuning) | 2024      |                                            |              |
| MMA        | MMA: Multi-Modal Adapter for Vision-Language Models          | [Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_MMA_Multi-Modal_Adapter_for_Vision-Language_Models_CVPR_2024_paper.pdf) | [Code](https://github.com/ZjjConan/Multi-Modal-Adapter)      | 2024      |                                            |              |
