# Awesome-Reasearch-Work

---
Awesome-Reasearch-Work

## Transformers-MultiModal Models(Sort A-Z)

|                   | Title                                                        | Paper Link | Code Link | Published | Keywords | Venue                                            |
| ----------------- | ------------------------------------------------------------ | ---------- | --------- | --------- | -------- | ------------------------------------------------ |
| ALIGN             | Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision | Paper      | Code      | 2021      |          |                                                  |
| AltCLIP           | AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities | Paper      | Code      |           |          |                                                  |
| BLIP              | BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation | Paper      | Code      |           |          |                                                  |
| BLIP-2            | BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models | Paper      | Code      |           |          |                                                  |
| BridgeTower       | BridgeTower: Building Bridges Between Encoders in Vision-Language Representative Learning | Paper      | Code      |           |          | [AAAI’23](https://aaai.org/Conferences/AAAI-23/) |
| BROS              | BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents | Paper      | Code      |           |          |                                                  |
| Chameleon         | Chameleon: Mixed-Modal Early-Fusion Foundation Models        | Paper      | Code      |           |          |                                                  |
| Chinese-CLIP      | Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese | Paper      | Code      |           |          |                                                  |
| CLIP              | Learning Transferable Visual Models From Natural Language Supervision | Paper      | Code      |           |          |                                                  |
| CLIPSeg           | Image Segmentation Using Text and Image Prompts              | Paper      | Code      |           |          |                                                  |
| CLVP              | Better speech synthesis through scaling                      | Paper      | Code      |           |          |                                                  |
| Data2Vec          | data 2 vec：A General Framework for Self-supervised Learning in Speech，Vision and Language | Paper      | Code      |           |          |                                                  |
| DePlot            | DePlot: One-shot visual language reasoning by plot-to-table translation | Paper      | Code      |           |          |                                                  |
| Donut             | OCR-free Document Understanding Transformer                  | Paper      | Code      |           |          |                                                  |
| FLAVA             | FLAVA: A Foundational Language And Vision Alignment Model    | Paper      | Code      |           |          |                                                  |
| GIT               | GIT: A Generative Image-to-text Transformer for Vision and Language | Paper      | Code      |           |          |                                                  |
| Grounding DINO    | Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection | Paper      | Code      |           |          |                                                  |
| GroupViT          | GroupViT: Semantic Segmentation Emerges from Text Supervision | Paper      | Code      |           |          |                                                  |
| IDEFICS           | OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents | Paper      | Code      |           |          |                                                  |
| IDEFICS-2         | What matters when building vision-language models?           | Paper      | Code      |           |          |                                                  |
| InstructBLIP      | InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning | Paper      | Code      |           |          |                                                  |
| InstructBlipVideo | InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning | Paper      | Code      |           |          |                                                  |
| KOSMOS-2          | Kosmos-2: Grounding Multimodal Large Language Models to the World | Paper      | Code      |           |          |                                                  |
| LayoutLM          | LayoutLM: Pre-training of Text and Layout for Document Image Understanding | Paper      | Code      |           |          |                                                  |
| LayoutLMV2        | LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding | Paper      | Code      |           |          |                                                  |
| LayoutLMv3        | LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking | Paper      | Code      |           |          |                                                  |
| LayoutXLM         | LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding | Paper      | Code      |           |          |                                                  |
| LiLT              | LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding | Paper      | Code      |           |          |                                                  |
| LLaVa             | Improved Baselines with Visual Instruction Tuning            | Paper      | Code      |           |          |                                                  |
| LLaVA-NeXT        | LLaVA-NeXT: Improved reasoning, OCR, and world knowledge     | Paper      | Code      |           |          |                                                  |
| LLaVa-NeXT-Video  | LLaVA-NeXT: A Strong Zero-shot Video Understanding Model     | Paper      | Code      |           |          |                                                  |
| LXMERT            | LXMERT: Learning Cross-Modality Encoder Representations from Transformers | Paper      | Code      |           |          |                                                  |
| MatCha            | MatCha：Enhancing Visual Language Pretraining with Math Reasoning and Chart Derrendering | Paper      | Code      |           |          |                                                  |
| MGP-STR           | Multi-Granularity Prediction for Scene Text Recognition      | Paper      | Code      |           |          |                                                  |
| Nougat            | Nougat: Neural Optical Understanding for Academic Documents  | Paper      | Code      |           |          |                                                  |
| OneFormer         | OneFormer: One Transformer to Rule Universal Image Segmentation | Paper      | Code      |           |          |                                                  |
| OWL-ViT           | Simple Open-Vocabulary Object Detection with Vision Transformers | Paper      | Code      |           |          |                                                  |
| PaliGemma         | PaliGemma – Google’s Cutting-Edge Open Vision Language Model | Paper      | Code      |           |          |                                                  |
| Perceiver         | Perceiver IO: A General Architecture for Structured Inputs & Outputs | Paper      | Code      |           |          |                                                  |
| Pix2Struct        | Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding | Paper      | Code      |           |          |                                                  |
| SAM               | Segment Anything                                             | Paper      | Code      |           |          |                                                  |
| SigLIP            | Sigmoid Loss for Language Image Pre-Training                 | Paper      | Code      |           |          |                                                  |
| TAPAS             | TAPAS: Weakly Supervised Table Parsing via Pre-training      | Paper      | Code      |           |          |                                                  |
| TrOCR             | TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models | Paper      | Code      |           |          |                                                  |
| TVLT              | TVLT: Textless Vision-Language Transformer                   | Paper      | Code      |           |          |                                                  |
| TVP               | Text-Visual Prompting for Efficient 2D Temporal Video Grounding | Paper      | Code      |           |          |                                                  |
| UDOP              | Unifying Vision, Text, and Layout for Universal Document Processing | Paper      | Code      |           |          |                                                  |
| Video-LLaVA       | Video-LLaVA: Learning United Visual Representation by Alignment Before Projection | Paper      | Code      |           |          |                                                  |
| ViLT              | ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision | Paper      | Code      |           |          |                                                  |
| VipLlava          | Making Large Multimodal Models Understand Arbitrary Visual Prompts | Paper      | Code      |           |          |                                                  |
| VisualBERT        | VisualBERT: A Simple and Performant Baseline for Vision and Language | Paper      | Code      |           |          |                                                  |
| X-CLIP            | Expanding Language-Image Pretrained Models for General Video Recognition | Paper      | Code      |           |          |                                                  |
|                   |                                                              |            |           |           |          |                                                  |

- Vision Encoder Decoder Models

  - Vision Models

    - [ViT](https://huggingface.co/docs/transformers/model_doc/vit)
    - [BEiT](https://huggingface.co/docs/transformers/model_doc/beit)
    - [Swin](https://huggingface.co/docs/transformers/model_doc/swin)


  - Language Models

    - [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)
    - [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)
    - [BERT](https://huggingface.co/docs/transformers/model_doc/bert)
    - [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)



- Vision TextDual Encoder
  
- Speech Encoder Decoder Models



## Vision Models


|      | Title | Paper Link | Code Link | Published | Keywords | Venue |
| ---- | ----- | ---------- | --------- | --------- | -------- | ----- |
|      |       |            |           |           |          |       |
|      |       |            |           |           |          |       |
|      |       |            |           |           |          |       |



## Language Models


|      | Title | Paper Link | Code Link | Published | Keywords | Venue |
| ---- | ----- | ---------- | --------- | --------- | -------- | ----- |
|      |       |            |           |           |          |       |
|      |       |            |           |           |          |       |
|      |       |            |           |           |          |       |
|      |       |            |           |           |          |       |



## Speech Models

|      | Title | Paper Link | Code Link | Published | Keywords | Venue |
| ---- | ----- | ---------- | --------- | --------- | -------- | ----- |
|      |       |            |           |           |          |       |
|      |       |            |           |           |          |       |
|      |       |            |           |           |          |       |
|      |       |            |           |           |          |       |





## VLM Pre-training Methods

---

|          | Title                                                        | Paper Link                                                 | Code Link                                                    | Published | Keywords | Venue     |
| -------- | ------------------------------------------------------------ | ---------------------------------------------------------- | ------------------------------------------------------------ | --------- | -------- | --------- |
|          | Efficient Vision-Language Pre-training by Cluster Masking    | [Paper](https://arxiv.org/pdf/2405.08815)                  | [Code](https://github.com/Zi-hao-Wei/Efficient-Vision-Language-Pre-training-by-Cluster-Masking) | 2024      |          | CVPR 2024 |
|          | Towards Better Vision-Inspired Vision-Language Models        | [Paper](https://www.lamda.nju.edu.cn/caoyh/files/VIVL.pdf) | Code                                                         | 2024      |          | CVPR 2024 |
|          | Non-autoregressive Sequence-to-Sequence Vision-Language Models | [Paper](https://arxiv.org/abs/2403.02249v1)                | Code                                                         | 2024      |          | CVPR 2024 |
| ViTamin  | ViTamin: Designing Scalable Vision Models in the Vision-Language Era | [Paper](https://arxiv.org/abs/2404.02132v1)                | [Code](https://github.com/Beckschen/ViTamin)                 | 2024      |          | CVPR 2024 |
|          | Iterated Learning Improves Compositionality in Large Vision-Language Models | [Paper](https://arxiv.org/abs/2404.02145v1)                | Code                                                         | 2024      |          | CVPR 2024 |
| FairCLIP | FairCLIP: Harnessing Fairness in Vision-Language Learning    | [Paper](https://arxiv.org/abs/2403.19949v1)                | [Code](https://ophai.hms.harvard.edu/datasets/fairvlmed10k)  | 2024      |          | CVPR 2024 |
| InternVL | InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks | [Paper](https://arxiv.org/abs/2312.14238)                  | [Code](https://github.com/OpenGVLab/InternVL)                | 2024      |          | CVPR 2024 |
| VILA     | VILA: On Pre-training for Visual Language Models             | [Paper](https://arxiv.org/abs/2312.07533)                  | Code                                                         | 2024      |          | CVPR 2024 |
|          | Generative Region-Language Pretraining for Open-Ended Object Detection | [Paper](https://arxiv.org/pdf/2403.10191v1.pdf)            | [Code](https://github.com/FoundationVision/GenerateU)        | 2024      |          | CVPR 2024 |
|          | Enhancing Vision-Language Pre-training with Rich Supervisions | [Paper](https://arxiv.org/pdf/2403.03346v1.pdf)            | Code                                                         | 2024      |          | CVPR 2024 |
|          | Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization | [Paper](https://arxiv.org/abs/2309.04669)                  | [Code](https://github.com/jy0205/LaVIT)                      | 2024      |          | ICLR 2024 |
| MMICL    | MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning | [Paper](https://arxiv.org/abs/2309.07915)                  | [Code](https://github.com/PKUnlp-icler/MIC)                  | 2024      |          | ICLR 2024 |
|          | Retrieval-Enhanced Contrastive Vision-Text Models            | [Paper](https://arxiv.org/abs/2306.07196)                  | Code                                                         | 2024      |          | ICLR 2024 |
|          |                                                              |                                                            |                                                              |           |          |           |

---

## VLM Transfer Learning Methods

| -                           | Title                                                        | Paper Link                                        | Code Link                                                    | Published | Keywords | Venue      |
| --------------------------- | ------------------------------------------------------------ | ------------------------------------------------- | ------------------------------------------------------------ | --------- | -------- | ---------- |
| CLAP                        | CLAP: Isolating Content from Style through Contrastive Learning with Augmented Prompts | [Paper](https://arxiv.org/abs/2311.16445)         | [Code](https://github.com/YichaoCai1/CLAP)                   | 2024      |          | ECCV 2024  |
| FALIP                       | FALIP: Visual Prompt as Foveal Attention Boosts CLIP Zero-Shot Performance | [Paper](https://arxiv.org/abs/2407.05578v1)       | [Code](https://pumpkin805.github.io/FALIP/)                  | 2024      |          | ECCV 2024  |
| GalLoP                      | GalLoP: Learning Global and Local Prompts for Vision-Language Models | [Paper](https://arxiv.org/pdf/2407.01400)         | Code                                                         | 2024      |          | ECCV 2024  |
| Mind the Interference       | Mind the Interference: Retaining Pre-trained Knowledge in Parameter Efficient Continual Learning of Vision-Language Models | [Paper](https://arxiv.org/pdf/2407.05342v1)       | [Code](https://github.com/lloongx/DIKI)                      | 2024      |          | ECCV 2024  |
|                             | One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models | [Paper](https://arxiv.org/abs/2403.01849v1)       | [Code](https://github.com/TreeLLi/APT)                       | 2024      |          | CVPR 2024  |
|                             | Any-Shift Prompting for Generalization over Distributions    | [Paper](https://arxiv.org/abs/2402.10099)         | Code                                                         | 2024      |          | CVPR 2024  |
| CLAP                        | A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models | [Paper](https://arxiv.org/abs/2312.12730)         | [Code](https://github.com/jusiro/CLAP)                       | 2024      |          | CVPR 2024  |
|                             | Anchor-based Robust Finetuning of Vision-Language Models     | [Paper](https://arxiv.org/abs/2404.06244)         | Code                                                         | 2024      |          | CVPR 2024  |
|                             | Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners | Paper                                             | Code                                                         | 2024      |          | CVPR 2024  |
|                             | Visual In-Context Prompting                                  | [Paper](https://arxiv.org/abs/2311.13601)         | [Code](https://github.com/UX-Decoder/DINOv)                  | 2024      |          | CVPR 2024  |
| TCP                         | TCP:Textual-based Class-aware Prompt tuning for Visual-Language Model | [Paper](https://arxiv.org/abs/2311.18231)         | [Code](https://github.com/htyao89/Textual-based_Class-aware_prompt_tuning/) | 2024      |          | CVPR 2024  |
|                             | Efficient Test-Time Adaptation of Vision-Language Models     | [Paper](https://arxiv.org/abs/2403.18293v1)       | [Code](https://kdiaaa.github.io/tda/)                        | 2024      |          | CVPR 2024  |
| Dual Memory Networks        | Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models | [Paper](https://arxiv.org/abs/2403.17589v1)       | [Code](https://github.com/YBZh/DMN)                          | 2024      |          | CVPR 2024  |
| DePT                        | DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning | [Paper](https://arxiv.org/abs/2309.05173)         | [Code](https://github.com/ZhengxiangShi/DePT)                | 2024      |          | ICLR 2024  |
| Nemesis                     | Nemesis: Normalizing the soft-prompt vectors of vision-language models | [Paper](https://openreview.net/pdf?id=zmJDzPh1Dm) | Code                                                         | 2024      |          | ICLR 2024  |
|                             | Prompt Gradient Projection for Continual Learning            | [Paper](https://openreview.net/pdf?id=EH2O3h7sBI) | Code                                                         | 2024      |          | ICLR 2024  |
| An Image Is Worth 1000 Lies | An Image Is Worth 1000 Lies: Transferability of Adversarial Images across Prompts on Vision-Language Models | [Paper](https://openreview.net/pdf?id=nc5GgFAvtk) | Code                                                         | 2024      |          | ICLR 2024  |
| Matcher                     | Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching | [Paper](https://arxiv.org/abs/2305.13310)         | Code                                                         | 2024      |          | ICLR 2024  |
|                             | Text-driven Prompt Generation for Vision-Language Models in Federated Learning | [Paper](https://arxiv.org/abs/2310.06123)         | Code                                                         | 2024      |          | ICLR 2024  |
|                             | Consistency-guided Prompt Learning for Vision-Language Models | [Paper](https://arxiv.org/abs/2306.01195)         | Code                                                         | 2024      |          | ICLR 2024  |
| C-TPT                       | C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion | [Paper](https://openreview.net/pdf?id=jzzEHTBFOT) | Code                                                         | 2024      |          | ICLR 2024  |
|                             | Learning to Prompt Segment Anything Models                   | [Paper](https://arxiv.org/pdf/2401.04651.pdf)     | Code                                                         | 2024      |          | arXiv 2024 |
|                             |                                                              |                                                   |                                                              |           |          |            |



## VLM Knowledge Distillation for Detection

|             | Title                                                        | Paper Link                                        | Code Link                                                 | Published | Keywords | Venue     |
| ----------- | ------------------------------------------------------------ | ------------------------------------------------- | --------------------------------------------------------- | --------- | -------- | --------- |
| RegionGPT   | RegionGPT: Towards Region Understanding Vision Language Model | [Paper](https://arxiv.org/pdf/2403.02330v1.pdf)   | [Code](https://guoqiushan.github.io/regiongpt.github.io/) |           |          | CVPR 2024 |
|             | LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors | [Paper](https://arxiv.org/pdf/2402.04630.pdf)     | Code                                                      |           |          | ICLR 2024 |
| Ins-DetCLIP | Ins-DetCLIP: Aligning Detection Model to Follow Human-Language Instruction | [Paper](https://openreview.net/pdf?id=M0MF4t3hE9) | Code                                                      |           |          | ICLR 2024 |

 

## VLM Knowledge Distillation for Segmentation

|          | Title                                                        | Paper Link                                | Code Link | Published | Keywords | Venue     |
| -------- | ------------------------------------------------------------ | ----------------------------------------- | --------- | --------- | -------- | --------- |
| CLIPSelf | CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction | [Paper](https://arxiv.org/abs/2310.01403) | Code      | 2024      |          | ICLR 2024 |
|          |                                                              |                                           |           |           |          |           |




## VLM Knowledge Distillation for Other Vision Tasks

|             | Title                                                        | Paper Link                                    | Code Link                                     | Published | Keywords | Venue     |
| ----------- | ------------------------------------------------------------ | --------------------------------------------- | --------------------------------------------- | --------- | -------- | --------- |
| FROSTER     | FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition | [Paper](https://arxiv.org/pdf/2402.03241.pdf) | Code                                          | 2024      |          | ICLR 2024 |
| AnomalyCLIP | AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection | [Paper](https://arxiv.org/pdf/2310.18961.pdf) | [Code](https://github.com/zqhang/AnomalyCLIP) | 2024      |          | ICLR 2024 |
|             |                                                              |                                               |                                               |           |          |           |



---

| -          | Title                                                        | Paper Link                                                   | Code Link                                                    | Published | Keywords                                   | Venue        |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | --------- | ------------------------------------------ | ------------ |
| CoOp       | Learning to Prompt for Vision-Language Models                | [Paper](https://arxiv.org/abs/2203.05557)                    | [Code](https://github.com/KaiyangZhou/CoOp)                  | 2022      | Use text-based learnable prompts/adapters  | IJCV 2022    |
| CoCoOp     | Conditional Prompt Learning for Vision-Language Models       | [Paper](https://arxiv.org/abs/2203.05557)                    | [Code](https://github.com/KaiyangZhou/CoOp)                  | 2022      | Use text-based learnable prompts/adapters  | CVPR 2022    |
| ProDA      | Prompt Distribution Learning                                 | [Paper](https://arxiv.org/abs/2205.03340)                    | [Code](https://github.com/bbbdylan/proda)                    | 2022      | Use text-based learnable prompts/adapters  | CVPR 2022    |
| VPT        | Visual Prompt Tuning                                         | [Paper](https://arxiv.org/abs/2203.12119)                    | [Code](https://github.com/kmnp/vpt)                          | 2022      | Use image-based learnable prompts/adapters | ECCV 2022    |
| MaPLe      | MaPLe: Multi-modal Prompt Learning                           | [Paper](https://arxiv.org/abs/2210.03117)                    | [Code](https://github.com/muzairkhattak/multimodal-prompt-learning) | 2023      |                                            | CVPR 2023    |
| KgCoOp     | Visual-Language Prompt Tuningx with Knowledge-guided Context Optimization | [Paper](https://arxiv.org/abs/2303.13283)                    | [Code](https://github.com/htyao89/KgCoOp)                    | 2023      |                                            | CVPR 2023    |
| LASP       | LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models | [Paper](https://arxiv.org/abs/2210.01115)                    | -                                                            | 2023      |                                            | CVPR 2023    |
| DAM-VP     | Diversity-Aware Meta Visual Prompting                        | [Paper](https://arxiv.org/abs/2303.08138)                    | [Code](https://github.com/shikiw/DAM-VP)                     | 2023      |                                            | CVPR 2023    |
| TaskRes    | Task Residual for Tuning Vision-Language Models              | [Paper](https://arxiv.org/abs/2211.10277)                    | [Code](https://github.com/geekyutao/TaskRes)                 | 2023      |                                            | CVPR 2023    |
| RPO        | Read-only Prompt Optimization for Vision-Language Few-shot Learning | [Paper](https://arxiv.org/abs/2308.14960)                    | [Code](https://github.com/mlvlab/rpo)                        | 2023      |                                            | ICCV 2023    |
| KAPT       | Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models | [Paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Kan_Knowledge-Aware_Prompt_Tuning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.pdf) | -                                                            | 2023      |                                            | ICCV 2023    |
| ProGrad    | Prompt-aligned Gradient for Prompt Tuning                    | [Paper](https://arxiv.org/abs/2205.14865)                    | [Code](https://github.com/BeierZhu/Prompt-align)             | 2023      |                                            | ICCV 2023    |
| PromptSRC  | Self-regulating Prompts: Foundational Model Adaptation without Forgetting | [Paper](https://openaccess.thecvf.com//content/ICCV2023/papers/Khattak_Self-regulating_Prompts_Foundational_Model_Adaptation_without_Forgetting_ICCV_2023_paper.pdf) | [Code](https://github.com/muzairkhattak/PromptSRC)           | 2023      |                                            | ICCV 2023    |
| DeFo       | Learning to Decompose Visual Features with Latent Textual Prompts | [Paper](https://arxiv.org/abs/2210.04287)                    | -                                                            | 2023      |                                            | ICLR 2023    |
| POMP       | Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition | [Paper](https://arxiv.org/abs/2304.04704)                    | [Code](https://github.com/amazon-science/prompt-pretraining) | 2023      |                                            | NeurIPS 2023 |
| MetaPrompt | Learning Domain Invariant Prompt for Vision-Language Models  | [Paper](https://arxiv.org/abs/2212.04196)                    | -                                                            | 2024      |                                            | TIP 2024     |
| SA2VP      | SA2VP: Spatially Aligned-and-Adapted Visual Prompt           | [Paper](https://arxiv.org/abs/2312.10376)                    | [Code](https://github.com/tommy-xq/SA2VP)                    | 2024      |                                            |              |
| HPT        | Learning Hierarchical Prompt with Structured Linguistic Knowledge for Vision-Language Models | [Paper](https://arxiv.org/abs/2312.06323)                    | [Code](https://github.com/Vill-Lab/2024-AAAI-HPT)            | 2024      |                                            |              |
| LaViP      | LaViP: Language-Grounded Visual Prompts                      | [Paper](https://arxiv.org/abs/2312.10945)                    | -                                                            | 2024      |                                            |              |
| CoPrompt   | Consistency-guided Prompt Learning for Vision-Language Models | [Paper](https://arxiv.org/abs/2306.01195)                    | [Code](https://github.com/ShuvenduRoy/CoPrompt)              | 2024      |                                            |              |
| ProText    | Learning to Prompt with Text Only Supervision for Vision-Language Models | [Paper](https://arxiv.org/abs/2401.02418)                    | [Code](https://github.com/muzairkhattak/ProText)             | 2024      |                                            |              |
| PromptKD   | Unsupervised Prompt Distillation for Vision Language Models  | [Paper](https://arxiv.org/abs/2403.02781)                    | [Code](https://github.com/zhengli97/PromptKD)                | 2024      |                                            |              |
| DePT       | DePT: Decoupled Prompt Tuning                                | [Paper](https://arxiv.org/abs/2309.07439)                    | [Code](https://github.com/Koorye/DePT)                       | 2024      |                                            |              |
| ArGue      | ArGue: Attribute-Guided Prompt Tuning for Vision-Language Models | [Paper](https://arxiv.org/abs/2311.16494)                    | -                                                            | 2024      |                                            |              |
| TCP        | TCP:Textual-based Class-aware Prompt tuning for Visual-Language Model | [Paper](https://arxiv.org/abs/2311.18231)                    | [Code](https://github.com/htyao89/Textual-based_Class-aware_prompt_tuning) | 2024      |                                            |              |
| MMA        | MMA: Multi-Modal Adapter for Vision-Language Models          | [Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_MMA_Multi-Modal_Adapter_for_Vision-Language_Models_CVPR_2024_paper.pdf) | [Code](https://github.com/ZjjConan/Multi-Modal-Adapter)      | 2024      |                                            |              |
