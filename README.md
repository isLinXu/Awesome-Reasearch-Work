# Awesome-Reasearch-Work

---
Awesome-Reasearch-Work

(Sort A-Z)

# Table of Contents

  <ol>
    <li><a href=#Transformers-MultiModal-Models>Transformers-MultiModal-Models</a></li>
    <li><a href=#Language-Models>Language-Models</a></li>
    <li><a href=#Audio-Models>Audio-Models</a></li>
    <li><a href=#Video-Models>Video-Models</a></li>
    <li><a href=#VLM-Pre-training-Methods>VLM-Pre-training-Methods</a></li>
    <li><a href=#VLM-Transfer-Learning-Methods>VLM-Transfer-Learning-Methods</a></li>
    <li><a href=#VLM-Knowledge-Distillation-for-Detection>VLM-Knowledge-Distillation-for-Detection</a></li>
    <li><a href=#VLM-Knowledge-Distillation-for-Segmentation>VLM-Knowledge-Distillation-for-Segmentation</a></li>
    <li><a href=#VLM-Knowledge-Distillation-for-Other-Vision-Tasks>VLM-Knowledge-Distillation-for-Other-Vision-Tasks</a></li>
    <li><a href=#Prompt-Learning>Prompt-Learning</a></li>
  </ol>
# Transformers-MultiModal-Models

| Model / Methods   | Title                                                        | Paper Link                                                   | Code Link                                                    | Published | Keywords  | Venue                                            |
| ----------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | --------- | --------- | ------------------------------------------------ |
| ALIGN             | Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision | [Paper](https://arxiv.org/abs/2102.05918)                    | Code                                                         | 2021      |           |                                                  |
| AltCLIP           | AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities | [Paper](https://arxiv.org/abs/2211.06679v2)                  | Code                                                         | 2022      |           |                                                  |
| BLIP              | BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation | [Paper](https://arxiv.org/abs/2201.12086)                    | [Code](https://github.com/salesforce/BLIP)                   | 2022      |           |                                                  |
| BLIP-2            | BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models | [Paper](https://arxiv.org/abs/2301.12597)                    | [Code](https://github.com/salesforce/LAVIS/tree/5ee63d688ba4cebff63acee04adaef2dee9af207) | 2023      |           |                                                  |
| BridgeTower       | BridgeTower: Building Bridges Between Encoders in Vision-Language Representative Learning | [Paper](https://arxiv.org/abs/2206.08657)                    | [Code](https://github.com/microsoft/BridgeTower)             | 2023      |           | [AAAI’23](https://aaai.org/Conferences/AAAI-23/) |
| BROS              | BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents | [Paper](https://arxiv.org/abs/2108.04539)                    | [Code](https://github.com/clovaai/bros)                      | 2021      |           |                                                  |
| Chameleon         | Chameleon: Mixed-Modal Early-Fusion Foundation Models        | [Paper](https://arxiv.org/abs/2405.09818v1)                  | [Code](https://github.com/facebookresearch/chameleon)        | 2024      |           |                                                  |
| Chinese-CLIP      | Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese | [Paper](https://arxiv.org/abs/2211.01335)                    | [Code](https://huggingface.co/OFA-Sys)                       | 2023      |           |                                                  |
| CLIP              | Learning Transferable Visual Models From Natural Language Supervision | [Paper](https://arxiv.org/abs/2103.00020)                    | [Code](https://github.com/openai/CLIP)                       | 2021      |           |                                                  |
| CLIPSeg           | Image Segmentation Using Text and Image Prompts              | [Paper](https://arxiv.org/abs/2112.10003)                    | [Code](https://github.com/timojl/clipseg)                    | 2021      |           |                                                  |
| CLVP              | Better speech synthesis through scaling                      | [Paper](https://arxiv.org/abs/2305.07243)                    | [Code](https://github.com/neonbjb/tortoise-tts)              | 2023      |           |                                                  |
| Data2Vec          | data 2 vec：A General Framework for Self-supervised Learning in Speech，Vision and Language | [Paper](https://arxiv.org/pdf/2202.03555)                    | [Code](https://github.com/facebookresearch/data2vec_vision/tree/main/beit) | 2022      |           |                                                  |
| DePlot            | DePlot: One-shot visual language reasoning by plot-to-table translation | [Paper](https://arxiv.org/abs/2212.10505)                    | Code                                                         | 2022      |           |                                                  |
| Donut             | OCR-free Document Understanding Transformer                  | [Paper](https://arxiv.org/abs/2111.15664)                    | [Code](https://github.com/clovaai/donut)                     | 2021      |           |                                                  |
| FLAVA             | FLAVA: A Foundational Language And Vision Alignment Model    | [Paper](https://arxiv.org/abs/2112.04482)                    | [Code](https://github.com/facebookresearch/multimodal/tree/main/examples/flava) | 2021      |           |                                                  |
| GIT               | GIT: A Generative Image-to-text Transformer for Vision and Language | [Paper](https://arxiv.org/abs/2205.14100)                    | [Code](https://github.com/microsoft/GenerativeImage2Text)    | 2022      |           |                                                  |
| Grounding DINO    | Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection | [Paper](https://arxiv.org/abs/2303.05499)                    | [Code](https://github.com/IDEA-Research/GroundingDINO)       | 2023      |           |                                                  |
| GroupViT          | GroupViT: Semantic Segmentation Emerges from Text Supervision | [Paper](https://arxiv.org/abs/2202.11094)                    | [Code](https://github.com/NVlabs/GroupViT)                   | 2022      |           |                                                  |
| IDEFICS           | OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents | [Paper](https://huggingface.co/papers/2306.16527)            | [Code](https://huggingface.co/docs/transformers/model_doc/INSERT%20LINK%20TO%20GITHUB%20REPO%20HERE) | 2023      |           |                                                  |
| IDEFICS-2         | What matters when building vision-language models?           | [Paper](https://arxiv.org/abs/2405.02246)                    | [Code](https://huggingface.co/HuggingFaceM4/idefics2)        | 2024      |           |                                                  |
| InstructBLIP      | InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning | [Paper](https://arxiv.org/abs/2305.06500)                    | [Code](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip) | 2023      |           |                                                  |
| InstructBlipVideo | InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning | [Paper](https://arxiv.org/abs/2305.06500)                    | [Code](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip) | 2023      |           |                                                  |
| KOSMOS-2          | Kosmos-2: Grounding Multimodal Large Language Models to the World | [Paper](https://arxiv.org/abs/2306.14824)                    | [Code](https://github.com/microsoft/unilm/tree/master/kosmos-2) | 2023      | microsoft |                                                  |
| LayoutLM          | LayoutLM: Pre-training of Text and Layout for Document Image Understanding | [Paper](https://arxiv.org/abs/1912.13318)                    | Code                                                         | 2019      | microsoft |                                                  |
| LayoutLMV2        | LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding | [Paper](https://arxiv.org/abs/2012.14740)                    | [Code](https://github.com/NielsRogge/Transformers-Tutorials) | 2020      | microsoft |                                                  |
| LayoutLMv3        | LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking | [Paper](https://arxiv.org/abs/2204.08387)                    | [Code](https://github.com/microsoft/unilm/tree/master/layoutlmv3) | 2022      | microsoft |                                                  |
| LayoutXLM         | LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding | [Paper](https://arxiv.org/abs/2104.08836)                    | [Code](https://github.com/microsoft/unilm)                   | 2021      | microsoft |                                                  |
| LiLT              | LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding | [Paper](https://arxiv.org/abs/2202.13669)                    | [Code](https://github.com/jpwang/lilt)                       | 2022      |           |                                                  |
| LLaVa             | Visual Instruction Tuning                                    | [Paper](https://arxiv.org/abs/2304.08485)                    | [Code](https://github.com/haotian-liu/LLaVA/tree/main/llava) | 2023      |           |                                                  |
| LLaVa-VL          | Improved Baselines with Visual Instruction Tuning            | [Paper](https://arxiv.org/pdf/2310.03744)                    | [Code](https://github.com/haotian-liu/LLaVA/tree/main/llava) | 2024      |           |                                                  |
| LLaVA-NeXT        | LLaVA-NeXT: Improved reasoning, OCR, and world knowledge     | [Paper](https://arxiv.org/abs/2310.03744)                    | [Code](https://github.com/haotian-liu/LLaVA/tree/main)       | 2024      |           |                                                  |
| LLaVa-NeXT-Video  | LLaVA-NeXT: A Strong Zero-shot Video Understanding Model     | [Paper](https://llava-vl.github.io/blog/2024-04-30-llava-next-video/) | [Code](https://github.com/LLaVA-VL/LLaVA-NeXT/tree/inference) | 2024      |           |                                                  |
| LXMERT            | LXMERT: Learning Cross-Modality Encoder Representations from Transformers | [Paper](https://arxiv.org/abs/1908.07490)                    | [Code](https://github.com/airsplay/lxmert)                   | 2019      |           |                                                  |
| MatCha            | MatCha：Enhancing Visual Language Pretraining with Math Reasoning and Chart Derrendering | [Paper](https://arxiv.org/abs/2212.09662)                    | [Code](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb) | 2022      | google    |                                                  |
| MGP-STR           | Multi-Granularity Prediction for Scene Text Recognition      | [Paper](https://arxiv.org/abs/2209.03592)                    | [Code](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/MGP-STR) | 2022      |           |                                                  |
| Nougat            | Nougat: Neural Optical Understanding for Academic Documents  | [Paper](https://arxiv.org/abs/2308.13418)                    | [Code](https://github.com/facebookresearch/nougat)           | 2023      |           |                                                  |
| OneFormer         | OneFormer: One Transformer to Rule Universal Image Segmentation | [Paper](https://arxiv.org/abs/2211.06220)                    | [Code](https://github.com/SHI-Labs/OneFormer)                | 2022      |           |                                                  |
| OWL-ViT           | Simple Open-Vocabulary Object Detection with Vision Transformers | [Paper](https://arxiv.org/abs/2205.06230)                    | [Code](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit) | 2022      | google    |                                                  |
| OWLv2             | Scaling Open-Vocabulary Object Detection                     | [Paper](https://arxiv.org/abs/2306.09683)                    | [Code](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit) | 2023      | google    |                                                  |
| PaliGemma         | PaliGemma – Google’s Cutting-Edge Open Vision Language Model | [Paper](https://huggingface.co/blog/paligemma)               | [Code](https://huggingface.co/google/paligemma-3b-pt-224)    | 2024      |           |                                                  |
| Perceiver         | Perceiver IO: A General Architecture for Structured Inputs & Outputs | [Paper](https://arxiv.org/abs/2107.14795)                    | [Code](https://github.com/deepmind/deepmind-research/tree/master/perceiver) | 2021      |           |                                                  |
| Pix2Struct        | Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding | [Paper](https://arxiv.org/abs/2210.03347)                    | [Code](https://github.com/google-research/pix2struct)        | 2022      |           |                                                  |
| SAM               | Segment Anything                                             | [Paper](https://arxiv.org/pdf/2304.02643v1.pdf)              | [Code](https://github.com/facebookresearch/segment-anything) | 2023      | meta      |                                                  |
| SAM v2            | SAM 2: Segment Anything in Images and Videos                 | [Paper](https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/) | [Code](https://github.com/facebookresearch/segment-anything-2) | 2024      |           |                                                  |
| SigLIP            | Sigmoid Loss for Language Image Pre-Training                 | [Paper](https://arxiv.org/abs/2303.15343)                    | [Code](https://github.com/google-research/big_vision/tree/main) | 2023      |           |                                                  |
| TAPAS             | TAPAS: Weakly Supervised Table Parsing via Pre-training      | [Paper](https://www.aclweb.org/anthology/2020.acl-main.398)  | [Code](https://github.com/google-research/tapas)             | 2020      |           |                                                  |
| TrOCR             | TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models | [Paper](https://arxiv.org/abs/2109.10282)                    | [Code](https://github.com/microsoft/unilm/tree/6f60612e7cc86a2a1ae85c47231507a587ab4e01/trocr) | 2021      |           |                                                  |
| TVLT              | TVLT: Textless Vision-Language Transformer                   | [Paper](https://arxiv.org/abs/2209.14156)                    | [Code](https://github.com/zinengtang/TVLT)                   | 2022      |           |                                                  |
| TVP               | Text-Visual Prompting for Efficient 2D Temporal Video Grounding | [Paper](https://arxiv.org/abs/2303.04995)                    | [Code](https://github.com/intel/TVP)                         | 2023      |           |                                                  |
| UDOP              | Unifying Vision, Text, and Layout for Universal Document Processing | [Paper](https://arxiv.org/abs/2212.02623)                    | [Code](https://arxiv.org/abs/2212.02623)                     | 2022      |           |                                                  |
| Video-LLaVA       | Video-LLaVA: Learning United Visual Representation by Alignment Before Projection | [Paper](https://arxiv.org/abs/2311.10122)                    | [Code](https://github.com/PKU-YuanGroup/Video-LLaVA)         | 2023      |           |                                                  |
| ViLT              | ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision | [Paper](https://arxiv.org/abs/2102.03334)                    | [Code](https://github.com/dandelin/ViLT)                     | 2021      |           |                                                  |
| VipLlava          | Making Large Multimodal Models Understand Arbitrary Visual Prompts | [Paper](https://arxiv.org/abs/2312.00784)                    | [Code](https://github.com/mu-cai/ViP-LLaVA)                  | 2023      |           |                                                  |
| VisualBERT        | VisualBERT: A Simple and Performant Baseline for Vision and Language | [Paper](https://arxiv.org/pdf/1908.03557)                    | [Code](https://github.com/uclanlp/visualbert)                | 2019      |           |                                                  |
| X-CLIP            | Expanding Language-Image Pretrained Models for General Video Recognition | [Paper](https://arxiv.org/abs/2208.02816)                    | [Code](https://github.com/microsoft/VideoX/tree/master/X-CLIP) | 2022      |           |                                                  |
|                   |                                                              |                                                              |                                                              |           |           |                                                  |

- Vision Encoder Decoder Models

  - Vision Models

    - [ViT](https://huggingface.co/docs/transformers/model_doc/vit)
    - [BEiT](https://huggingface.co/docs/transformers/model_doc/beit)
    - [Swin](https://huggingface.co/docs/transformers/model_doc/swin)


  - Language Models

    - [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)
    - [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)
    - [BERT](https://huggingface.co/docs/transformers/model_doc/bert)
    - [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)



- Vision TextDual Encoder
  
- Speech Encoder Decoder Models



# Language-Models


| Model / Methods      | Title                                                        | Paper Link                                                   | Code Link                                                    | Published | Keywords | Venue                                                        |
| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | --------- | -------- | ------------------------------------------------------------ |
| ALBERT               | ALBERT: A Lite BERT for Self-supervised Learning of Language Representations | [Paper](https://arxiv.org/abs/1909.11942)                    | [Code](https://github.com/google-research/ALBERT)            | 2019      |          |                                                              |
| BART                 | BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension | [Paper](https://arxiv.org/abs/1910.13461)                    | [Code](https://github.com/facebookresearch/fairseq/tree/main/examples/bart) | 2019      |          |                                                              |
| BARThez              | BARThez: a Skilled Pretrained French Sequence-to-Sequence Model | [Paper](https://arxiv.org/abs/2010.12321)                    | [Code](https://github.com/moussaKam/BARThez)                 | 2020      |          |                                                              |
| BARTpho              | BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese | [Paper](https://arxiv.org/abs/2109.09701)                    | [Code](https://github.com/VinAIResearch/BARTpho)             | 2022      |          | INTERSPEECH 2022                                             |
| BERT                 | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | [Paper](https://arxiv.org/abs/1810.04805)                    | [Code](https://github.com/google-research/bert)              | 2018      |          |                                                              |
| BertGeneration       | Leveraging Pre-trained Checkpoints for Sequence Generation Tasks | [Paper](https://arxiv.org/abs/1907.12461)                    | Code                                                         | 2019      |          |                                                              |
| BERTweet             | BERTweet: A pre-trained language model for English Tweets    | [Paper](https://www.aclweb.org/anthology/2020.emnlp-demos.2.pdf) | [Code](https://github.com/VinAIResearch/BERTweet)            | 2020      |          | EMNLP-2020                                                   |
| BigBird              | Big Bird: Transformers for Longer Sequences                  | [Paper](https://arxiv.org/abs/2007.14062)                    | [Code](https://github.com/google-research/bigbird)           | 2020      |          | [NeurIPS 2020](https://papers.nips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html) |
| BioGPT               | BioGPT: generative pre-trained transformer for biomedical text generation and mining | [Paper](https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9) | [Code](https://github.com/microsoft/BioGPT)                  | 2022      |          |                                                              |
| Blenderbot           | Recipes for building an open-domain chatbot                  | [Paper](https://arxiv.org/pdf/2004.13637.pdf)                | [Code](https://github.com/facebookresearch/ParlAI)           | 2020      |          |                                                              |
| BLOOM                | Introducing The World’s Largest Open Multilingual Language Model: BLOOM | [Blog](https://bigscience.huggingface.co/blog/bloom)         | Code                                                         | 2022      |          |                                                              |
| BORT                 | Optimal Subarchitecture Extraction for BERT                  | [Paper](https://arxiv.org/abs/2010.10499)                    | [Code](https://github.com/alexa/bort/)                       | 2020      |          |                                                              |
| ByT5                 | ByT5: Towards a token-free future with pre-trained byte-to-byte models | [Paper](https://arxiv.org/abs/2105.13626)                    | [Code](https://github.com/google-research/byt5)              | 2021      |          |                                                              |
| CamemBERT            | CamemBERT: a Tasty French Language Model                     | [Paper](https://arxiv.org/abs/1911.03894)                    | [Code](https://camembert-model.fr/)                          | 2019      |          |                                                              |
| CANINE               | CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation | [Paper](https://arxiv.org/abs/2103.06874)                    | [Code](https://github.com/google-research/language/tree/master/language/canine) | 2021      |          |                                                              |
| CodeGen              | A Conversational Paradigm for Program Synthesis              | [Paper](https://arxiv.org/abs/2203.13474)                    | [Code](https://github.com/salesforce/codegen)                | 2022      |          |                                                              |
| CodeLlama            | Code Llama: Open Foundation Models for Code                  | [Paper](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/) | [Code](https://github.com/facebookresearch/llama)            | 2023      |          |                                                              |
| Cohere               | Command-R: Retrieval Augmented Generation at Production Scale | [Paper](https://cohere.com/blog/command-r)                   | [Code](https://github.com/EleutherAI/gpt-neox)               | 2024      |          |                                                              |
| ConvBERT             | ConvBERT: Improving BERT with Span-based Dynamic Convolution | [Paper](https://arxiv.org/abs/2008.02496)                    | [Code](https://github.com/yitu-opensource/ConvBert)          | 2020      |          |                                                              |
| CPM                  | CPM: A Large-scale Generative Chinese Pre-trained Language Model | [Paper](https://arxiv.org/abs/2012.00413)                    | [Code](https://github.com/TsinghuaAI/CPM-Generate)           | 2020      |          |                                                              |
| CPMAnt               | CPM-Ant is an open-source Chinese pre-trained language model (PLM) with 10B parameters. | [Paper](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live) | [Code](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live) | 2020      |          |                                                              |
| CTRL                 | CTRL: A Conditional Transformer Language Model for Controllable Generation | [Paper](https://arxiv.org/abs/1909.05858)                    | [Code](https://github.com/salesforce/ctrl)                   | 2019      |          |                                                              |
| DBRX                 | DBRX is a [transformer-based](https://www.isattentionallyouneed.com/) decoder-only large language model (LLM) that was trained using next-token prediction. | [Blog](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm) | [Code](https://github.com/databricks/dbrx-instruct)          | 2024      |          |                                                              |
| DeBERTa              | DeBERTa：Decoding-enhanced BERT with Disentangled Attention  | [Paper](https://arxiv.org/abs/2006.03654)                    | [Code](https://github.com/microsoft/DeBERTa)                 | 2020      |          |                                                              |
| DeBERTa-v2           | DeBERTa：Decoding-enhanced BERT with Disentangled Attention  | [Paper](https://arxiv.org/abs/2006.03654)                    | [Code](https://github.com/microsoft/DeBERTa)                 | 2020      |          |                                                              |
| DialoGPT             | DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation | [Paper](https://arxiv.org/abs/1911.00536)                    | [Code](https://github.com/microsoft/DialoGPT)                | 2019      |          |                                                              |
| DistilBERT           | DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter | [Paper](https://arxiv.org/abs/1910.01108)                    | [Code](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) | 2019      |          |                                                              |
| DPR                  | Dense Passage Retrieval for Open-Domain Question Answering   | [Paper](https://arxiv.org/abs/2004.04906)                    | [Code](https://github.com/facebookresearch/DPR)              | 2020      |          |                                                              |
| ELECTRA              | ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators | [Paper](https://openreview.net/pdf?id=r1xMH1BtvB)            | [Code](https://github.com/google-research/electra)           | 2020      |          |                                                              |
| ERNIE 1.0            | ERNIE: Enhanced Representation through Knowledge Integration | [Paper](https://arxiv.org/abs/1904.09223)                    | [Code](https://github.com/PaddlePaddle/ERNIE/tree/repro)     | 2019      |          |                                                              |
| ERNIE 2.0            | ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding | [Paper](https://ojs.aaai.org/index.php/AAAI/article/view/6428) | [Code](https://github.com/PaddlePaddle/ERNIE/blob/repro/README.zh.md) | 2020      |          | AAAI 2020                                                    |
| ERNIE 3.0            | ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation | [Paper](https://arxiv.org/abs/2107.02137)                    | [Code](https://github.com/PaddlePaddle/ERNIE/blob/repro/README.zh.md) | 2021      |          |                                                              |
| ERNIE-Gram           | ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding | [Paper](https://arxiv.org/abs/2010.12148)                    | [Code](https://github.com/PaddlePaddle/ERNIE/blob/repro/README.zh.md) | 2020      |          |                                                              |
| ERNIE-health         | Building Chinese Biomedical Language Models via Multi-Level Text Discrimination | [Paper](https://arxiv.org/abs/2110.07244)                    | [Code](https://github.com/PaddlePaddle/ERNIE/blob/repro/README.zh.md) | 2022      |          |                                                              |
| ErnieM               | ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora | [Paper](https://arxiv.org/abs/2012.15674)                    | [Code](https://github.com/PaddlePaddle/ERNIE/blob/repro/README.zh.md) | 2020      |          |                                                              |
| ESM                  | Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences | [Paper](https://www.pnas.org/content/118/15/e2016239118)     | [Code](https://github.com/facebookresearch/esm)              | 2022      |          |                                                              |
| Falcon               | The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only | [Paper](https://arxiv.org/abs/2306.01116)                    | [Code](https://github.com/huggingface/transformers/tree/main/src/transformers/models/falcon) | 2023      |          |                                                              |
| FastSpeech2Conformer | Recent Developments On Espnet Toolkit Boosted By Conformer   | [Paper](https://arxiv.org/abs/2010.13956)                    | [Code](https://github.com/espnet/espnet/blob/master/espnet2/tts/fastspeech2/fastspeech2.py) | 2020      |          |                                                              |
| FLAN-T5              | Scaling Instruction-Finetuned Language Models                | [Paper](https://arxiv.org/pdf/2210.11416.pdf)                | [Code](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints) | 2022      |          |                                                              |
| FLAN-UL2             | UL2: Unifying Language Learning Paradigms                    | [Paper](https://arxiv.org/pdf/2210.11416.pdf)                | [Code](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints) | 2022      |          |                                                              |
| FlauBERT             | FlauBERT：Unsupervised Language Model Pre-training for French | [Paper](https://arxiv.org/abs/1912.05372)                    | [Code](https://github.com/getalp/Flaubert)                   | 2019      |          |                                                              |
| FNet                 | FNet: Mixing Tokens with Fourier Transforms                  | [Paper](https://arxiv.org/abs/2105.03824)                    | [Code](https://github.com/google-research/google-research/tree/master/f_net) | 2021      |          |                                                              |
| FSMT                 | Facebook FAIR’s WMT19 News Translation Task Submission       | [Paper](https://arxiv.org/abs/1907.06616)                    | [Code](https://github.com/pytorch/fairseq/tree/master/examples/wmt19) | 2019      |          |                                                              |
| Funnel Transformer   | Funnel-Transformer：Filtering out Sequential Redemption for Efficient Language Processing | [Paper](https://arxiv.org/abs/2006.03236)                    | [Code](https://github.com/laiguokun/Funnel-Transformer)      | 2020      |          |                                                              |
| Fuyu                 | Fuyu-8B: A Multimodal Architecture for AI Agents             | [Blog](https://www.adept.ai/blog/fuyu-8b)                    | [Code](https://github.com/persimmon-ai-labs/adept-inference) | 2023      |          |                                                              |
| Gemma                | Gemma：Open Models Based on Gemini Technology and Research   | [Blog](https://blog.google/technology/developers/gemma-open-models/) | [Code](https://www.kaggle.com/models/google/gemma)           | 2023      |          |                                                              |
| Gemma2               | Gemma2: Open Models Based on Gemini Technology and Research  | [Blog](https://blog.google/technology/developers/google-gemma-2/) | [Code](https://www.kaggle.com/models/google/gemma)           | 2023      |          |                                                              |
| OpenAI GPT           | Improving Language Understanding by Generative Pre-Training  | [Paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) | [Code](https://github.com/openai/finetune-transformer-lm)    | 2018      |          |                                                              |
| GPT Neo              | The Pile: An 800GB Dataset of Diverse Text for Language Modeling | [Paper](https://arxiv.org/abs/2101.00027)                    | [Code](https://github.com/EleutherAI/gpt-neo)                | 2020      |          |                                                              |
| GPTBigCode           | SantaCoder: don't reach for the stars!                       | [Paper](https://arxiv.org/abs/2301.03988)                    | Code                                                         | 2023      |          |                                                              |
| OpenAI GPT2          | Language Models are Unsupervised Multitask Learners          | [Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) | [Code](https://openai.com/index/better-language-models/)     | 2019      | 1.5B     |                                                              |
| GPT-Sw3              | Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish | [Paper](http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.376.pdf) | Code                                                         | 2022      |          |                                                              |
| HerBERT              | KLEJ: Comprehensive Benchmark for Polish Language Understanding | [Paper](https://www.aclweb.org/anthology/2020.acl-main.111.pdf) | [Code](https://github.com/allegro/HerBERT)                   | 2020      |          |                                                              |
| I-BERT               | I-BERT: Integer-only BERT Quantization                       | [Paper](https://arxiv.org/abs/2101.01321)                    | [Code](https://github.com/kssteven418/I-BERT)                | 2021      |          |                                                              |
| Jamba                | Introducing Jamba: AI21's Groundbreaking SSM-Transformer Model | [Blog](https://www.ai21.com/blog/announcing-jamba)           | Code                                                         | 2024      |          |                                                              |
| Jukebox              | Jukebox: A generative model for music                        | [Paper](https://arxiv.org/pdf/2005.00341.pdf)                | [Code](https://github.com/openai/jukebox)                    | 2020      |          |                                                              |
| LED                  | Longformer: The Long-Document Transformer                    | [Paper](https://arxiv.org/abs/2004.05150)                    | [Code](https://colab.research.google.com/drive/12INTTR6n64TzS4RrXZxMSXfrOd9Xzamo?usp=sharing) | 2020      |          |                                                              |
| LLaMA                | LLaMA: Open and Efficient Foundation Language Models         | [Paper](https://arxiv.org/abs/2302.13971)                    | [Code](https://github.com/meta-llama/llama)                  | 2023      |          |                                                              |
| Llama2               | LLaMA: Open Foundation and Fine-Tuned Chat Models            | [Paper](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) | [Code](https://github.com/facebookresearch/llama)            | 2023      |          |                                                              |
| Llama3               | Introducing Meta Llama 3: The most capable openly available LLM to date | [Paper](https://ai.meta.com/blog/meta-llama-3/)              | [Code](https://github.com/meta-llama/llama3)                 | 2024      |          |                                                              |
| Longformer           | Longformer: The Long-Document Transformer                    | [Paper](https://arxiv.org/pdf/2004.05150.pdf)                | [Code](https://github.com/allenai/longformer)                | 2020      |          |                                                              |
| LongT5               | LongT5: Efficient Text-To-Text Transformer for Long Sequences | [Paper](https://arxiv.org/abs/2112.07916)                    | [Code](https://github.com/google-research/longt5)            | 2021      |          |                                                              |
| LUKE                 | LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention | [Paper](https://arxiv.org/abs/2010.01057)                    | [Code](https://github.com/studio-ousia/luke)                 | 2020      |          |                                                              |
| M2M100               | Beyond English-Centric Multilingual Machine Translation      | [Paper](https://arxiv.org/abs/2010.11125)                    | Code                                                         | 2020      |          |                                                              |
| MADLAD-400           | MADLAD-400：A Multilingual And Document-Level Large Audited Dataset]（MADLAD-400：A Multilingual And Document-Level Large Audited Dataset | [Paper](https://arxiv.org/abs/2309.04662)                    | [Code](https://github.com/google-research/google-research/tree/master/madlad_400) | 2023      |          |                                                              |
| Mamba                | Mamba：Linear-Time Sequence Modeling with Selective State Spaces | [Paper](https://arxiv.org/abs/2312.00752)                    | [Code](https://github.com/state-spaces/mamba)                | 2024      |          |                                                              |
| MarianMT             | A framework for translation models, using the same models as BART. | [Blog](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/marian.md) | [Code](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/marian.md) | 2024      |          |                                                              |
| MarkupLM             | MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding | [Paper](https://arxiv.org/abs/2110.08518)                    | [Code](https://github.com/microsoft/unilm/tree/master/markuplm) | 2021      |          |                                                              |
| MBart and MBart-50   | Multilingual Denoising Pre-training for Neural Machine Translation | [Paper](https://arxiv.org/abs/2001.08210)                    | [Code](https://github.com/pytorch/fairseq/tree/master/examples/mbart) | 2020      |          |                                                              |
| Mega                 | Mega: Moving Average Equipped Gated Attention                | [Paper](https://arxiv.org/abs/2209.10655)                    | [Code](https://github.com/facebookresearch/mega)             | 2022      |          |                                                              |
| MegatronBERT         | Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism | [Paper](https://arxiv.org/abs/1909.08053)                    | [Code](https://github.com/NVIDIA/Megatron-LM)                | 2019      |          |                                                              |
| MegatronGPT2         | Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism | [Paper](https://arxiv.org/abs/1909.08053)                    | [Code](https://github.com/NVIDIA/Megatron-LM)                | 2019      |          |                                                              |
| Mistral              | Mistral-7B is a decoder-only Transformer                     | [Blog](https://mistral.ai/news/announcing-mistral-7b/)       | Code                                                         | 2023      |          |                                                              |
| Mixtral              | a high-quality sparse mixture of experts models (SMoE) with open weights. | [Blog](https://mistral.ai/news/mixtral-of-experts/)          | Code                                                         | 2023      |          |                                                              |
| mLUKE                | mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models | [Paper](https://arxiv.org/abs/2110.08151)                    | [Code](https://github.com/studio-ousia/luke)                 | 2021      |          |                                                              |
| MobileBERT           | MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices | [Paper](https://arxiv.org/abs/2004.02984)                    | [Code](https://github.com/google-research/google-research/tree/master/mobilebert) | 2020      |          |                                                              |
| MPNet                | MPNet：Masked and Permuted Pre-training for Language Understanding | [Paper](https://arxiv.org/abs/2004.09297)                    | [Code](https://github.com/microsoft/MPNet)                   | 2020      |          |                                                              |
| MPT                  | MPT models are GPT-style decoder-only transformers with several improvements | [Blog](https://www.mosaicml.com/blog/mpt-7b)                 | [Code](https://github.com/mosaicml/llm-foundry/tree/main)    | 2023      |          |                                                              |
| MRA                  | Multi Resolution Analysis (MRA) for Approximate Self-Attention | [Paper](https://arxiv.org/abs/2207.10284)                    | [Code](https://github.com/mlpen/mra-attention)               | 2022      |          |                                                              |
| MT5                  | mT5: A massively multilingual pre-trained text-to-text transformer | [Paper](https://arxiv.org/abs/2010.11934)                    | [Code](https://github.com/google-research/multilingual-t5)   | 2020      |          |                                                              |
| MVP                  | MVP: Multi-task Supervised Pre-training for Natural Language Generation | [Paper](https://arxiv.org/abs/2206.12131)                    | [Code](https://github.com/RUCAIBox/MVP)                      | 2022      |          |                                                              |
| Nezha                | NEZHA: Neural Contextualized Representation for Chinese Language Understanding | [Paper](https://arxiv.org/abs/1909.00204)                    | [Code](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-PyTorch) | 2019      |          |                                                              |
| NLLB                 | No Language Left Behind: Scaling Human-Centered Machine Translation | [Paper](https://arxiv.org/abs/2207.04672)                    | [Code](https://github.com/facebookresearch/fairseq/tree/nllb) | 2022      |          |                                                              |
| NLLB-MOE             | No Language Left Behind: Scaling Human-Centered Machine Translation | [Paper](https://arxiv.org/abs/2207.04672)                    | [Code](https://github.com/facebookresearch/fairseq)          | 2022      |          |                                                              |
| Nyströmformer        | Nyströmformer：A Nyström-Based Algorithm for Approximating Self-Attention | [Paper](https://arxiv.org/abs/2102.03902)                    | [Code](https://github.com/mlpen/Nystromformer)               | 2021      |          |                                                              |
| OLMo                 | OLMo: Accelerating the Science of Language Models            | [Paper](https://arxiv.org/abs/2402.00838)                    | [Code](https://github.com/allenai/OLMo/tree/main/olmo)       | 2024      |          |                                                              |
| Open-Llama           | The Open-Llama model was proposed in the open source Open-Llama project by community developer s-JoL. | Paper                                                        | [Code](https://huggingface.co/models?search=openllama)       | 2023      |          |                                                              |
| OPT                  | Open Pre-trained Transformer Language Models                 | [Paper](https://arxiv.org/pdf/2205.01068)                    | [Code](https://github.com/facebookresearch/metaseq)          | 2022      |          |                                                              |
| Pegasus              | PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization | [Paper](https://arxiv.org/pdf/1912.08777.pdf)                | [Code](https://github.com/google-research/pegasus)           | 2019      |          |                                                              |
| PEGASUS-X            | Investigating Efficiently Extending Transformers for Long Input Summarization | [Paper](https://arxiv.org/abs/2208.04347)                    | [Code](https://github.com/google-research/pegasus)           | 2022      |          |                                                              |
| Persimmon            | Persimmon-8B is a fully permissively-licensed model with approximately 8 billion parameters | Paper                                                        | [Code](https://github.com/persimmon-ai-labs/adept-inference) | 2022      |          |                                                              |
| Phi                  | Textbooks Are All You Need                                   | [Paper](https://arxiv.org/abs/2309.05463)                    | [Code](https://huggingface.co/microsoft/phi-1)               | 2023      |          |                                                              |
| Phi-3                | Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone | [Report](https://arxiv.org/abs/2404.14219)                   | [Code](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) | 2024      |          |                                                              |
| PhoBERT              | PhoBERT: Pre-trained language models for Vietnamese          | [Paper](https://www.aclweb.org/anthology/2020.findings-emnlp.92.pdf) | [Code](https://github.com/VinAIResearch/PhoBERT)             | 2022      |          |                                                              |
| PLBart               | Unified Pre-training for Program Understanding and Generation | [Paper](https://arxiv.org/abs/2103.06333)                    | [Code](https://github.com/wasiahmad/PLBART)                  | 2021      |          |                                                              |
| ProphetNet           | ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training | [Paper](https://arxiv.org/abs/2001.04063)                    | [Code](https://github.com/microsoft/ProphetNet)              | 2020      |          |                                                              |
| QDQBERT              | Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation | [Paper](https://arxiv.org/abs/2004.09602)                    | Code                                                         | 2020      |          |                                                              |
| Qwen                 | Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. | [Paper](https://arxiv.org/abs/2309.16609)                    | [Code](https://github.com/QwenLM/Qwen)                       | 2023      |          |                                                              |
| Qwen2                | Qwen2 is the new model series of large language models from the Qwen team. | [Paper](https://arxiv.org/abs/2407.10671)                    | [Code](https://github.com/QwenLM/Qwen2)                      | 2024      |          |                                                              |
| Qwen-VL              | Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond | [Paper](https://arxiv.org/abs/2308.12966)                    | [Code](https://github.com/QwenLM/Qwen-VL)                    | 2023      |          |                                                              |
| Qwen2MoE             | Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters | [Blog](https://qwenlm.github.io/blog/qwen-moe/)              | [Code](https://huggingface.co/Qwen)                          | 2024      |          |                                                              |
| REALM                | REALM: Retrieval-Augmented Language Model Pre-Training       | [Paper](https://arxiv.org/abs/2002.08909)                    | [Code](https://github.com/google-research/language/tree/master/language/realm) | 2020      |          |                                                              |
| RecurrentGemma       | RecurrentGemma: Moving Past Transformers for Efficient Open Language Models | [Paper](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf) | [Code](https://github.com/google-deepmind/recurrentgemma)    | 2024      |          |                                                              |
| Reformer             | Reformer: The Efficient Transformer                          | [Paper](https://arxiv.org/abs/2001.04451.pdf)                | [Code](https://github.com/google/trax/tree/master/trax/models/reformer) | 2020      |          |                                                              |
| RemBERT              | Rethinking Embedding Coupling in Pre-trained Language Models | [Paper](https://arxiv.org/abs/2010.12821)                    | Code                                                         | 2020      |          |                                                              |
| RetriBERT            | Explain Anything Like I’m Five: A Model for Open Domain Long Form Question Answering | [Blog](https://yjernite.github.io/lfqa.html)                 | [Code](https://github.com/huggingface/transformers/tree/main/examples/research-projects/distillation) | 2020      |          |                                                              |
| RoBERTa              | RoBERTa: A Robustly Optimized BERT Pretraining Approach      | [Paper](https://arxiv.org/abs/1907.11692)                    | [Code](https://github.com/pytorch/fairseq/tree/master/examples/roberta) | 2019      |          |                                                              |
| RoBERTa-PreLayerNorm | fairseq: A Fast, Extensible Toolkit for Sequence Modeling    | [Paper](https://arxiv.org/abs/1904.01038)                    | [Code](https://github.com/princeton-nlp/DinkyTrain)          | 2022      |          |                                                              |
| RoCBert              | RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining | [Paper](https://aclanthology.org/2022.acl-long.65.pdf)       | Code                                                         | 2022      |          |                                                              |
| RoFormer             | RoFormer: Enhanced Transformer with Rotary Position Embedding | [Paper](https://arxiv.org/pdf/2104.09864v1.pdf)              | [Code](https://github.com/ZhuiyiTechnology/roformer)         | 2021      |          |                                                              |
| RWKV-LM              | RWKV is an RNN with transformer-level LLM performance        | Paper                                                        | [Code](https://github.com/BlinkDL/RWKV-LM)                   | 2022      |          |                                                              |
| RWKV-4.0             | RWKV: Reinventing RNNs for the Transformer Era               | [Paper](https://arxiv.org/abs/2305.13048)                    | [Code](https://github.com/BlinkDL/RWKV-LM)                   | 2023      |          |                                                              |
| RWKV-5/6 Eagle/Finch | Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence | [Paper](https://arxiv.org/abs/2404.05892)                    | [Code](https://github.com/BlinkDL/RWKV-LM)                   | 2024      |          |                                                              |
| Splinter             | Few-Shot Question Answering by Pretraining Span Selection    | [Paper](https://arxiv.org/abs/2101.00438)                    | [Code](https://github.com/oriram/splinter)                   | 2021      |          |                                                              |
| SqueezeBERT          | SqueezeBERT: What can computer vision teach NLP about efficient neural networks? | [Paper](https://arxiv.org/abs/2006.11316)                    | Code                                                         | 2020      |          |                                                              |
| StableLM             | StableLM-3B-4E1T                                             | [Paper](https://stability.wandb.io/stability-llm/stable-lm/reports/StableLM-3B-4E1T--VmlldzoyMjU4?accessToken=u3zujipenkx5g7rtcj9qojjgxpconyjktjkli2po09nffrffdhhchq045vp0wyfo) | Code                                                         | 2024      |          |                                                              |
| Starcoder2           | StarCoder 2 and The Stack v2: The Next Generation            | [Paper](https://arxiv.org/abs/2402.19173)                    | Code                                                         | 2024      |          |                                                              |
| SwitchTransformers   | Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity | [Paper](https://arxiv.org/abs/2101.03961)                    | [Code](https://github.com/google/flaxformer/tree/main/flaxformer/architectures/moe) | 2021      |          |                                                              |
| T5                   | Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer | [Paper](https://arxiv.org/pdf/1910.10683.pdf)                | [Code](https://github.com/google-research/text-to-text-transfer-transformer) | 2023      |          |                                                              |
| T5v1.1               | Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer | [Paper](https://arxiv.org/pdf/1910.10683.pdf)                | [Code](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511) | 2023      |          |                                                              |
| TAPEX                | TAPEX: Table Pre-training via Learning a Neural SQL Executor | [Paper](https://arxiv.org/abs/2107.07653)                    | Code                                                         | 2021      |          |                                                              |
| Transformer XL       | Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context | [Paper](https://arxiv.org/abs/1901.02860)                    | [Code](https://github.com/kimiyoung/transformer-xl)          | 2019      |          |                                                              |
| UL2                  | Unifying Language Learning Paradigms                         | [Paper](https://arxiv.org/pdf/2205.05131v1.pdf)              | [Code](https://github.com/google-research/google-research/tree/master/ul2) | 2022      |          |                                                              |
| UMT5                 | UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining | [Paper](https://openreview.net/forum?id=kXwdL1cWOAi)         | [Code](https://github.com/google-research/t5x)               | 2024      |          |                                                              |
| X-MOD                | Lifting the Curse of Multilinguality by Pre-training Modular Transformers | [Paper](http://dx.doi.org/10.18653/v1/2022.naacl-main.255)   | [Code](https://github.com/facebookresearch/fairseq/tree/58cc6cca18f15e6d56e3f60c959fe4f878960a60/fairseq/models/xmod) | 2022      |          |                                                              |
| XGLM                 | Few-shot Learning with Multilingual Language Models          | [Paper](https://arxiv.org/abs/2112.10668)                    | [Code](https://github.com/pytorch/fairseq/tree/main/examples/xglm) | 2021      |          |                                                              |
| XLM                  | Cross-lingual Language Model Pretraining                     | [Paper](https://arxiv.org/abs/1901.07291)                    | [Code](https://github.com/facebookresearch/XLM/)             | 2019      |          |                                                              |
| XLM-ProphetNet       | ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training | [Paper](https://arxiv.org/abs/2001.04063)                    | [Code](https://github.com/microsoft/ProphetNet)              | 2020      |          |                                                              |
| XLM-RoBERTa          | Unsupervised Cross-lingual Representation Learning at Scale  | [Paper](https://arxiv.org/abs/1911.02116)                    | [Code](https://github.com/pytorch/fairseq/tree/master/examples/xlmr) | 2019      |          |                                                              |
| XLM-RoBERTa-XL       | Larger-Scale Transformers for Multilingual Masked Language Modeling | [Paper](https://arxiv.org/abs/2105.00572)                    | [Code](https://github.com/pytorch/fairseq/tree/master/examples/xlmr) | 2021      |          |                                                              |
| XLM-V                | XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models | [Paper](https://arxiv.org/abs/2301.10472)                    | [Code](https://github.com/stefan-it/xlm-v-experiments)       | 2023      |          |                                                              |
| XLNet                | XLNet: Generalized Autoregressive Pretraining for Language Understanding | [Paper](https://arxiv.org/abs/1906.08237)                    | [Code](https://github.com/zihangdai/xlnet/)                  | 2019      |          |                                                              |
| YOSO                 | You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling | [Paper](https://arxiv.org/abs/2111.09714)                    | [Code](https://github.com/mlpen/YOSO)                        | 2021      |          |                                                              |
|                      |                                                              |                                                              |                                                              |           |          |                                                              |

- Encoder Decoder Models
- RAG



# Vision Models


| Model / Methods     | Title                                                        | Paper Link                                  | Code Link                                                    | Published | Keywords         | Venue        |
| ------------------- | ------------------------------------------------------------ | ------------------------------------------- | ------------------------------------------------------------ | --------- | ---------------- | ------------ |
| BEiT                | BEiT: BERT Pre-Training of Image Transformers                | [Paper](https://arxiv.org/abs/2106.08254)   | [Code](https://github.com/microsoft/unilm/tree/master/beit)  | 2022      |                  |              |
| BiT                 | Big Transfer (BiT): General Visual Representation Learning   | [Paper](https://arxiv.org/abs/1912.11370)   | [Code](https://github.com/google-research/big_transfer)      | 2020      |                  |              |
| Conditional DETR    | Conditional DETR for Fast Training Convergence               | [Paper](https://arxiv.org/abs/2108.06152)   | [Code](https://github.com/Atten4Vis/ConditionalDETR)         | 2023      |                  |              |
| ConvNeXT            | A ConvNet for the 2020s                                      | [Paper](https://arxiv.org/abs/2201.03545)   | [Code](https://github.com/facebookresearch/ConvNeXt)         | 2022      |                  |              |
| ConvNeXt V2         | ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders | [Paper](https://arxiv.org/abs/2301.00808)   | [Code](https://github.com/facebookresearch/ConvNeXt-V2)      | 2023      |                  |              |
| CvT                 | CvT: Introducing Convolutions to Vision Transformers         | [Paper](https://arxiv.org/abs/2103.15808)   | [Code](https://github.com/microsoft/CvT)                     | 2021      |                  |              |
| Deformable DETR     | Deformable DETR: Deformable Transformers for End-to-End Object Detection | [Paper](https://arxiv.org/abs/2010.04159)   | [Code](https://github.com/fundamentalvision/Deformable-DETR) | 2020      |                  |              |
| DeiT                | Training data-efficient image transformers & distillation through attention | [Paper](https://arxiv.org/abs/2012.12877)   | Code                                                         | 2021      |                  |              |
| Depth Anything      | Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data | [Paper](https://arxiv.org/abs/2401.10891)   | [Code](https://github.com/LiheYoung/Depth-Anything)          | 2024      |                  |              |
| Depth Anything V2   | Depth Anything V2                                            | [Paper](https://arxiv.org/abs/2406.09414)   | [Code](https://github.com/DepthAnything/Depth-Anything-V2)   | 2024      |                  |              |
| DETA                | NMS Strikes Back                                             | [Paper](https://arxiv.org/abs/2212.06137)   | [Code](https://github.com/jozhang97/DETA)                    | 2021      |                  |              |
| DETR                | End-to-End Object Detection with Transformers                | [Paper](https://arxiv.org/abs/2005.12872)   | [Code](https://github.com/facebookresearch/detr)             | 2020      |                  |              |
| DiNAT               | Dilated Neighborhood Attention Transformer                   | [Paper](https://arxiv.org/abs/2209.15001)   | [Code](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer) | 2022      |                  |              |
| DINOv2              | DINOv2: Learning Robust Visual Features without Supervision  | [Paper](https://arxiv.org/abs/2304.07193)   | [Code](https://github.com/facebookresearch/dinov2)           | 2023      |                  |              |
| DiT                 | DiT: Self-supervised Pre-training for Document Image Transformer | [Paper](https://arxiv.org/abs/2203.02378)   | [Code](https://github.com/microsoft/unilm/tree/master/dit)   | 2022      |                  |              |
| DPT                 | Vision Transformers for Dense Prediction                     | [Paper](https://arxiv.org/abs/2103.13413)   | [Code](https://github.com/isl-org/DPT)                       | 2021      |                  |              |
| EfficientFormer     | EfficientFormer: Vision Transformers at MobileNet Speed      | [Paper](https://arxiv.org/abs/2206.01191)   | [Code](https://github.com/snap-research/EfficientFormer)     | 2022      |                  | NeurIPs 2022 |
| EfficientFormerV2   | Rethinking Vision Transformers for MobileNet Size and Speed  | [Paper](https://arxiv.org/abs/2212.08059)   | [Code](https://github.com/snap-research/EfficientFormer)     | 2023      |                  | ICCV 2023    |
| EfficientNet        | EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks | [Paper](https://arxiv.org/abs/1905.11946)   | [Code](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet) | 2019      |                  |              |
| FocalNet            | Focal Modulation Networks                                    | [Paper](https://arxiv.org/abs/2203.11926)   | [Code](https://github.com/microsoft/FocalNet)                | 2022      |                  |              |
| GLPN                | Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth | [Paper](https://arxiv.org/abs/2201.07436)   | [Code](https://github.com/vinvino02/GLPDepth)                | 2022      |                  |              |
| Hiera               | Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles | [Paper](https://arxiv.org/abs/2306.00989)   | [Code](https://github.com/facebookresearch/hiera)            | 2023      |                  |              |
| OpenAI ImageGPT     | Generative Pretraining from Pixels                           | [Paper](https://openai.com/blog/image-gpt)  | [Code](https://github.com/openai/image-gpt)                  | 2020      |                  | ICML 2020    |
| LeViT               | LeViT: Introducing Convolutions to Vision Transformers       | [Paper](https://arxiv.org/abs/2104.01136)   | [Code](https://github.com/facebookresearch/LeViT)            | 2021      |                  |              |
| Mask2Former         | Masked-attention Mask Transformer for Universal Image Segmentation | [Paper](https://arxiv.org/abs/2112.01527)   | [Code](https://github.com/facebookresearch/Mask2Former)      | 2022      |                  |              |
| MaskFormer          | Per-Pixel Classification is Not All You Need for Semantic Segmentation | [Paper](https://arxiv.org/abs/2107.06278)   | Code                                                         | 2021      |                  | NeurIPS 2021 |
| MobileNet V1        | MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications | [Paper](https://arxiv.org/abs/1704.04861)   | [Code](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md) | 2017      |                  |              |
| MobileNet V2        | MobileNetV2: Inverted Residuals and Linear Bottlenecks       | [Paper](https://arxiv.org/abs/1801.04381)   | [Code](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet) | 2018      |                  |              |
| MobileViT           | MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer | [Paper](https://arxiv.org/abs/2110.02178)   | [Code](https://github.com/apple/ml-cvnets)                   | 2021      | apple            |              |
| MobileViT V2        | Separable Self-attention for Mobile Vision Transformers      | [Paper](https://arxiv.org/abs/2206.02680)   | [Code](https://github.com/apple/ml-cvnets)                   | 2021      | apple            |              |
| NAT                 | Neighborhood Attention Transformer                           | [Paper](https://arxiv.org/abs/2204.07143)   | [Code](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer) | 2022      |                  |              |
| PoolFormer          | MetaFormer is Actually What You Need for Vision              | [Paper](https://arxiv.org/abs/2111.11418)   | [Code](https://github.com/sail-sg/poolformer)                | 2021      |                  |              |
| PVT                 | Pyramid Vision Transformer (PVT)                             | [Paper](https://arxiv.org/abs/2102.12122)   | [Code](https://github.com/whai362/PVT)                       | 2021      |                  |              |
| PVTv2               | PVT v2: Improved Baselines with Pyramid Vision Transformer   | [Paper](https://arxiv.org/abs/2106.13797)   | [Code](https://github.com/whai362/PVT)                       | 2021      |                  |              |
| RegNet              | Designing Network Design Spaces                              | [Paper](https://arxiv.org/abs/2003.13678)   | [Code](https://github.com/facebookresearch/pycls)            | 2020      |                  |              |
| ResNet              | Deep Residual Learning for Image Recognition                 | [Paper](https://arxiv.org/abs/1512.03385)   | [Code](https://github.com/KaimingHe/deep-residual-networks)  | 2015      |                  |              |
| RT-DETR             | DETRs Beat YOLOs on Real-time Object Detection               | [Paper](https://arxiv.org/abs/2304.08069)   | [Code](https://github.com/lyuwenyu/RT-DETR/)                 | 2023      |                  |              |
| SegFormer           | SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers | [Paper](https://arxiv.org/abs/2105.15203)   | [Code](https://github.com/NVlabs/SegFormer)                  | 2021      |                  |              |
| SegGPT              | SegGPT: Segmenting Everything In Context                     | [Paper](https://arxiv.org/abs/2304.03284)   | [Code](https://huggingface.co/docs/transformers/model_doc/%5B(https://github.com/baaivision/Painter/tree/main)) | 2023      |                  |              |
| SuperPoint          | SuperPoint: Self-Supervised Interest Point Detection and Description | [Paper](https://arxiv.org/abs/1712.07629)   | [Code](https://github.com/magicleap/SuperPointPretrainedNetwork) | 2017      |                  |              |
| SwiftFormer         | SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications | [Paper](https://arxiv.org/abs/2303.15446)   | [Code](https://github.com/Amshaker/SwiftFormer)              | 2023      |                  |              |
| Swin Transformer    | Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | [Paper](https://arxiv.org/abs/2103.14030)   | [Code](https://github.com/microsoft/Swin-Transformer)        | 2021      | microsoft        |              |
| Swin Transformer V2 | Swin Transformer V2: Scaling Up Capacity and Resolution      | [Paper](https://arxiv.org/abs/2111.09883)   | [Code](https://github.com/microsoft/Swin-Transformer)        | 2021      | microsoft        |              |
| Swin2SR             | Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration | [Paper](https://arxiv.org/abs/2209.11345)   | [Code](https://github.com/mv-lab/swin2sr)                    | 2022      |                  | ECCV 2022    |
| Table Transformer   | PubTables-1M: Towards comprehensive table extraction from unstructured documents | [Paper](https://arxiv.org/abs/2110.00061)   | [Code](https://github.com/microsoft/table-transformer)       | 2021      |                  |              |
| UPerNet             | Unified Perceptual Parsing for Scene Understanding           | [Paper](https://arxiv.org/abs/1807.10221)   | [Code](https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/uper_head.py) | 2018      |                  |              |
| VAN                 | Visual Attention Network                                     | [Paper](https://arxiv.org/abs/2202.09741)   | [Code](https://github.com/Visual-Attention-Network/VAN-Classification) | 2022      |                  |              |
| ViT                 | An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale | [Paper](https://arxiv.org/abs/2010.11929)   | [Code](https://github.com/google-research/vision_transformer) | 2020      |                  |              |
| ViT Hybrid          | An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale | [Paper](https://arxiv.org/abs/2010.11929)   | [Code](https://github.com/google-research/vision_transformer) | 2020      |                  |              |
| ViTDet              | Exploring Plain Vision Transformer Backbones for Object Detection | [Paper](https://arxiv.org/abs/2203.16527)   | [Code](https://github.com/facebookresearch/detectron2/tree/main/projects/ViTDet) | 2022      |                  |              |
| ViTMAE              | Masked Autoencoders Are Scalable Vision Learners             | [Paper](https://arxiv.org/abs/2111.06377v2) | [Code](https://github.com/facebookresearch/mae)              | 2021      |                  |              |
| ViTMatte            | Boosting Image Matting with Pretrained Plain Vision Transformers | [Paper](https://arxiv.org/abs/2305.15272)   | [Code](https://github.com/hustvl/ViTMatte)                   | 2023      |                  |              |
| ViTMSN              | Masked Siamese Networks for Label-Efficient Learning         | [Paper](https://arxiv.org/abs/2204.07141)   | [Code](https://github.com/facebookresearch/msn)              | 2022      | facebookresearch |              |
| YOLOS               | You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection | [Paper](https://arxiv.org/abs/2106.00666)   | [Code](https://github.com/hustvl/YOLOS)                      | 2021      |                  |              |
| ZoeDepth            | ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth | [Paper](https://arxiv.org/abs/2302.12288)   | [Code](https://github.com/isl-org/ZoeDepth)                  | 2023      |                  |              |
|                     |                                                              |                                             |                                                              |           |                  |              |



# Audio-Models

| Model / Methods | Title | Paper Link | Code Link | Published | Keywords | Venue |
| --------------- | ----- | ---------- | --------- | --------- | -------- | ----- |
|                 |       |            |           |           |          |       |
|                 |       |            |           |           |          |       |
|                 |       |            |           |           |          |       |
|                 |       |            |           |           |          |       |



# Video-Models

| Model / Methods | Title | Paper Link | Code Link | Published | Keywords | Venue |
| --------------- | ----- | ---------- | --------- | --------- | -------- | ----- |
|                 |       |            |           |           |          |       |
|                 |       |            |           |           |          |       |
|                 |       |            |           |           |          |       |
|                 |       |            |           |           |          |       |

## Detection

| Model / Methods | Title | Paper Link | Code Link | Published | Keywords | Venue |
| --------------- | ----- | ---------- | --------- | --------- | -------- | ----- |
|                 |       |            |           |           |          |       |
|                 |       |            |           |           |          |       |
|                 |       |            |           |           |          |       |

## Open Vocabulary Object Detection

### on MSCOCO

| Model / Methods                  | Title                                                        | Paper Link                                                   | Code Link                                                    | Published | AP(0.5⬆️) | Keywords | Venue     |
| -------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | --------- | -------- | -------- | --------- |
| Cooperative Foundational Models  | Enhancing Novel Object Detection via Cooperative Foundational Models | [![Paper](https://img.shields.io/badge/Paper-cdd7e6?style=for-the-badge)](https://arxiv.org/abs/2311.12068v2) | [![Star](https://img.shields.io/github/stars/rohit901/cooperative-foundational-models.svg?style=social&label=Star)](https://github.com/rohit901/cooperative-foundational-models) | 2023      | 50.3     |          |           |
|                                  |                                                              | [Paper](https://arxiv.org/abs/2311.12068v2)                  | [Code](https://github.com/rohit901/cooperative-foundational-models) |           |          |          |           |
| DE-ViT                           | Detect Everything with Few Examples                          | [Paper](https://arxiv.org/abs/2309.12969v3)                  | [Code](https://github.com/mlzxy/devit)                       | 2023      | 50       |          |           |
| DITO                             | Region-centric Image-Language Pretraining for Open-Vocabulary Detection | [Paper](https://arxiv.org/abs/2310.00161v2)                  | [Code](https://github.com/google-research/google-research/tree/master/fvlm/dito) | 2023      | 46.1     |          |           |
| OV-DQUO<br />(RN50x4)            | OV-DQUO: Open-Vocabulary DETR with Denoising Text Query Training and Open-World Unknown Objects Supervision | [Paper](https://arxiv.org/abs/2405.17913v1)                  | [Code](https://github.com/xiaomoguhz/ov-dquo)                | 2024      | 45.6     |          |           |
| LP-OVOD<br />(OWL-ViT Proposals) | LP-OVOD: Open-Vocabulary Object Detection by Linear Probing  | [Paper](https://arxiv.org/abs/2310.17109v2)                  | [Code](https://github.com/vinairesearch/lp-ovod)             | 2023      | 44.9     |          |           |
| CLIPSelf                         | CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction | [Paper](https://arxiv.org/abs/2310.01403v2)                  | [Code](https://github.com/wusize/clipself)                   | 2023      | 44.3     |          |           |
| CORA+                            | CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching | [Paper](https://arxiv.org/abs/2303.13076v1)                  | [Code](https://github.com/tgxs002/cora)                      | 2023      | 43.1     |          | CVPR 2023 |
| BARON                            | Aligning Bag of Regions for Open-Vocabulary Object Detection | [Paper](https://arxiv.org/abs/2302.13996v1)                  | [Code](https://github.com/wusize/ovdet)                      | 2023      | 42.7     |          | CVPR 2023 |
| CORA                             | CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching | [Paper](https://arxiv.org/abs/2303.13076v1)                  | [Code](https://github.com/tgxs002/cora)                      | 2023      | 41.7     |          | CVPR 2023 |
| RALF                             | Retrieval-Augmented Open-Vocabulary Object Detection         | [Paper](http://openaccess.thecvf.com//content/CVPR2024/papers/Kim_Retrieval-Augmented_Open-Vocabulary_Object_Detection_CVPR_2024_paper.pdf) | [Code](https://github.com/mlvlab/RALF)                       | 2024      | 41.3     |          | CVPR 2024 |
| LP-OVOD                          | LP-OVOD: Open-Vocabulary Object Detection by Linear Probing  | [Paper](https://arxiv.org/abs/2310.17109v2)                  | [Code](https://github.com/vinairesearch/lp-ovod)             | 2023      | 40.5     |          |           |
| Region-CLIP<br />(RN50x4-C4)     | RegionCLIP: Region-based Language-Image Pretraining          | [Paper](http://openaccess.thecvf.com//content/CVPR2022/html/Zhong_RegionCLIP_Region-Based_Language-Image_Pretraining_CVPR_2022_paper.html) | [Code](https://github.com/microsoft/regionclip)              | 2021      | 39.3     |          | CVPR 2022 |
| OV-DQUO<br/>(R50)                | OV-DQUO: Open-Vocabulary DETR with Denoising Text Query Training and Open-World Unknown Objects Supervision | [Paper](https://arxiv.org/abs/2405.17913v1)                  | [Code](https://github.com/xiaomoguhz/ov-dquo)                | 2024      | 39.2     |          |           |
| Object-Centric-OVD               | Bridging the Gap between Object and Image-level Representations for Open-Vocabulary Detection | [Paper](https://arxiv.org/abs/2207.03482v3)                  | [Code](https://github.com/mmaaz60/mvits_for_class_agnostic_od) | 2022      | 36.9     |          |           |
| CLIM<br/>(RN50)                  | CLIM: Contrastive Language-Image Mosaic for Region Representation | [Paper](https://arxiv.org/abs/2312.11376v2)                  | [Code](https://github.com/wusize/clim)                       | 2023      | 36.9     |          |           |
| OADP<br/>(G-OVD)]                | Object-Aware Distillation Pyramid for Open-Vocabulary Object Detection | [Paper](http://openaccess.thecvf.com//content/CVPR2023/html/Wang_Object-Aware_Distillation_Pyramid_for_Open-Vocabulary_Object_Detection_CVPR_2023_paper.html) | [Code](https://github.com/lutingwang/oadp)                   | 2023      | 35.6     |          | CVPR 2023 |
| VL-PLM<br/>(RN50)                | Exploiting Unlabeled Data with Vision and Language Models for Object Detection | [Paper](https://arxiv.org/abs/2207.08954v1)                  | [Code](https://github.com/xiaofeng94/vl-plm)                 | 2022      | 34.4     |          |           |
| CFM-ViT                          | Contrastive Feature Masking Open-Vocabulary Vision Transformer | [Paper](https://paperswithcode.com/paper/contrastive-feature-masking-open-vocabulary) | Code                                                         | 2023      | 34.1     |          | ICCV 2023 |
| MEDet<br/>(RN50)                 | Open Vocabulary Object Detection with Proposal Mining and Prediction Equalization | [Paper](https://arxiv.org/abs/2206.11134v4)                  | [Code](https://github.com/peixianchen/medet)                 | 2022      | 32.6     |          |           |
| Region-CLIP<br/>(RN50-C4)        | RegionCLIP: Region-based Language-Image Pretraining          | [Paper](http://openaccess.thecvf.com//content/CVPR2022/html/Zhong_RegionCLIP_Region-Based_Language-Image_Pretraining_CVPR_2022_paper.html) | [Code](https://github.com/microsoft/regionclip)              | 2022      | 31.4     |          | CVPR 2022 |
| OVAD-Baseline                    | Open-vocabulary Attribute Detection                          | [Paper](http://openaccess.thecvf.com//content/CVPR2023/html/Bravo_Open-Vocabulary_Attribute_Detection_CVPR_2023_paper.html) | [Code](https://github.com/OVAD-Benchmark/ovad-bechmark-code) | 2022      | 30.0     |          | CVPR 2023 |
| OADP                             | Object-Aware Distillation Pyramid for Open-Vocabulary Object Detection | [Paper](http://openaccess.thecvf.com//content/CVPR2023/html/Wang_Object-Aware_Distillation_Pyramid_for_Open-Vocabulary_Object_Detection_CVPR_2023_paper.html) | [Code](https://github.com/lutingwang/oadp)                   | 2023      | 30.0     |          | CVPR 2023 |
| OV-DERT                          | Open-Vocabulary DETR with Conditional Matching               | [Paper](https://arxiv.org/abs/2203.11876v2)                  | [Code](https://github.com/yuhangzang/ov-detr)                | 2022      | 29.4     |          |           |
| LocOv<br/>(RN50-C4)]             | Localized Vision-Language Matching for Open-vocabulary Object Detection | [Paper](https://arxiv.org/abs/2205.06160v2)                  | [Code](https://github.com/lmb-freiburg/locov)                | 2022      | 28.6     |          |           |
| Detic                            | Detecting Twenty-thousand Classes using Image-level Supervision | [Paper](https://arxiv.org/abs/2201.02605v3)                  | [Code](https://github.com/facebookresearch/Detic)            | 2022      | 27.8     |          |           |
| ViLD                             | Open-vocabulary Object Detection via Vision and Language Knowledge Distillation | [Paper](https://openreview.net/forum?id=lL3lnMbR4WU)         | [Code](https://colab.research.google.com/drive/19LBqQg0cS36rTLL_TaXZ7Ka9KJGkxiSe?usp=sharing) | 2021      | 27.6     |          | ICIR 2022 |
| OVR-CNN                          | Open-Vocabulary Object Detection Using Captions              | [Paper](http://openaccess.thecvf.com//content/CVPR2021/html/Zareian_Open-Vocabulary_Object_Detection_Using_Captions_CVPR_2021_paper.html) | [Code](https://github.com/alirezazareian/ovr-cnn)            | 2021      | 22.8     |          | CVPR 2021 |
| HierKD                           | Open-Vocabulary One-Stage Detection with Hierarchical Visual-Language Knowledge Distillation | [Paper](http://openaccess.thecvf.com//content/CVPR2022/html/Ma_Open-Vocabulary_One-Stage_Detection_With_Hierarchical_Visual-Language_Knowledge_Distillation_CVPR_2022_paper.html) | [Code](https://github.com/mengqidyangge/hierkd)              | 2022      | 20.3     |          | CVPR 2022 |
|                                  |                                                              |                                                              |                                                              |           |          |          |           |

### on LVIS v1.0

| Model / Methods | Title | Paper Link | Code Link | Published | AP(0.5⬆️) | Keywords | Venue |
| --------------- | ----- | ---------- | --------- | --------- | -------- | -------- | ----- |
|                 |       |            |           |           |          |          |       |
|                 |       |            |           |           |          |          |       |
|                 |       |            |           |           |          |          |       |
|                 |       |            |           |           |          |          |       |

### on OpenImages-v4

| Model / Methods | Title | Paper Link | Code Link | Published | AP(0.5⬆️) | Keywords | Venue |
| --------------- | ----- | ---------- | --------- | --------- | -------- | -------- | ----- |
|                 |       |            |           |           |          |          |       |
|                 |       |            |           |           |          |          |       |
|                 |       |            |           |           |          |          |       |
|                 |       |            |           |           |          |          |       |

### on Objects365

| Model / Methods | Title | Paper Link | Code Link | Published | AP(0.5⬆️) | Keywords | Venue |
| --------------- | ----- | ---------- | --------- | --------- | -------- | -------- | ----- |
|                 |       |            |           |           |          |          |       |
|                 |       |            |           |           |          |          |       |
|                 |       |            |           |           |          |          |       |
|                 |       |            |           |           |          |          |       |











## Open-Vocabulary Segmentation

| Model / Methods | Title | Paper Link | Code Link | Published | Keywords | Venue |
| --------------- | ----- | ---------- | --------- | --------- | -------- | ----- |
|                 |       |            |           |           |          |       |
|                 |       |            |           |           |          |       |
|                 |       |            |           |           |          |       |
|                 |       |            |           |           |          |       |

# VLM-Pre-training-Methods

---

| Model / Methods | Title                                                        | Paper Link                                                 | Code Link                                                    | Published | Keywords | Venue     |
| --------------- | ------------------------------------------------------------ | ---------------------------------------------------------- | ------------------------------------------------------------ | --------- | -------- | --------- |
|                 | Efficient Vision-Language Pre-training by Cluster Masking    | [Paper](https://arxiv.org/pdf/2405.08815)                  | [Code](https://github.com/Zi-hao-Wei/Efficient-Vision-Language-Pre-training-by-Cluster-Masking) | 2024      |          | CVPR 2024 |
|                 | Towards Better Vision-Inspired Vision-Language Models        | [Paper](https://www.lamda.nju.edu.cn/caoyh/files/VIVL.pdf) | Code                                                         | 2024      |          | CVPR 2024 |
|                 | Non-autoregressive Sequence-to-Sequence Vision-Language Models | [Paper](https://arxiv.org/abs/2403.02249v1)                | Code                                                         | 2024      |          | CVPR 2024 |
| ViTamin         | ViTamin: Designing Scalable Vision Models in the Vision-Language Era | [Paper](https://arxiv.org/abs/2404.02132v1)                | [Code](https://github.com/Beckschen/ViTamin)                 | 2024      |          | CVPR 2024 |
|                 | Iterated Learning Improves Compositionality in Large Vision-Language Models | [Paper](https://arxiv.org/abs/2404.02145v1)                | Code                                                         | 2024      |          | CVPR 2024 |
| FairCLIP        | FairCLIP: Harnessing Fairness in Vision-Language Learning    | [Paper](https://arxiv.org/abs/2403.19949v1)                | [Code](https://ophai.hms.harvard.edu/datasets/fairvlmed10k)  | 2024      |          | CVPR 2024 |
| InternVL        | InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks | [Paper](https://arxiv.org/abs/2312.14238)                  | [Code](https://github.com/OpenGVLab/InternVL)                | 2024      |          | CVPR 2024 |
| VILA            | VILA: On Pre-training for Visual Language Models             | [Paper](https://arxiv.org/abs/2312.07533)                  | Code                                                         | 2024      |          | CVPR 2024 |
|                 | Generative Region-Language Pretraining for Open-Ended Object Detection | [Paper](https://arxiv.org/pdf/2403.10191v1.pdf)            | [Code](https://github.com/FoundationVision/GenerateU)        | 2024      |          | CVPR 2024 |
|                 | Enhancing Vision-Language Pre-training with Rich Supervisions | [Paper](https://arxiv.org/pdf/2403.03346v1.pdf)            | Code                                                         | 2024      |          | CVPR 2024 |
|                 | Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization | [Paper](https://arxiv.org/abs/2309.04669)                  | [Code](https://github.com/jy0205/LaVIT)                      | 2024      |          | ICLR 2024 |
| MMICL           | MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning | [Paper](https://arxiv.org/abs/2309.07915)                  | [Code](https://github.com/PKUnlp-icler/MIC)                  | 2024      |          | ICLR 2024 |
|                 | Retrieval-Enhanced Contrastive Vision-Text Models            | [Paper](https://arxiv.org/abs/2306.07196)                  | Code                                                         | 2024      |          | ICLR 2024 |
|                 |                                                              |                                                            |                                                              |           |          |           |



---

# VLM-Transfer-Learning-Methods

| -                           | Title                                                        | Paper Link                                        | Code Link                                                    | Published | Keywords | Venue      |
| --------------------------- | ------------------------------------------------------------ | ------------------------------------------------- | ------------------------------------------------------------ | --------- | -------- | ---------- |
| CLAP                        | CLAP: Isolating Content from Style through Contrastive Learning with Augmented Prompts | [Paper](https://arxiv.org/abs/2311.16445)         | [Code](https://github.com/YichaoCai1/CLAP)                   | 2024      |          | ECCV 2024  |
| FALIP                       | FALIP: Visual Prompt as Foveal Attention Boosts CLIP Zero-Shot Performance | [Paper](https://arxiv.org/abs/2407.05578v1)       | [Code](https://pumpkin805.github.io/FALIP/)                  | 2024      |          | ECCV 2024  |
| GalLoP                      | GalLoP: Learning Global and Local Prompts for Vision-Language Models | [Paper](https://arxiv.org/pdf/2407.01400)         | Code                                                         | 2024      |          | ECCV 2024  |
| Mind the Interference       | Mind the Interference: Retaining Pre-trained Knowledge in Parameter Efficient Continual Learning of Vision-Language Models | [Paper](https://arxiv.org/pdf/2407.05342v1)       | [Code](https://github.com/lloongx/DIKI)                      | 2024      |          | ECCV 2024  |
|                             | One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models | [Paper](https://arxiv.org/abs/2403.01849v1)       | [Code](https://github.com/TreeLLi/APT)                       | 2024      |          | CVPR 2024  |
|                             | Any-Shift Prompting for Generalization over Distributions    | [Paper](https://arxiv.org/abs/2402.10099)         | Code                                                         | 2024      |          | CVPR 2024  |
| CLAP                        | A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models | [Paper](https://arxiv.org/abs/2312.12730)         | [Code](https://github.com/jusiro/CLAP)                       | 2024      |          | CVPR 2024  |
|                             | Anchor-based Robust Finetuning of Vision-Language Models     | [Paper](https://arxiv.org/abs/2404.06244)         | Code                                                         | 2024      |          | CVPR 2024  |
|                             | Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners | Paper                                             | Code                                                         | 2024      |          | CVPR 2024  |
|                             | Visual In-Context Prompting                                  | [Paper](https://arxiv.org/abs/2311.13601)         | [Code](https://github.com/UX-Decoder/DINOv)                  | 2024      |          | CVPR 2024  |
| TCP                         | TCP:Textual-based Class-aware Prompt tuning for Visual-Language Model | [Paper](https://arxiv.org/abs/2311.18231)         | [Code](https://github.com/htyao89/Textual-based_Class-aware_prompt_tuning/) | 2024      |          | CVPR 2024  |
|                             | Efficient Test-Time Adaptation of Vision-Language Models     | [Paper](https://arxiv.org/abs/2403.18293v1)       | [Code](https://kdiaaa.github.io/tda/)                        | 2024      |          | CVPR 2024  |
| Dual Memory Networks        | Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models | [Paper](https://arxiv.org/abs/2403.17589v1)       | [Code](https://github.com/YBZh/DMN)                          | 2024      |          | CVPR 2024  |
| DePT                        | DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning | [Paper](https://arxiv.org/abs/2309.05173)         | [Code](https://github.com/ZhengxiangShi/DePT)                | 2024      |          | ICLR 2024  |
| Nemesis                     | Nemesis: Normalizing the soft-prompt vectors of vision-language models | [Paper](https://openreview.net/pdf?id=zmJDzPh1Dm) | Code                                                         | 2024      |          | ICLR 2024  |
|                             | Prompt Gradient Projection for Continual Learning            | [Paper](https://openreview.net/pdf?id=EH2O3h7sBI) | Code                                                         | 2024      |          | ICLR 2024  |
| An Image Is Worth 1000 Lies | An Image Is Worth 1000 Lies: Transferability of Adversarial Images across Prompts on Vision-Language Models | [Paper](https://openreview.net/pdf?id=nc5GgFAvtk) | Code                                                         | 2024      |          | ICLR 2024  |
| Matcher                     | Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching | [Paper](https://arxiv.org/abs/2305.13310)         | Code                                                         | 2024      |          | ICLR 2024  |
|                             | Text-driven Prompt Generation for Vision-Language Models in Federated Learning | [Paper](https://arxiv.org/abs/2310.06123)         | Code                                                         | 2024      |          | ICLR 2024  |
|                             | Consistency-guided Prompt Learning for Vision-Language Models | [Paper](https://arxiv.org/abs/2306.01195)         | Code                                                         | 2024      |          | ICLR 2024  |
| C-TPT                       | C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion | [Paper](https://openreview.net/pdf?id=jzzEHTBFOT) | Code                                                         | 2024      |          | ICLR 2024  |
|                             | Learning to Prompt Segment Anything Models                   | [Paper](https://arxiv.org/pdf/2401.04651.pdf)     | Code                                                         | 2024      |          | arXiv 2024 |
|                             |                                                              |                                                   |                                                              |           |          |            |



# VLM-Knowledge-Distillation-for-Detection

|             | Title                                                        | Paper Link                                        | Code Link                                                 | Published | Keywords | Venue     |
| ----------- | ------------------------------------------------------------ | ------------------------------------------------- | --------------------------------------------------------- | --------- | -------- | --------- |
| RegionGPT   | RegionGPT: Towards Region Understanding Vision Language Model | [Paper](https://arxiv.org/pdf/2403.02330v1.pdf)   | [Code](https://guoqiushan.github.io/regiongpt.github.io/) |           |          | CVPR 2024 |
|             | LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors | [Paper](https://arxiv.org/pdf/2402.04630.pdf)     | Code                                                      |           |          | ICLR 2024 |
| Ins-DetCLIP | Ins-DetCLIP: Aligning Detection Model to Follow Human-Language Instruction | [Paper](https://openreview.net/pdf?id=M0MF4t3hE9) | Code                                                      |           |          | ICLR 2024 |

 

# VLM-Knowledge-Distillation-for-Segmentation

|          | Title                                                        | Paper Link                                | Code Link | Published | Keywords | Venue     |
| -------- | ------------------------------------------------------------ | ----------------------------------------- | --------- | --------- | -------- | --------- |
| CLIPSelf | CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction | [Paper](https://arxiv.org/abs/2310.01403) | Code      | 2024      |          | ICLR 2024 |
|          |                                                              |                                           |           |           |          |           |



# VLM-Knowledge-Distillation-for-Other-Vision-Tasks

|             | Title                                                        | Paper Link                                    | Code Link                                     | Published | Keywords | Venue     |
| ----------- | ------------------------------------------------------------ | --------------------------------------------- | --------------------------------------------- | --------- | -------- | --------- |
| FROSTER     | FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition | [Paper](https://arxiv.org/pdf/2402.03241.pdf) | Code                                          | 2024      |          | ICLR 2024 |
| AnomalyCLIP | AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection | [Paper](https://arxiv.org/pdf/2310.18961.pdf) | [Code](https://github.com/zqhang/AnomalyCLIP) | 2024      |          | ICLR 2024 |
|             |                                                              |                                               |                                               |           |          |           |



---

# Prompt-Learning

| -          | Title                                                        | Paper Link                                                   | Code Link                                                    | Published | Keywords                                   | Venue        |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | --------- | ------------------------------------------ | ------------ |
| CoOp       | Learning to Prompt for Vision-Language Models                | [Paper](https://arxiv.org/abs/2203.05557)                    | [Code](https://github.com/KaiyangZhou/CoOp)                  | 2022      | Use text-based learnable prompts/adapters  | IJCV 2022    |
| CoCoOp     | Conditional Prompt Learning for Vision-Language Models       | [Paper](https://arxiv.org/abs/2203.05557)                    | [Code](https://github.com/KaiyangZhou/CoOp)                  | 2022      | Use text-based learnable prompts/adapters  | CVPR 2022    |
| ProDA      | Prompt Distribution Learning                                 | [Paper](https://arxiv.org/abs/2205.03340)                    | [Code](https://github.com/bbbdylan/proda)                    | 2022      | Use text-based learnable prompts/adapters  | CVPR 2022    |
| VPT        | Visual Prompt Tuning                                         | [Paper](https://arxiv.org/abs/2203.12119)                    | [Code](https://github.com/kmnp/vpt)                          | 2022      | Use image-based learnable prompts/adapters | ECCV 2022    |
| MaPLe      | MaPLe: Multi-modal Prompt Learning                           | [Paper](https://arxiv.org/abs/2210.03117)                    | [Code](https://github.com/muzairkhattak/multimodal-prompt-learning) | 2023      |                                            | CVPR 2023    |
| KgCoOp     | Visual-Language Prompt Tuningx with Knowledge-guided Context Optimization | [Paper](https://arxiv.org/abs/2303.13283)                    | [Code](https://github.com/htyao89/KgCoOp)                    | 2023      |                                            | CVPR 2023    |
| LASP       | LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models | [Paper](https://arxiv.org/abs/2210.01115)                    | -                                                            | 2023      |                                            | CVPR 2023    |
| DAM-VP     | Diversity-Aware Meta Visual Prompting                        | [Paper](https://arxiv.org/abs/2303.08138)                    | [Code](https://github.com/shikiw/DAM-VP)                     | 2023      |                                            | CVPR 2023    |
| TaskRes    | Task Residual for Tuning Vision-Language Models              | [Paper](https://arxiv.org/abs/2211.10277)                    | [Code](https://github.com/geekyutao/TaskRes)                 | 2023      |                                            | CVPR 2023    |
| RPO        | Read-only Prompt Optimization for Vision-Language Few-shot Learning | [Paper](https://arxiv.org/abs/2308.14960)                    | [Code](https://github.com/mlvlab/rpo)                        | 2023      |                                            | ICCV 2023    |
| KAPT       | Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models | [Paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Kan_Knowledge-Aware_Prompt_Tuning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.pdf) | -                                                            | 2023      |                                            | ICCV 2023    |
| ProGrad    | Prompt-aligned Gradient for Prompt Tuning                    | [Paper](https://arxiv.org/abs/2205.14865)                    | [Code](https://github.com/BeierZhu/Prompt-align)             | 2023      |                                            | ICCV 2023    |
| PromptSRC  | Self-regulating Prompts: Foundational Model Adaptation without Forgetting | [Paper](https://openaccess.thecvf.com//content/ICCV2023/papers/Khattak_Self-regulating_Prompts_Foundational_Model_Adaptation_without_Forgetting_ICCV_2023_paper.pdf) | [Code](https://github.com/muzairkhattak/PromptSRC)           | 2023      |                                            | ICCV 2023    |
| DeFo       | Learning to Decompose Visual Features with Latent Textual Prompts | [Paper](https://arxiv.org/abs/2210.04287)                    | -                                                            | 2023      |                                            | ICLR 2023    |
| POMP       | Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition | [Paper](https://arxiv.org/abs/2304.04704)                    | [Code](https://github.com/amazon-science/prompt-pretraining) | 2023      |                                            | NeurIPS 2023 |
| MetaPrompt | Learning Domain Invariant Prompt for Vision-Language Models  | [Paper](https://arxiv.org/abs/2212.04196)                    | -                                                            | 2024      |                                            | TIP 2024     |
| SA2VP      | SA2VP: Spatially Aligned-and-Adapted Visual Prompt           | [Paper](https://arxiv.org/abs/2312.10376)                    | [Code](https://github.com/tommy-xq/SA2VP)                    | 2024      |                                            |              |
| HPT        | Learning Hierarchical Prompt with Structured Linguistic Knowledge for Vision-Language Models | [Paper](https://arxiv.org/abs/2312.06323)                    | [Code](https://github.com/Vill-Lab/2024-AAAI-HPT)            | 2024      |                                            |              |
| LaViP      | LaViP: Language-Grounded Visual Prompts                      | [Paper](https://arxiv.org/abs/2312.10945)                    | -                                                            | 2024      |                                            |              |
| CoPrompt   | Consistency-guided Prompt Learning for Vision-Language Models | [Paper](https://arxiv.org/abs/2306.01195)                    | [Code](https://github.com/ShuvenduRoy/CoPrompt)              | 2024      |                                            |              |
| ProText    | Learning to Prompt with Text Only Supervision for Vision-Language Models | [Paper](https://arxiv.org/abs/2401.02418)                    | [Code](https://github.com/muzairkhattak/ProText)             | 2024      |                                            |              |
| PromptKD   | Unsupervised Prompt Distillation for Vision Language Models  | [Paper](https://arxiv.org/abs/2403.02781)                    | [Code](https://github.com/zhengli97/PromptKD)                | 2024      |                                            |              |
| DePT       | DePT: Decoupled Prompt Tuning                                | [Paper](https://arxiv.org/abs/2309.07439)                    | [Code](https://github.com/Koorye/DePT)                       | 2024      |                                            |              |
| ArGue      | ArGue: Attribute-Guided Prompt Tuning for Vision-Language Models | [Paper](https://arxiv.org/abs/2311.16494)                    | -                                                            | 2024      |                                            |              |
| TCP        | TCP:Textual-based Class-aware Prompt tuning for Visual-Language Model | [Paper](https://arxiv.org/abs/2311.18231)                    | [Code](https://github.com/htyao89/Textual-based_Class-aware_prompt_tuning) | 2024      |                                            |              |
| MMA        | MMA: Multi-Modal Adapter for Vision-Language Models          | [Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_MMA_Multi-Modal_Adapter_for_Vision-Language_Models_CVPR_2024_paper.pdf) | [Code](https://github.com/ZjjConan/Multi-Modal-Adapter)      | 2024      |                                            |              |
