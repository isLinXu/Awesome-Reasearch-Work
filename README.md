# Awesome-Reasearch-Work

---
Awesome-Reasearch-Work

## VLM Pre-training Methods

---

|          | Title                                                        | Paper Link                                                 | Code Link                                                    | Published | Keywords | Venue     |
| -------- | ------------------------------------------------------------ | ---------------------------------------------------------- | ------------------------------------------------------------ | --------- | -------- | --------- |
|          | Efficient Vision-Language Pre-training by Cluster Masking    | [Paper](https://arxiv.org/pdf/2405.08815)                  | [Code](https://github.com/Zi-hao-Wei/Efficient-Vision-Language-Pre-training-by-Cluster-Masking) | 2024      |          | CVPR 2024 |
|          | Towards Better Vision-Inspired Vision-Language Models        | [Paper](https://www.lamda.nju.edu.cn/caoyh/files/VIVL.pdf) | Code                                                         | 2024      |          | CVPR 2024 |
|          | Non-autoregressive Sequence-to-Sequence Vision-Language Models | [Paper](https://arxiv.org/abs/2403.02249v1)                | Code                                                         | 2024      |          | CVPR 2024 |
| ViTamin  | ViTamin: Designing Scalable Vision Models in the Vision-Language Era | [Paper](https://arxiv.org/abs/2404.02132v1)                | [Code](https://github.com/Beckschen/ViTamin)                 | 2024      |          | CVPR 2024 |
|          | Iterated Learning Improves Compositionality in Large Vision-Language Models | [Paper](https://arxiv.org/abs/2404.02145v1)                | Code                                                         | 2024      |          | CVPR 2024 |
| FairCLIP | FairCLIP: Harnessing Fairness in Vision-Language Learning    | [Paper](https://arxiv.org/abs/2403.19949v1)                | [Code](https://ophai.hms.harvard.edu/datasets/fairvlmed10k)  | 2024      |          | CVPR 2024 |
| InternVL | InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks | [Paper](https://arxiv.org/abs/2312.14238)                  | [Code](https://github.com/OpenGVLab/InternVL)                | 2024      |          | CVPR 2024 |
| VILA     | VILA: On Pre-training for Visual Language Models             | [Paper](https://arxiv.org/abs/2312.07533)                  | Code                                                         | 2024      |          | CVPR 2024 |
|          | Generative Region-Language Pretraining for Open-Ended Object Detection | [Paper](https://arxiv.org/pdf/2403.10191v1.pdf)            | [Code](https://github.com/FoundationVision/GenerateU)        | 2024      |          | CVPR 2024 |
|          | Enhancing Vision-Language Pre-training with Rich Supervisions | [Paper](https://arxiv.org/pdf/2403.03346v1.pdf)            | Code                                                         | 2024      |          | CVPR 2024 |
|          | Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization | [Paper](https://arxiv.org/abs/2309.04669)                  | [Code](https://github.com/jy0205/LaVIT)                      | 2024      |          | ICLR 2024 |
| MMICL    | MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning | [Paper](https://arxiv.org/abs/2309.07915)                  | [Code](https://github.com/PKUnlp-icler/MIC)                  | 2024      |          | ICLR 2024 |
|          | Retrieval-Enhanced Contrastive Vision-Text Models            | [Paper](https://arxiv.org/abs/2306.07196)                  | Code                                                         | 2024      |          | ICLR 2024 |
|          |                                                              |                                                            |                                                              |           |          |           |

---

## VLM Transfer Learning Methods

| -                           | Title                                                        | Paper Link                                        | Code Link                                                    | Published | Keywords | Venue      |
| --------------------------- | ------------------------------------------------------------ | ------------------------------------------------- | ------------------------------------------------------------ | --------- | -------- | ---------- |
| CLAP                        | CLAP: Isolating Content from Style through Contrastive Learning with Augmented Prompts | [Paper](https://arxiv.org/abs/2311.16445)         | [Code](https://github.com/YichaoCai1/CLAP)                   | 2024      |          | ECCV 2024  |
| FALIP                       | FALIP: Visual Prompt as Foveal Attention Boosts CLIP Zero-Shot Performance | [Paper](https://arxiv.org/abs/2407.05578v1)       | [Code](https://pumpkin805.github.io/FALIP/)                  | 2024      |          | ECCV 2024  |
| GalLoP                      | GalLoP: Learning Global and Local Prompts for Vision-Language Models | [Paper](https://arxiv.org/pdf/2407.01400)         | Code                                                         | 2024      |          | ECCV 2024  |
| Mind the Interference       | Mind the Interference: Retaining Pre-trained Knowledge in Parameter Efficient Continual Learning of Vision-Language Models | [Paper](https://arxiv.org/pdf/2407.05342v1)       | [Code](https://github.com/lloongx/DIKI)                      | 2024      |          | ECCV 2024  |
|                             | One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models | [Paper](https://arxiv.org/abs/2403.01849v1)       | [Code](https://github.com/TreeLLi/APT)                       | 2024      |          | CVPR 2024  |
|                             | Any-Shift Prompting for Generalization over Distributions    | [Paper](https://arxiv.org/abs/2402.10099)         | Code                                                         | 2024      |          | CVPR 2024  |
| CLAP                        | A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models | [Paper](https://arxiv.org/abs/2312.12730)         | [Code](https://github.com/jusiro/CLAP)                       | 2024      |          | CVPR 2024  |
|                             | Anchor-based Robust Finetuning of Vision-Language Models     | [Paper](https://arxiv.org/abs/2404.06244)         | Code                                                         | 2024      |          | CVPR 2024  |
|                             | Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners | Paper                                             | Code                                                         | 2024      |          | CVPR 2024  |
|                             | Visual In-Context Prompting                                  | [Paper](https://arxiv.org/abs/2311.13601)         | [Code](https://github.com/UX-Decoder/DINOv)                  | 2024      |          | CVPR 2024  |
| TCP                         | TCP:Textual-based Class-aware Prompt tuning for Visual-Language Model | [Paper](https://arxiv.org/abs/2311.18231)         | [Code](https://github.com/htyao89/Textual-based_Class-aware_prompt_tuning/) | 2024      |          | CVPR 2024  |
|                             | Efficient Test-Time Adaptation of Vision-Language Models     | [Paper](https://arxiv.org/abs/2403.18293v1)       | [Code](https://kdiaaa.github.io/tda/)                        | 2024      |          | CVPR 2024  |
| Dual Memory Networks        | Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models | [Paper](https://arxiv.org/abs/2403.17589v1)       | [Code](https://github.com/YBZh/DMN)                          | 2024      |          | CVPR 2024  |
| DePT                        | DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning | [Paper](https://arxiv.org/abs/2309.05173)         | [Code](https://github.com/ZhengxiangShi/DePT)                | 2024      |          | ICLR 2024  |
| Nemesis                     | Nemesis: Normalizing the soft-prompt vectors of vision-language models | [Paper](https://openreview.net/pdf?id=zmJDzPh1Dm) | Code                                                         | 2024      |          | ICLR 2024  |
|                             | Prompt Gradient Projection for Continual Learning            | [Paper](https://openreview.net/pdf?id=EH2O3h7sBI) | Code                                                         | 2024      |          | ICLR 2024  |
| An Image Is Worth 1000 Lies | An Image Is Worth 1000 Lies: Transferability of Adversarial Images across Prompts on Vision-Language Models | [Paper](https://openreview.net/pdf?id=nc5GgFAvtk) | Code                                                         | 2024      |          | ICLR 2024  |
| Matcher                     | Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching | [Paper](https://arxiv.org/abs/2305.13310)         | Code                                                         | 2024      |          | ICLR 2024  |
|                             | Text-driven Prompt Generation for Vision-Language Models in Federated Learning | [Paper](https://arxiv.org/abs/2310.06123)         | Code                                                         | 2024      |          | ICLR 2024  |
|                             | Consistency-guided Prompt Learning for Vision-Language Models | [Paper](https://arxiv.org/abs/2306.01195)         | Code                                                         | 2024      |          | ICLR 2024  |
| C-TPT                       | C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion | [Paper](https://openreview.net/pdf?id=jzzEHTBFOT) | Code                                                         | 2024      |          | ICLR 2024  |
|                             | Learning to Prompt Segment Anything Models                   | [Paper](https://arxiv.org/pdf/2401.04651.pdf)     | Code                                                         | 2024      |          | arXiv 2024 |
|                             |                                                              |                                                   |                                                              |           |          |            |



## VLM Knowledge Distillation for Detection

|             | Title                                                        | Paper Link                                        | Code Link                                                 | Published | Keywords | Venue     |
| ----------- | ------------------------------------------------------------ | ------------------------------------------------- | --------------------------------------------------------- | --------- | -------- | --------- |
| RegionGPT   | RegionGPT: Towards Region Understanding Vision Language Model | [Paper](https://arxiv.org/pdf/2403.02330v1.pdf)   | [Code](https://guoqiushan.github.io/regiongpt.github.io/) |           |          | CVPR 2024 |
|             | LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors | [Paper](https://arxiv.org/pdf/2402.04630.pdf)     | Code                                                      |           |          | ICLR 2024 |
| Ins-DetCLIP | Ins-DetCLIP: Aligning Detection Model to Follow Human-Language Instruction | [Paper](https://openreview.net/pdf?id=M0MF4t3hE9) | Code                                                      |           |          | ICLR 2024 |

 

## VLM Knowledge Distillation for Segmentation

|          | Title                                                        | Paper Link                                | Code Link | Published | Keywords | Venue     |
| -------- | ------------------------------------------------------------ | ----------------------------------------- | --------- | --------- | -------- | --------- |
| CLIPSelf | CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction | [Paper](https://arxiv.org/abs/2310.01403) | Code      | 2024      |          | ICLR 2024 |
|          |                                                              |                                           |           |           |          |           |




## VLM Knowledge Distillation for Other Vision Tasks

|             | Title                                                        | Paper Link                                    | Code Link                                     | Published | Keywords | Venue     |
| ----------- | ------------------------------------------------------------ | --------------------------------------------- | --------------------------------------------- | --------- | -------- | --------- |
| FROSTER     | FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition | [Paper](https://arxiv.org/pdf/2402.03241.pdf) | Code                                          | 2024      |          | ICLR 2024 |
| AnomalyCLIP | AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection | [Paper](https://arxiv.org/pdf/2310.18961.pdf) | [Code](https://github.com/zqhang/AnomalyCLIP) | 2024      |          | ICLR 2024 |
|             |                                                              |                                               |                                               |           |          |           |



---

| -          | Title                                                        | Paper Link                                                   | Code Link                                                    | Published | Keywords                                   | Venue        |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | --------- | ------------------------------------------ | ------------ |
| CoOp       | Learning to Prompt for Vision-Language Models                | [Paper](https://arxiv.org/abs/2203.05557)                    | [Code](https://github.com/KaiyangZhou/CoOp)                  | 2022      | Use text-based learnable prompts/adapters  | IJCV 2022    |
| CoCoOp     | Conditional Prompt Learning for Vision-Language Models       | [Paper](https://arxiv.org/abs/2203.05557)                    | [Code](https://github.com/KaiyangZhou/CoOp)                  | 2022      | Use text-based learnable prompts/adapters  | CVPR 2022    |
| ProDA      | Prompt Distribution Learning                                 | [Paper](https://arxiv.org/abs/2205.03340)                    | [Code](https://github.com/bbbdylan/proda)                    | 2022      | Use text-based learnable prompts/adapters  | CVPR 2022    |
| VPT        | Visual Prompt Tuning                                         | [Paper](https://arxiv.org/abs/2203.12119)                    | [Code](https://github.com/kmnp/vpt)                          | 2022      | Use image-based learnable prompts/adapters | ECCV 2022    |
| MaPLe      | MaPLe: Multi-modal Prompt Learning                           | [Paper](https://arxiv.org/abs/2210.03117)                    | [Code](https://github.com/muzairkhattak/multimodal-prompt-learning) | 2023      |                                            | CVPR 2023    |
| KgCoOp     | Visual-Language Prompt Tuningx with Knowledge-guided Context Optimization | [Paper](https://arxiv.org/abs/2303.13283)                    | [Code](https://github.com/htyao89/KgCoOp)                    | 2023      |                                            | CVPR 2023    |
| LASP       | LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models | [Paper](https://arxiv.org/abs/2210.01115)                    | -                                                            | 2023      |                                            | CVPR 2023    |
| DAM-VP     | Diversity-Aware Meta Visual Prompting                        | [Paper](https://arxiv.org/abs/2303.08138)                    | [Code](https://github.com/shikiw/DAM-VP)                     | 2023      |                                            | CVPR 2023    |
| TaskRes    | Task Residual for Tuning Vision-Language Models              | [Paper](https://arxiv.org/abs/2211.10277)                    | [Code](https://github.com/geekyutao/TaskRes)                 | 2023      |                                            | CVPR 2023    |
| RPO        | Read-only Prompt Optimization for Vision-Language Few-shot Learning | [Paper](https://arxiv.org/abs/2308.14960)                    | [Code](https://github.com/mlvlab/rpo)                        | 2023      |                                            | ICCV 2023    |
| KAPT       | Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models | [Paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Kan_Knowledge-Aware_Prompt_Tuning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.pdf) | -                                                            | 2023      |                                            | ICCV 2023    |
| ProGrad    | Prompt-aligned Gradient for Prompt Tuning                    | [Paper](https://arxiv.org/abs/2205.14865)                    | [Code](https://github.com/BeierZhu/Prompt-align)             | 2023      |                                            | ICCV 2023    |
| PromptSRC  | Self-regulating Prompts: Foundational Model Adaptation without Forgetting | [Paper](https://openaccess.thecvf.com//content/ICCV2023/papers/Khattak_Self-regulating_Prompts_Foundational_Model_Adaptation_without_Forgetting_ICCV_2023_paper.pdf) | [Code](https://github.com/muzairkhattak/PromptSRC)           | 2023      |                                            | ICCV 2023    |
| DeFo       | Learning to Decompose Visual Features with Latent Textual Prompts | [Paper](https://arxiv.org/abs/2210.04287)                    | -                                                            | 2023      |                                            | ICLR 2023    |
| POMP       | Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition | [Paper](https://arxiv.org/abs/2304.04704)                    | [Code](https://github.com/amazon-science/prompt-pretraining) | 2023      |                                            | NeurIPS 2023 |
| MetaPrompt | Learning Domain Invariant Prompt for Vision-Language Models  | [Paper](https://arxiv.org/abs/2212.04196)                    | -                                                            | 2024      |                                            | TIP 2024     |
| SA2VP      | SA2VP: Spatially Aligned-and-Adapted Visual Prompt           | [Paper](https://arxiv.org/abs/2312.10376)                    | [Code](https://github.com/tommy-xq/SA2VP)                    | 2024      |                                            |              |
| HPT        | Learning Hierarchical Prompt with Structured Linguistic Knowledge for Vision-Language Models | [Paper](https://arxiv.org/abs/2312.06323)                    | [Code](https://github.com/Vill-Lab/2024-AAAI-HPT)            | 2024      |                                            |              |
| LaViP      | LaViP: Language-Grounded Visual Prompts                      | [Paper](https://arxiv.org/abs/2312.10945)                    | -                                                            | 2024      |                                            |              |
| CoPrompt   | Consistency-guided Prompt Learning for Vision-Language Models | [Paper](https://arxiv.org/abs/2306.01195)                    | [Code](https://github.com/ShuvenduRoy/CoPrompt)              | 2024      |                                            |              |
| ProText    | Learning to Prompt with Text Only Supervision for Vision-Language Models | [Paper](https://arxiv.org/abs/2401.02418)                    | [Code](https://github.com/muzairkhattak/ProText)             | 2024      |                                            |              |
| PromptKD   | Unsupervised Prompt Distillation for Vision Language Models  | [Paper](https://arxiv.org/abs/2403.02781)                    | [Code](https://github.com/zhengli97/PromptKD)                | 2024      |                                            |              |
| DePT       | DePT: Decoupled Prompt Tuning                                | [Paper](https://arxiv.org/abs/2309.07439)                    | [Code](https://github.com/Koorye/DePT)                       | 2024      |                                            |              |
| ArGue      | ArGue: Attribute-Guided Prompt Tuning for Vision-Language Models | [Paper](https://arxiv.org/abs/2311.16494)                    | -                                                            | 2024      |                                            |              |
| TCP        | TCP:Textual-based Class-aware Prompt tuning for Visual-Language Model | [Paper](https://arxiv.org/abs/2311.18231)                    | [Code](https://github.com/htyao89/Textual-based_Class-aware_prompt_tuning) | 2024      |                                            |              |
| MMA        | MMA: Multi-Modal Adapter for Vision-Language Models          | [Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_MMA_Multi-Modal_Adapter_for_Vision-Language_Models_CVPR_2024_paper.pdf) | [Code](https://github.com/ZjjConan/Multi-Modal-Adapter)      | 2024      |                                            |              |
