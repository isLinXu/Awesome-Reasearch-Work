# Awesome-Reasearch-Work

---
Awesome-Reasearch-Work

## Transformers-MultiModal Models(Sort A-Z)

|                   | Title                                                        | Paper Link | Code Link | Published | Keywords | Venue                                            |
| ----------------- | ------------------------------------------------------------ | ---------- | --------- | --------- | -------- | ------------------------------------------------ |
| ALIGN             | Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision | Paper      | Code      | 2021      |          |                                                  |
| AltCLIP           | AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities | Paper      | Code      |           |          |                                                  |
| BLIP              | BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation | Paper      | Code      |           |          |                                                  |
| BLIP-2            | BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models | Paper      | Code      |           |          |                                                  |
| BridgeTower       | BridgeTower: Building Bridges Between Encoders in Vision-Language Representative Learning | Paper      | Code      |           |          | [AAAI’23](https://aaai.org/Conferences/AAAI-23/) |
| BROS              | BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents | Paper      | Code      |           |          |                                                  |
| Chameleon         | Chameleon: Mixed-Modal Early-Fusion Foundation Models        | Paper      | Code      |           |          |                                                  |
| Chinese-CLIP      | Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese | Paper      | Code      |           |          |                                                  |
| CLIP              | Learning Transferable Visual Models From Natural Language Supervision | Paper      | Code      |           |          |                                                  |
| CLIPSeg           | Image Segmentation Using Text and Image Prompts              | Paper      | Code      |           |          |                                                  |
| CLVP              | Better speech synthesis through scaling                      | Paper      | Code      |           |          |                                                  |
| Data2Vec          | data 2 vec：A General Framework for Self-supervised Learning in Speech，Vision and Language | Paper      | Code      |           |          |                                                  |
| DePlot            | DePlot: One-shot visual language reasoning by plot-to-table translation | Paper      | Code      |           |          |                                                  |
| Donut             | OCR-free Document Understanding Transformer                  | Paper      | Code      |           |          |                                                  |
| FLAVA             | FLAVA: A Foundational Language And Vision Alignment Model    | Paper      | Code      |           |          |                                                  |
| GIT               | GIT: A Generative Image-to-text Transformer for Vision and Language | Paper      | Code      |           |          |                                                  |
| Grounding DINO    | Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection | Paper      | Code      |           |          |                                                  |
| GroupViT          | GroupViT: Semantic Segmentation Emerges from Text Supervision | Paper      | Code      |           |          |                                                  |
| IDEFICS           | OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents | Paper      | Code      |           |          |                                                  |
| IDEFICS-2         | What matters when building vision-language models?           | Paper      | Code      |           |          |                                                  |
| InstructBLIP      | InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning | Paper      | Code      |           |          |                                                  |
| InstructBlipVideo | InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning | Paper      | Code      |           |          |                                                  |
| KOSMOS-2          | Kosmos-2: Grounding Multimodal Large Language Models to the World | Paper      | Code      |           |          |                                                  |
| LayoutLM          | LayoutLM: Pre-training of Text and Layout for Document Image Understanding | Paper      | Code      |           |          |                                                  |
| LayoutLMV2        | LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding | Paper      | Code      |           |          |                                                  |
| LayoutLMv3        | LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking | Paper      | Code      |           |          |                                                  |
| LayoutXLM         | LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding | Paper      | Code      |           |          |                                                  |
| LiLT              | LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding | Paper      | Code      |           |          |                                                  |
| LLaVa             | Improved Baselines with Visual Instruction Tuning            | Paper      | Code      |           |          |                                                  |
| LLaVA-NeXT        | LLaVA-NeXT: Improved reasoning, OCR, and world knowledge     | Paper      | Code      |           |          |                                                  |
| LLaVa-NeXT-Video  | LLaVA-NeXT: A Strong Zero-shot Video Understanding Model     | Paper      | Code      |           |          |                                                  |
| LXMERT            | LXMERT: Learning Cross-Modality Encoder Representations from Transformers | Paper      | Code      |           |          |                                                  |
| MatCha            | MatCha：Enhancing Visual Language Pretraining with Math Reasoning and Chart Derrendering | Paper      | Code      |           |          |                                                  |
| MGP-STR           | Multi-Granularity Prediction for Scene Text Recognition      | Paper      | Code      |           |          |                                                  |
| Nougat            | Nougat: Neural Optical Understanding for Academic Documents  | Paper      | Code      |           |          |                                                  |
| OneFormer         | OneFormer: One Transformer to Rule Universal Image Segmentation | Paper      | Code      |           |          |                                                  |
| OWL-ViT           | Simple Open-Vocabulary Object Detection with Vision Transformers | Paper      | Code      |           |          |                                                  |
| PaliGemma         | PaliGemma – Google’s Cutting-Edge Open Vision Language Model | Paper      | Code      |           |          |                                                  |
| Perceiver         | Perceiver IO: A General Architecture for Structured Inputs & Outputs | Paper      | Code      |           |          |                                                  |
| Pix2Struct        | Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding | Paper      | Code      |           |          |                                                  |
| SAM               | Segment Anything                                             | Paper      | Code      |           |          |                                                  |
| SigLIP            | Sigmoid Loss for Language Image Pre-Training                 | Paper      | Code      |           |          |                                                  |
| TAPAS             | TAPAS: Weakly Supervised Table Parsing via Pre-training      | Paper      | Code      |           |          |                                                  |
| TrOCR             | TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models | Paper      | Code      |           |          |                                                  |
| TVLT              | TVLT: Textless Vision-Language Transformer                   | Paper      | Code      |           |          |                                                  |
| TVP               | Text-Visual Prompting for Efficient 2D Temporal Video Grounding | Paper      | Code      |           |          |                                                  |
| UDOP              | Unifying Vision, Text, and Layout for Universal Document Processing | Paper      | Code      |           |          |                                                  |
| Video-LLaVA       | Video-LLaVA: Learning United Visual Representation by Alignment Before Projection | Paper      | Code      |           |          |                                                  |
| ViLT              | ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision | Paper      | Code      |           |          |                                                  |
| VipLlava          | Making Large Multimodal Models Understand Arbitrary Visual Prompts | Paper      | Code      |           |          |                                                  |
| VisualBERT        | VisualBERT: A Simple and Performant Baseline for Vision and Language | Paper      | Code      |           |          |                                                  |
| X-CLIP            | Expanding Language-Image Pretrained Models for General Video Recognition | Paper      | Code      |           |          |                                                  |
|                   |                                                              |            |           |           |          |                                                  |

- Vision Encoder Decoder Models

  - Vision Models

    - [ViT](https://huggingface.co/docs/transformers/model_doc/vit)
    - [BEiT](https://huggingface.co/docs/transformers/model_doc/beit)
    - [Swin](https://huggingface.co/docs/transformers/model_doc/swin)


  - Language Models

    - [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)
    - [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)
    - [BERT](https://huggingface.co/docs/transformers/model_doc/bert)
    - [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)



- Vision TextDual Encoder
  
- Speech Encoder Decoder Models



## Language Models


|                      | Title                                                        | Paper Link                                                   | Code Link                                                    | Published | Keywords | Venue     |
| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | --------- | -------- | --------- |
| ALBERT               | ALBERT: A Lite BERT for Self-supervised Learning of Language Representations | Paper                                                        | Code                                                         |           |          |           |
| BART                 | BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension | Paper                                                        | Code                                                         |           |          |           |
| BARThez              | BARThez: a Skilled Pretrained French Sequence-to-Sequence Model | Paper                                                        | Code                                                         |           |          |           |
| BARTpho              | BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese | Paper                                                        | Code                                                         |           |          |           |
| BERT                 | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | Paper                                                        | Code                                                         |           |          |           |
| BertGeneration       | Leveraging Pre-trained Checkpoints for Sequence Generation Tasks | Paper                                                        | Code                                                         |           |          |           |
| BERTweet             | BERTweet: A pre-trained language model for English Tweets    | Paper                                                        | Code                                                         |           |          |           |
| BigBird              | Big Bird: Transformers for Longer Sequences                  | Paper                                                        | Code                                                         |           |          |           |
| BioGPT               | BioGPT: generative pre-trained transformer for biomedical text generation and mining | Paper                                                        | Code                                                         |           |          |           |
| Blenderbot           | Recipes for building an open-domain chatbot                  | Paper                                                        | Code                                                         |           |          |           |
| BLOOM                | Introducing The World’s Largest Open Multilingual Language Model: BLOOM | [Blog](https://bigscience.huggingface.co/blog/bloom)         | Code                                                         |           |          |           |
| BORT                 | Optimal Subarchitecture Extraction for BERT                  | Paper                                                        | Code                                                         |           |          |           |
| ByT5                 | ByT5: Towards a token-free future with pre-trained byte-to-byte models | Paper                                                        | Code                                                         |           |          |           |
| CamemBERT            | CamemBERT: a Tasty French Language Model                     | Paper                                                        | Code                                                         |           |          |           |
| CANINE               | CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation | Paper                                                        | Code                                                         |           |          |           |
| CodeGen              | A Conversational Paradigm for Program Synthesis              | Paper                                                        | Code                                                         |           |          |           |
| CodeLlama            | Code Llama: Open Foundation Models for Code                  | Paper                                                        | Code                                                         |           |          |           |
| Cohere               | Command-R: Retrieval Augmented Generation at Production Scale | Paper                                                        | Code                                                         |           |          |           |
| ConvBERT             | ConvBERT: Improving BERT with Span-based Dynamic Convolution | Paper                                                        | Code                                                         |           |          |           |
| CPM                  | CPM: A Large-scale Generative Chinese Pre-trained Language Model | Paper                                                        | Code                                                         |           |          |           |
| CPMAnt               | CPM-Ant is an open-source Chinese pre-trained language model (PLM) with 10B parameters. | Paper                                                        | [Code](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live) |           |          |           |
| CTRL                 | CTRL: A Conditional Transformer Language Model for Controllable Generation | Paper                                                        | Code                                                         |           |          |           |
| DBRX                 | DBRX is a [transformer-based](https://www.isattentionallyouneed.com/) decoder-only large language model (LLM) that was trained using next-token prediction. | [Blog](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm) | Code                                                         |           |          |           |
| DeBERTa              | DeBERTa：Decoding-enhanced BERT with Disentangled Attention  | Paper                                                        | Code                                                         |           |          |           |
| DeBERTa-v2           | DeBERTa：Decoding-enhanced BERT with Disentangled Attention  | Paper                                                        | Code                                                         |           |          |           |
| DialoGPT             | DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation | Paper                                                        | Code                                                         |           |          |           |
| DistilBERT           | DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter | Paper                                                        | Code                                                         |           |          |           |
| DPR                  | Dense Passage Retrieval for Open-Domain Question Answering   | Paper                                                        | Code                                                         |           |          |           |
| ELECTRA              | ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators | Paper                                                        | Code                                                         |           |          |           |
| ERNIE 1.0            | ERNIE: Enhanced Representation through Knowledge Integration | [Paper](https://arxiv.org/abs/1904.09223)                    | Code                                                         | 2019      |          |           |
| ERNIE 2.0            | ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding | [Paper](https://ojs.aaai.org/index.php/AAAI/article/view/6428) | Code                                                         | 2020      |          | AAAI 2020 |
| ERNIE 3.0            | ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation | [Paper](https://arxiv.org/abs/2107.02137)                    | Code                                                         | 2021      |          |           |
| ERNIE-Gram           | ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding | [Paper](https://arxiv.org/abs/2010.12148)                    | Code                                                         | 2020      |          |           |
| ERNIE-health         | Building Chinese Biomedical Language Models via Multi-Level Text Discrimination | [Paper](https://arxiv.org/abs/2110.07244)                    | Code                                                         | 2022      |          |           |
| ErnieM               | ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora | [Paper](https://arxiv.org/abs/2012.15674)                    | Code                                                         | 2020      |          |           |
| ESM                  | Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences | Paper                                                        | Code                                                         |           |          |           |
| Falcon               | The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only | [Paper](https://arxiv.org/abs/2306.01116)                    | Code                                                         |           |          |           |
| FastSpeech2Conformer | Recent Developments On Espnet Toolkit Boosted By Conformer   | Paper                                                        | Code                                                         |           |          |           |
| FLAN-T5              | Scaling Instruction-Finetuned Language Models                | [Paper](https://arxiv.org/pdf/2210.11416.pdf)                | Code                                                         |           |          |           |
| FLAN-UL2             | Scaling Instruction-Finetuned Language Models                | [Paper](https://arxiv.org/pdf/2210.11416.pdf)                | Code                                                         |           |          |           |
| FlauBERT             | FlauBERT：Unsupervised Language Model Pre-training for French | Paper                                                        | Code                                                         |           |          |           |
| FNet                 | FNet: Mixing Tokens with Fourier Transforms                  | Paper                                                        | Code                                                         |           |          |           |
| FSMT                 | Facebook FAIR’s WMT19 News Translation Task Submission       | Paper                                                        | Code                                                         |           |          |           |
| Funnel Transformer   | Funnel-Transformer：Filtering out Sequential Redemption for Efficient Language Processing | Paper                                                        | Code                                                         |           |          |           |
| Fuyu                 | Fuyu-8B: A Multimodal Architecture for AI Agents             | [Blog](https://www.adept.ai/blog/fuyu-8b)                    | Code                                                         |           |          |           |
| Gemma                | Gemma：Open Models Based on Gemini Technology and Research   | Paper                                                        | Code                                                         |           |          |           |
| Gemma2               | Gemma2: Open Models Based on Gemini Technology and Research  | Paper                                                        | Code                                                         |           |          |           |
| OpenAI GPT           | Improving Language Understanding by Generative Pre-Training  | Paper                                                        | Code                                                         |           |          |           |
| GPT Neo              | The Pile: An 800GB Dataset of Diverse Text for Language Modeling | [Paper](https://arxiv.org/abs/2101.00027)                    | [Code](https://github.com/EleutherAI/gpt-neo)                |           |          |           |
| GPTBigCode           | SantaCoder: don't reach for the stars!                       | [Paper](https://arxiv.org/abs/2301.03988)                    | Code                                                         |           |          |           |
| OpenAI GPT2          | Language Models are Unsupervised Multitask Learners          | Paper                                                        | Code                                                         |           | 1.5B     |           |
| GPT-Sw3              | Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish | Paper                                                        | Code                                                         |           |          |           |
| HerBERT              | KLEJ: Comprehensive Benchmark for Polish Language Understanding | Paper                                                        | Code                                                         |           |          |           |
| I-BERT               | I-BERT: Integer-only BERT Quantization                       | Paper                                                        | Code                                                         |           |          |           |
| Jamba                | Introducing Jamba: AI21's Groundbreaking SSM-Transformer Model | Blog                                                         | Code                                                         |           |          |           |
| Jukebox              | Jukebox: A generative model for music                        | Paper                                                        | Code                                                         |           |          |           |
| LED                  | Longformer: The Long-Document Transformer                    | Paper                                                        | Code                                                         |           |          |           |
| LLaMA                | LLaMA: Open and Efficient Foundation Language Models         | Paper                                                        | Code                                                         |           |          |           |
| Llama2               | LLaMA: Open Foundation and Fine-Tuned Chat Models            | Paper                                                        | Code                                                         |           |          |           |
| Llama3               | Introducing Meta Llama 3: The most capable openly available LLM to date | Paper                                                        | Code                                                         |           |          |           |
| Longformer           | Longformer: The Long-Document Transformer                    | Paper                                                        | Code                                                         |           |          |           |
| LongT5               | LongT5: Efficient Text-To-Text Transformer for Long Sequences | Paper                                                        | Code                                                         |           |          |           |
| LUKE                 | LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention | Paper                                                        | Code                                                         |           |          |           |
| M2M100               | Beyond English-Centric Multilingual Machine Translation      | Paper                                                        | Code                                                         |           |          |           |
| MADLAD-400           | MADLAD-400：A Multilingual And Document-Level Large Audited Dataset]（MADLAD-400：A Multilingual And Document-Level Large Audited Dataset | Paper                                                        | Code                                                         |           |          |           |
| Mamba                | Mamba：Linear-Time Sequence Modeling with Selective State Spaces | Paper                                                        | Code                                                         |           |          |           |
| MarianMT             | A framework for translation models, using the same models as BART. | Paper                                                        | Code                                                         |           |          |           |
| MarkupLM             | MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding | Paper                                                        | Code                                                         |           |          |           |
| MBart and MBart-50   | Multilingual Denoising Pre-training for Neural Machine Translation | Paper                                                        | Code                                                         |           |          |           |
| Mega                 | Mega: Moving Average Equipped Gated Attention                | Paper                                                        | Code                                                         |           |          |           |
| MegatronBERT         | Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism | Paper                                                        | Code                                                         |           |          |           |
| MegatronGPT2         | Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism | Paper                                                        | Code                                                         |           |          |           |
| Mistral              | Mistral-7B is a decoder-only Transformer                     | [Blog](https://mistral.ai/news/announcing-mistral-7b/)       | Code                                                         |           |          |           |
| Mixtral              | a high-quality sparse mixture of experts models (SMoE) with open weights. | [Blog](https://mistral.ai/news/mixtral-of-experts/)          | Code                                                         |           |          |           |
| mLUKE                | mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models | Paper                                                        | Code                                                         |           |          |           |
| MobileBERT           | MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices | Paper                                                        | Code                                                         |           |          |           |
| MPNet                | MPNet：Masked and Permuted Pre-training for Language Understanding | Paper                                                        | Code                                                         |           |          |           |
| MPT                  | MPT models are GPT-style decoder-only transformers with several improvements | Paper                                                        | Code                                                         |           |          |           |
| MRA                  | Multi Resolution Analysis (MRA) for Approximate Self-Attention | Paper                                                        | Code                                                         |           |          |           |
| MT5                  | mT5: A massively multilingual pre-trained text-to-text transformer | Paper                                                        | Code                                                         |           |          |           |
| MVP                  | MVP: Multi-task Supervised Pre-training for Natural Language Generation | Paper                                                        | Code                                                         |           |          |           |
| Nezha                | NEZHA: Neural Contextualized Representation for Chinese Language Understanding | Paper                                                        | Code                                                         |           |          |           |
| NLLB                 | No Language Left Behind: Scaling Human-Centered Machine Translation | Paper                                                        | Code                                                         |           |          |           |
| NLLB-MOE             | No Language Left Behind: Scaling Human-Centered Machine Translation | Paper                                                        | Code                                                         |           |          |           |
| Nyströmformer        | Nyströmformer：A Nyström-Based Algorithm for Approximating Self-Attention | Paper                                                        | Code                                                         |           |          |           |
| OLMo                 | OLMo: Accelerating the Science of Language Models            | Paper                                                        | Code                                                         |           |          |           |
| Open-Llama           | The Open-Llama model was proposed in the open source Open-Llama project by community developer s-JoL. | Paper                                                        | Code                                                         |           |          |           |
| OPT                  | Open Pre-trained Transformer Language Models                 | Paper                                                        | Code                                                         |           |          |           |
| Pegasus              | PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization | Paper                                                        | Code                                                         |           |          |           |
| PEGASUS-X            | Investigating Efficiently Extending Transformers for Long Input Summarization | Paper                                                        | Code                                                         |           |          |           |
| Persimmon            | Persimmon-8B is a fully permissively-licensed model with approximately 8 billion parameters | Paper                                                        | Code                                                         |           |          |           |
| Phi                  | Textbooks Are All You Need                                   | [Paper](https://arxiv.org/abs/2309.05463)                    | Code                                                         |           |          |           |
| Phi-3                | Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone | Paper                                                        | Code                                                         |           |          |           |
| PhoBERT              | PhoBERT: Pre-trained language models for Vietnamese          | Paper                                                        | Code                                                         |           |          |           |
| PLBart               | Unified Pre-training for Program Understanding and Generation | Paper                                                        | Code                                                         |           |          |           |
| ProphetNet           | ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training | Paper                                                        | Code                                                         |           |          |           |
| QDQBERT              | Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation | Paper                                                        | Code                                                         |           |          |           |
| Qwen2                | Qwen2 is the new model series of large language models from the Qwen team. | Paper                                                        | [Code](https://github.com/QwenLM/Qwen2)                      |           |          |           |
| Qwen2MoE             | Qwen2MoE is the new model series of large language models from the Qwen team. | Paper                                                        | Code                                                         |           |          |           |
| REALM                | REALM: Retrieval-Augmented Language Model Pre-Training       | Paper                                                        | Code                                                         |           |          |           |
| RecurrentGemma       | RecurrentGemma: Moving Past Transformers for Efficient Open Language Models | Paper                                                        | Code                                                         |           |          |           |
| Reformer             | Reformer: The Efficient Transformer                          | Paper                                                        | Code                                                         |           |          |           |
| RemBERT              | Rethinking Embedding Coupling in Pre-trained Language Models | Paper                                                        | Code                                                         |           |          |           |
| RetriBERT            | Explain Anything Like I’m Five: A Model for Open Domain Long Form Question Answering | Paper                                                        | Code                                                         |           |          |           |
| RoBERTa              | RoBERTa: A Robustly Optimized BERT Pretraining Approach      | Paper                                                        | Code                                                         |           |          |           |
| RoBERTa-PreLayerNorm | fairseq: A Fast, Extensible Toolkit for Sequence Modeling    | Paper                                                        | Code                                                         |           |          |           |
| RoCBert              | RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining | Paper                                                        | Code                                                         |           |          |           |
| RoFormer             | RoFormer: Enhanced Transformer with Rotary Position Embedding | Paper                                                        | Code                                                         |           |          |           |
| RWKV-4.0             | RWKV: Reinventing RNNs for the Transformer Era               | [Paper](https://arxiv.org/abs/2305.13048)                    | Code                                                         |           |          |           |
| RWKV-5/6 Eagle/Finch | Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence | [Paper](https://arxiv.org/abs/2404.05892)                    | Code                                                         |           |          |           |
| Splinter             | Few-Shot Question Answering by Pretraining Span Selection    | Paper                                                        | Code                                                         |           |          |           |
| SqueezeBERT          | SqueezeBERT: What can computer vision teach NLP about efficient neural networks? | Paper                                                        | Code                                                         |           |          |           |
| StableLM             | StableLM-3B-4E1T                                             | [Paper](https://stability.wandb.io/stability-llm/stable-lm/reports/StableLM-3B-4E1T--VmlldzoyMjU4?accessToken=u3zujipenkx5g7rtcj9qojjgxpconyjktjkli2po09nffrffdhhchq045vp0wyfo) | Code                                                         |           |          |           |
| Starcoder2           | StarCoder 2 and The Stack v2: The Next Generation            | Paper                                                        | Code                                                         |           |          |           |
| SwitchTransformers   | Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity | Paper                                                        | Code                                                         |           |          |           |
| T5                   | Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer | Paper                                                        | Code                                                         |           |          |           |
| T5v1.1               | Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer | Paper                                                        | Code                                                         |           |          |           |
| TAPEX                | TAPEX: Table Pre-training via Learning a Neural SQL Executor | Paper                                                        | Code                                                         |           |          |           |
| Transformer XL       | Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context | Paper                                                        | Code                                                         |           |          |           |
| UL2                  | Unifying Language Learning Paradigms                         | Paper                                                        | Code                                                         |           |          |           |
| UMT5                 | UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining | Paper                                                        | Code                                                         |           |          |           |
| X-MOD                | Lifting the Curse of Multilinguality by Pre-training Modular Transformers | Paper                                                        | Code                                                         |           |          |           |
| XGLM                 | Few-shot Learning with Multilingual Language Models          | Paper                                                        | Code                                                         |           |          |           |
| XLM                  | Cross-lingual Language Model Pretraining                     | Paper                                                        | Code                                                         |           |          |           |
| XLM-ProphetNet       | ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training | Paper                                                        | Code                                                         | 2020      |          |           |
| XLM-RoBERTa          | Unsupervised Cross-lingual Representation Learning at Scale  | Paper                                                        | Code                                                         |           |          |           |
| XLM-RoBERTa-XL       | Larger-Scale Transformers for Multilingual Masked Language Modeling | Paper                                                        | Code                                                         |           |          |           |
| XLM-V                | XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models | Paper                                                        | Code                                                         |           |          |           |
| XLNet                | XLNet: Generalized Autoregressive Pretraining for Language Understanding | Paper                                                        | Code                                                         |           |          |           |
| YOSO                 | You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling | Paper                                                        | Code                                                         |           |          |           |
|                      |                                                              |                                                              |                                                              |           |          |           |

- Encoder Decoder Models
- RAG



## Vision Models


|      | Title | Paper Link | Code Link | Published | Keywords | Venue |
| ---- | ----- | ---------- | --------- | --------- | -------- | ----- |
|      |       |            |           |           |          |       |
|      |       |            |           |           |          |       |
|      |       |            |           |           |          |       |



## Speech Models

|      | Title | Paper Link | Code Link | Published | Keywords | Venue |
| ---- | ----- | ---------- | --------- | --------- | -------- | ----- |
|      |       |            |           |           |          |       |
|      |       |            |           |           |          |       |
|      |       |            |           |           |          |       |
|      |       |            |           |           |          |       |





## VLM Pre-training Methods

---

|          | Title                                                        | Paper Link                                                 | Code Link                                                    | Published | Keywords | Venue     |
| -------- | ------------------------------------------------------------ | ---------------------------------------------------------- | ------------------------------------------------------------ | --------- | -------- | --------- |
|          | Efficient Vision-Language Pre-training by Cluster Masking    | [Paper](https://arxiv.org/pdf/2405.08815)                  | [Code](https://github.com/Zi-hao-Wei/Efficient-Vision-Language-Pre-training-by-Cluster-Masking) | 2024      |          | CVPR 2024 |
|          | Towards Better Vision-Inspired Vision-Language Models        | [Paper](https://www.lamda.nju.edu.cn/caoyh/files/VIVL.pdf) | Code                                                         | 2024      |          | CVPR 2024 |
|          | Non-autoregressive Sequence-to-Sequence Vision-Language Models | [Paper](https://arxiv.org/abs/2403.02249v1)                | Code                                                         | 2024      |          | CVPR 2024 |
| ViTamin  | ViTamin: Designing Scalable Vision Models in the Vision-Language Era | [Paper](https://arxiv.org/abs/2404.02132v1)                | [Code](https://github.com/Beckschen/ViTamin)                 | 2024      |          | CVPR 2024 |
|          | Iterated Learning Improves Compositionality in Large Vision-Language Models | [Paper](https://arxiv.org/abs/2404.02145v1)                | Code                                                         | 2024      |          | CVPR 2024 |
| FairCLIP | FairCLIP: Harnessing Fairness in Vision-Language Learning    | [Paper](https://arxiv.org/abs/2403.19949v1)                | [Code](https://ophai.hms.harvard.edu/datasets/fairvlmed10k)  | 2024      |          | CVPR 2024 |
| InternVL | InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks | [Paper](https://arxiv.org/abs/2312.14238)                  | [Code](https://github.com/OpenGVLab/InternVL)                | 2024      |          | CVPR 2024 |
| VILA     | VILA: On Pre-training for Visual Language Models             | [Paper](https://arxiv.org/abs/2312.07533)                  | Code                                                         | 2024      |          | CVPR 2024 |
|          | Generative Region-Language Pretraining for Open-Ended Object Detection | [Paper](https://arxiv.org/pdf/2403.10191v1.pdf)            | [Code](https://github.com/FoundationVision/GenerateU)        | 2024      |          | CVPR 2024 |
|          | Enhancing Vision-Language Pre-training with Rich Supervisions | [Paper](https://arxiv.org/pdf/2403.03346v1.pdf)            | Code                                                         | 2024      |          | CVPR 2024 |
|          | Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization | [Paper](https://arxiv.org/abs/2309.04669)                  | [Code](https://github.com/jy0205/LaVIT)                      | 2024      |          | ICLR 2024 |
| MMICL    | MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning | [Paper](https://arxiv.org/abs/2309.07915)                  | [Code](https://github.com/PKUnlp-icler/MIC)                  | 2024      |          | ICLR 2024 |
|          | Retrieval-Enhanced Contrastive Vision-Text Models            | [Paper](https://arxiv.org/abs/2306.07196)                  | Code                                                         | 2024      |          | ICLR 2024 |
|          |                                                              |                                                            |                                                              |           |          |           |

---

## VLM Transfer Learning Methods

| -                           | Title                                                        | Paper Link                                        | Code Link                                                    | Published | Keywords | Venue      |
| --------------------------- | ------------------------------------------------------------ | ------------------------------------------------- | ------------------------------------------------------------ | --------- | -------- | ---------- |
| CLAP                        | CLAP: Isolating Content from Style through Contrastive Learning with Augmented Prompts | [Paper](https://arxiv.org/abs/2311.16445)         | [Code](https://github.com/YichaoCai1/CLAP)                   | 2024      |          | ECCV 2024  |
| FALIP                       | FALIP: Visual Prompt as Foveal Attention Boosts CLIP Zero-Shot Performance | [Paper](https://arxiv.org/abs/2407.05578v1)       | [Code](https://pumpkin805.github.io/FALIP/)                  | 2024      |          | ECCV 2024  |
| GalLoP                      | GalLoP: Learning Global and Local Prompts for Vision-Language Models | [Paper](https://arxiv.org/pdf/2407.01400)         | Code                                                         | 2024      |          | ECCV 2024  |
| Mind the Interference       | Mind the Interference: Retaining Pre-trained Knowledge in Parameter Efficient Continual Learning of Vision-Language Models | [Paper](https://arxiv.org/pdf/2407.05342v1)       | [Code](https://github.com/lloongx/DIKI)                      | 2024      |          | ECCV 2024  |
|                             | One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models | [Paper](https://arxiv.org/abs/2403.01849v1)       | [Code](https://github.com/TreeLLi/APT)                       | 2024      |          | CVPR 2024  |
|                             | Any-Shift Prompting for Generalization over Distributions    | [Paper](https://arxiv.org/abs/2402.10099)         | Code                                                         | 2024      |          | CVPR 2024  |
| CLAP                        | A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models | [Paper](https://arxiv.org/abs/2312.12730)         | [Code](https://github.com/jusiro/CLAP)                       | 2024      |          | CVPR 2024  |
|                             | Anchor-based Robust Finetuning of Vision-Language Models     | [Paper](https://arxiv.org/abs/2404.06244)         | Code                                                         | 2024      |          | CVPR 2024  |
|                             | Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners | Paper                                             | Code                                                         | 2024      |          | CVPR 2024  |
|                             | Visual In-Context Prompting                                  | [Paper](https://arxiv.org/abs/2311.13601)         | [Code](https://github.com/UX-Decoder/DINOv)                  | 2024      |          | CVPR 2024  |
| TCP                         | TCP:Textual-based Class-aware Prompt tuning for Visual-Language Model | [Paper](https://arxiv.org/abs/2311.18231)         | [Code](https://github.com/htyao89/Textual-based_Class-aware_prompt_tuning/) | 2024      |          | CVPR 2024  |
|                             | Efficient Test-Time Adaptation of Vision-Language Models     | [Paper](https://arxiv.org/abs/2403.18293v1)       | [Code](https://kdiaaa.github.io/tda/)                        | 2024      |          | CVPR 2024  |
| Dual Memory Networks        | Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models | [Paper](https://arxiv.org/abs/2403.17589v1)       | [Code](https://github.com/YBZh/DMN)                          | 2024      |          | CVPR 2024  |
| DePT                        | DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning | [Paper](https://arxiv.org/abs/2309.05173)         | [Code](https://github.com/ZhengxiangShi/DePT)                | 2024      |          | ICLR 2024  |
| Nemesis                     | Nemesis: Normalizing the soft-prompt vectors of vision-language models | [Paper](https://openreview.net/pdf?id=zmJDzPh1Dm) | Code                                                         | 2024      |          | ICLR 2024  |
|                             | Prompt Gradient Projection for Continual Learning            | [Paper](https://openreview.net/pdf?id=EH2O3h7sBI) | Code                                                         | 2024      |          | ICLR 2024  |
| An Image Is Worth 1000 Lies | An Image Is Worth 1000 Lies: Transferability of Adversarial Images across Prompts on Vision-Language Models | [Paper](https://openreview.net/pdf?id=nc5GgFAvtk) | Code                                                         | 2024      |          | ICLR 2024  |
| Matcher                     | Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching | [Paper](https://arxiv.org/abs/2305.13310)         | Code                                                         | 2024      |          | ICLR 2024  |
|                             | Text-driven Prompt Generation for Vision-Language Models in Federated Learning | [Paper](https://arxiv.org/abs/2310.06123)         | Code                                                         | 2024      |          | ICLR 2024  |
|                             | Consistency-guided Prompt Learning for Vision-Language Models | [Paper](https://arxiv.org/abs/2306.01195)         | Code                                                         | 2024      |          | ICLR 2024  |
| C-TPT                       | C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion | [Paper](https://openreview.net/pdf?id=jzzEHTBFOT) | Code                                                         | 2024      |          | ICLR 2024  |
|                             | Learning to Prompt Segment Anything Models                   | [Paper](https://arxiv.org/pdf/2401.04651.pdf)     | Code                                                         | 2024      |          | arXiv 2024 |
|                             |                                                              |                                                   |                                                              |           |          |            |



## VLM Knowledge Distillation for Detection

|             | Title                                                        | Paper Link                                        | Code Link                                                 | Published | Keywords | Venue     |
| ----------- | ------------------------------------------------------------ | ------------------------------------------------- | --------------------------------------------------------- | --------- | -------- | --------- |
| RegionGPT   | RegionGPT: Towards Region Understanding Vision Language Model | [Paper](https://arxiv.org/pdf/2403.02330v1.pdf)   | [Code](https://guoqiushan.github.io/regiongpt.github.io/) |           |          | CVPR 2024 |
|             | LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors | [Paper](https://arxiv.org/pdf/2402.04630.pdf)     | Code                                                      |           |          | ICLR 2024 |
| Ins-DetCLIP | Ins-DetCLIP: Aligning Detection Model to Follow Human-Language Instruction | [Paper](https://openreview.net/pdf?id=M0MF4t3hE9) | Code                                                      |           |          | ICLR 2024 |

 

## VLM Knowledge Distillation for Segmentation

|          | Title                                                        | Paper Link                                | Code Link | Published | Keywords | Venue     |
| -------- | ------------------------------------------------------------ | ----------------------------------------- | --------- | --------- | -------- | --------- |
| CLIPSelf | CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction | [Paper](https://arxiv.org/abs/2310.01403) | Code      | 2024      |          | ICLR 2024 |
|          |                                                              |                                           |           |           |          |           |




## VLM Knowledge Distillation for Other Vision Tasks

|             | Title                                                        | Paper Link                                    | Code Link                                     | Published | Keywords | Venue     |
| ----------- | ------------------------------------------------------------ | --------------------------------------------- | --------------------------------------------- | --------- | -------- | --------- |
| FROSTER     | FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition | [Paper](https://arxiv.org/pdf/2402.03241.pdf) | Code                                          | 2024      |          | ICLR 2024 |
| AnomalyCLIP | AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection | [Paper](https://arxiv.org/pdf/2310.18961.pdf) | [Code](https://github.com/zqhang/AnomalyCLIP) | 2024      |          | ICLR 2024 |
|             |                                                              |                                               |                                               |           |          |           |



---

| -          | Title                                                        | Paper Link                                                   | Code Link                                                    | Published | Keywords                                   | Venue        |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | --------- | ------------------------------------------ | ------------ |
| CoOp       | Learning to Prompt for Vision-Language Models                | [Paper](https://arxiv.org/abs/2203.05557)                    | [Code](https://github.com/KaiyangZhou/CoOp)                  | 2022      | Use text-based learnable prompts/adapters  | IJCV 2022    |
| CoCoOp     | Conditional Prompt Learning for Vision-Language Models       | [Paper](https://arxiv.org/abs/2203.05557)                    | [Code](https://github.com/KaiyangZhou/CoOp)                  | 2022      | Use text-based learnable prompts/adapters  | CVPR 2022    |
| ProDA      | Prompt Distribution Learning                                 | [Paper](https://arxiv.org/abs/2205.03340)                    | [Code](https://github.com/bbbdylan/proda)                    | 2022      | Use text-based learnable prompts/adapters  | CVPR 2022    |
| VPT        | Visual Prompt Tuning                                         | [Paper](https://arxiv.org/abs/2203.12119)                    | [Code](https://github.com/kmnp/vpt)                          | 2022      | Use image-based learnable prompts/adapters | ECCV 2022    |
| MaPLe      | MaPLe: Multi-modal Prompt Learning                           | [Paper](https://arxiv.org/abs/2210.03117)                    | [Code](https://github.com/muzairkhattak/multimodal-prompt-learning) | 2023      |                                            | CVPR 2023    |
| KgCoOp     | Visual-Language Prompt Tuningx with Knowledge-guided Context Optimization | [Paper](https://arxiv.org/abs/2303.13283)                    | [Code](https://github.com/htyao89/KgCoOp)                    | 2023      |                                            | CVPR 2023    |
| LASP       | LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models | [Paper](https://arxiv.org/abs/2210.01115)                    | -                                                            | 2023      |                                            | CVPR 2023    |
| DAM-VP     | Diversity-Aware Meta Visual Prompting                        | [Paper](https://arxiv.org/abs/2303.08138)                    | [Code](https://github.com/shikiw/DAM-VP)                     | 2023      |                                            | CVPR 2023    |
| TaskRes    | Task Residual for Tuning Vision-Language Models              | [Paper](https://arxiv.org/abs/2211.10277)                    | [Code](https://github.com/geekyutao/TaskRes)                 | 2023      |                                            | CVPR 2023    |
| RPO        | Read-only Prompt Optimization for Vision-Language Few-shot Learning | [Paper](https://arxiv.org/abs/2308.14960)                    | [Code](https://github.com/mlvlab/rpo)                        | 2023      |                                            | ICCV 2023    |
| KAPT       | Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models | [Paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Kan_Knowledge-Aware_Prompt_Tuning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.pdf) | -                                                            | 2023      |                                            | ICCV 2023    |
| ProGrad    | Prompt-aligned Gradient for Prompt Tuning                    | [Paper](https://arxiv.org/abs/2205.14865)                    | [Code](https://github.com/BeierZhu/Prompt-align)             | 2023      |                                            | ICCV 2023    |
| PromptSRC  | Self-regulating Prompts: Foundational Model Adaptation without Forgetting | [Paper](https://openaccess.thecvf.com//content/ICCV2023/papers/Khattak_Self-regulating_Prompts_Foundational_Model_Adaptation_without_Forgetting_ICCV_2023_paper.pdf) | [Code](https://github.com/muzairkhattak/PromptSRC)           | 2023      |                                            | ICCV 2023    |
| DeFo       | Learning to Decompose Visual Features with Latent Textual Prompts | [Paper](https://arxiv.org/abs/2210.04287)                    | -                                                            | 2023      |                                            | ICLR 2023    |
| POMP       | Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition | [Paper](https://arxiv.org/abs/2304.04704)                    | [Code](https://github.com/amazon-science/prompt-pretraining) | 2023      |                                            | NeurIPS 2023 |
| MetaPrompt | Learning Domain Invariant Prompt for Vision-Language Models  | [Paper](https://arxiv.org/abs/2212.04196)                    | -                                                            | 2024      |                                            | TIP 2024     |
| SA2VP      | SA2VP: Spatially Aligned-and-Adapted Visual Prompt           | [Paper](https://arxiv.org/abs/2312.10376)                    | [Code](https://github.com/tommy-xq/SA2VP)                    | 2024      |                                            |              |
| HPT        | Learning Hierarchical Prompt with Structured Linguistic Knowledge for Vision-Language Models | [Paper](https://arxiv.org/abs/2312.06323)                    | [Code](https://github.com/Vill-Lab/2024-AAAI-HPT)            | 2024      |                                            |              |
| LaViP      | LaViP: Language-Grounded Visual Prompts                      | [Paper](https://arxiv.org/abs/2312.10945)                    | -                                                            | 2024      |                                            |              |
| CoPrompt   | Consistency-guided Prompt Learning for Vision-Language Models | [Paper](https://arxiv.org/abs/2306.01195)                    | [Code](https://github.com/ShuvenduRoy/CoPrompt)              | 2024      |                                            |              |
| ProText    | Learning to Prompt with Text Only Supervision for Vision-Language Models | [Paper](https://arxiv.org/abs/2401.02418)                    | [Code](https://github.com/muzairkhattak/ProText)             | 2024      |                                            |              |
| PromptKD   | Unsupervised Prompt Distillation for Vision Language Models  | [Paper](https://arxiv.org/abs/2403.02781)                    | [Code](https://github.com/zhengli97/PromptKD)                | 2024      |                                            |              |
| DePT       | DePT: Decoupled Prompt Tuning                                | [Paper](https://arxiv.org/abs/2309.07439)                    | [Code](https://github.com/Koorye/DePT)                       | 2024      |                                            |              |
| ArGue      | ArGue: Attribute-Guided Prompt Tuning for Vision-Language Models | [Paper](https://arxiv.org/abs/2311.16494)                    | -                                                            | 2024      |                                            |              |
| TCP        | TCP:Textual-based Class-aware Prompt tuning for Visual-Language Model | [Paper](https://arxiv.org/abs/2311.18231)                    | [Code](https://github.com/htyao89/Textual-based_Class-aware_prompt_tuning) | 2024      |                                            |              |
| MMA        | MMA: Multi-Modal Adapter for Vision-Language Models          | [Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_MMA_Multi-Modal_Adapter_for_Vision-Language_Models_CVPR_2024_paper.pdf) | [Code](https://github.com/ZjjConan/Multi-Modal-Adapter)      | 2024      |                                            |              |
