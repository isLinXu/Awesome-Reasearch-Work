## On-Hugging-face

| Model / Methods     | Title                                                                                                | Paper Link                                                                                                    | Code Link                                                                                                                                                                                                                                                                  | Published | Keywords         | Venue        |
| ------------------- | ---------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | ---------------- | ------------ |
| BEiT                | BEiT: BERT Pre-Training of Image Transformers                                                        | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2111.09714)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/unilm/tree/master/beit)![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star)                                                       | 2022      | microsoft        |              |
| BiT                 | Big Transfer (BiT): General Visual Representation Learning                                           | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1912.11370)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/big_transfer)![Star](https://img.shields.io/github/stars/google-research/big_transfer.svg?style=social&label=Star)                                              | 2020      | google-research  |              |
| Conditional DETR    | Conditional DETR for Fast Training Convergence                                                       | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2108.06152)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/Atten4Vis/ConditionalDETR)![Star](https://img.shields.io/github/stars/Atten4Vis/ConditionalDETR.svg?style=social&label=Star)                                                    | 2023      |                  |              |
| ConvNeXT            | A ConvNet for the 2020s                                                                              | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2201.03545)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/ConvNeXt)![Star](https://img.shields.io/github/stars/facebookresearch/ConvNeXt.svg?style=social&label=Star)                                                    | 2022      |                  |              |
| ConvNeXt V2         | ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders                              | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2301.00808)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/ConvNeXt-V2)![Star](https://img.shields.io/github/stars/facebookresearch/ConvNeXt-V2.svg?style=social&label=Star)                                              | 2023      |                  |              |
| CvT                 | CvT: Introducing Convolutions to Vision Transformers                                                 | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2103.15808)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/CvT)![Star](https://img.shields.io/github/stars/microsoft/CvT.svg?style=social&label=Star)                                                                            | 2021      |                  |              |
| Deformable DETR     | Deformable DETR: Deformable Transformers for End-to-End Object Detection                             | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2010.04159)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/fundamentalvision/Deformable-DETR)![Star](https://img.shields.io/github/stars/fundamentalvision/Deformable-DETR.svg?style=social&label=Star)                                    | 2020      |                  |              |
| DeiT                | Training data-efficient image transformers & distillation through attention                          | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2012.12877)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/deit)![Star](https://img.shields.io/github/stars/facebookresearch/deit.svg?style=social&label=Star)                                                            | 2021      |                  |              |
| Depth Anything      | Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data                                   | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2401.10891)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/LiheYoung/Depth-Anything)![Star](https://img.shields.io/github/stars/LiheYoung/Depth-Anything.svg?style=social&label=Star)                                                      | 2024      |                  |              |
| Depth Anything V2   | Depth Anything V2                                                                                    | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2406.09414)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/DepthAnything/Depth-Anything-V2)![Star](https://img.shields.io/github/stars/DepthAnything/Depth-Anything-V2.svg?style=social&label=Star)                                        | 2024      |                  |              |
| DETA                | NMS Strikes Back                                                                                     | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2212.06137)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/jozhang97/DETA)![Star](https://img.shields.io/github/stars/jozhang97/DETA.svg?style=social&label=Star)                                                                          | 2021      |                  |              |
| DETR                | End-to-End Object Detection with Transformers                                                        | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2005.12872)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/detr)![Star](https://img.shields.io/github/stars/facebookresearch/detr.svg?style=social&label=Star)                                                            | 2020      | facebookresearch |              |
| DiNAT               | Dilated Neighborhood Attention Transformer                                                           | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2209.15001)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer)![Star](https://img.shields.io/github/stars/SHI-Labs/Neighborhood-Attention-Transformer.svg?style=social&label=Star)                | 2022      | SHI-Labs         |              |
| DINOv2              | DINOv2: Learning Robust Visual Features without Supervision                                          | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2304.07193)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/dinov2)![Star](https://img.shields.io/github/stars/facebookresearch/dinov2.svg?style=social&label=Star)                                                        | 2023      | facebookresearch |              |
| DiT                 | DiT: Self-supervised Pre-training for Document Image Transformer                                     | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2203.02378)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/unilm/tree/master/dit)![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star)                                                        | 2022      |                  |              |
| DPT                 | Vision Transformers for Dense Prediction                                                             | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2103.13413)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/isl-org/DPT)![Star](https://img.shields.io/github/stars/isl-org/DPT.svg?style=social&label=Star)                                                                                | 2021      |                  |              |
| EfficientFormer     | EfficientFormer: Vision Transformers at MobileNet Speed                                              | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2206.01191)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/snap-research/EfficientFormer)![Star](https://img.shields.io/github/stars/snap-research/EfficientFormer.svg?style=social&label=Star)                                            | 2022      | snap-research    | NeurIPs 2022 |
| EfficientFormerV2   | Rethinking Vision Transformers for MobileNet Size and Speed                                          | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2212.08059)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/snap-research/EfficientFormer)![Star](https://img.shields.io/github/stars/snap-research/EfficientFormer.svg?style=social&label=Star)                                            | 2023      | snap-research    | ICCV 2023    |
| EfficientNet        | EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks                             | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1905.11946)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet)![Star](https://img.shields.io/github/stars/tensorflow/tpu.svg?style=social&label=Star)                                 | 2019      | tensorflow       |              |
| FocalNet            | Focal Modulation Networks                                                                            | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2203.11926)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/FocalNet)![Star](https://img.shields.io/github/stars/microsoft/FocalNet.svg?style=social&label=Star)                                                                  | 2022      |                  |              |
| GLPN                | Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth                     | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2201.07436)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/vinvino02/GLPDepth)![Star](https://img.shields.io/github/stars/vinvino02/GLPDepth.svg?style=social&label=Star)                                                                  | 2022      |                  |              |
| Hiera               | Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles                              | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2306.00989)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/hiera)![Star](https://img.shields.io/github/stars/facebookresearch/hiera.svg?style=social&label=Star)                                                          | 2023      |                  |              |
| OpenAI ImageGPT     | Generative Pretraining from Pixels                                                                   | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://openai.com/blog/image-gpt)  | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/openai/image-gpt)![Star](https://img.shields.io/github/stars/openai/image-gpt.svg?style=social&label=Star)                                                                      | 2020      | openai           | ICML 2020    |
| LeViT               | LeViT: Introducing Convolutions to Vision Transformers                                               | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2104.01136)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/LeViT)![Star](https://img.shields.io/github/stars/facebookresearch/LeViT.svg?style=social&label=Star)                                                          | 2021      | facebookresearch |              |
| Mask2Former         | Masked-attention Mask Transformer for Universal Image Segmentation                                   | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2112.01527)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/Mask2Former)![Star](https://img.shields.io/github/stars/facebookresearch/Mask2Former.svg?style=social&label=Star)                                              | 2022      |                  |              |
| MaskFormer          | Per-Pixel Classification is Not All You Need for Semantic Segmentation                               | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2107.06278)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/MaskFormer)![Star](https://img.shields.io/github/stars/facebookresearch/MaskFormer.svg?style=social&label=Star)                                                | 2021      | facebookresearch | NeurIPS 2021 |
| MobileNet V1        | MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications                   | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1704.04861)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md)![Star](https://img.shields.io/github/stars/tensorflow/models.svg?style=social&label=Star)                     | 2017      |                  |              |
| MobileNet V2        | MobileNetV2: Inverted Residuals and Linear Bottlenecks                                               | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1801.04381)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md)![Star](https://img.shields.io/github/stars/tensorflow/models.svg?style=social&label=Star)                     | 2018      |                  |              |
| MobileViT           | MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer                     | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2110.02178)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/apple/ml-cvnets)![Star](https://img.shields.io/github/stars/apple/ml-cvnets.svg?style=social&label=Star)                                                                        | 2021      | apple            |              |
| MobileViT V2        | Separable Self-attention for Mobile Vision Transformers                                              | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2206.02680)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/apple/ml-cvnets)![Star](https://img.shields.io/github/stars/apple/ml-cvnets.svg?style=social&label=Star)                                                                        | 2021      | apple            |              |
| NAT                 | Neighborhood Attention Transformer                                                                   | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2204.07143)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer)![Star](https://img.shields.io/github/stars/SHI-Labs/Neighborhood-Attention-Transformer.svg?style=social&label=Star)                | 2022      | SHI-Labs         |              |
| PoolFormer          | MetaFormer is Actually What You Need for Vision                                                      | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2111.11418)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/sail-sg/poolformer)![Star](https://img.shields.io/github/stars/sail-sg/poolformer.svg?style=social&label=Star)                                                                  | 2021      |                  |              |
| PVT                 | Pyramid Vision Transformer (PVT)                                                                     | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2102.12122)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/whai362/PVT)![Star](https://img.shields.io/github/stars/whai362/PVT.svg?style=social&label=Star)                                                                                | 2021      |                  |              |
| PVTv2               | PVT v2: Improved Baselines with Pyramid Vision Transformer                                           | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2106.13797)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/whai362/PVT)![Star](https://img.shields.io/github/stars/whai362/PVT.svg?style=social&label=Star)                                                                                | 2021      |                  |              |
| RegNet              | Designing Network Design Spaces                                                                      | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2003.13678)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/pycls)![Star](https://img.shields.io/github/stars/facebookresearch/pycls.svg?style=social&label=Star)                                                          | 2020      | facebookresearch |              |
| ResNet              | Deep Residual Learning for Image Recognition                                                         | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1512.03385)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/KaimingHe/deep-residual-networks)![Star](https://img.shields.io/github/stars/KaimingHe/deep-residual-networks.svg?style=social&label=Star)                                      | 2015      |                  |              |
| RT-DETR             | DETRs Beat YOLOs on Real-time Object Detection                                                       | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2304.08069)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/lyuwenyu/RT-DETR/)![Star](https://img.shields.io/github/stars/lyuwenyu/RT-DETR.svg?style=social&label=Star)                                                                     | 2023      |                  |              |
| SegFormer           | SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers                   | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2105.15203)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/NVlabs/SegFormer)![Star](https://img.shields.io/github/stars/NVlabs/SegFormer.svg?style=social&label=Star)                                                                      | 2021      |                  |              |
| SegGPT              | SegGPT: Segmenting Everything In Context                                                             | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2304.03284)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/baaivision/Painter)![Star](https://img.shields.io/github/stars/baaivision/Painter.svg?style=social&label=Star)                                                                  | 2023      |                  |              |
| SuperPoint          | SuperPoint: Self-Supervised Interest Point Detection and Description                                 | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1712.07629)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/magicleap/SuperPointPretrainedNetwork)![Star](https://img.shields.io/github/stars/magicleap/SuperPointPretrainedNetwork.svg?style=social&label=Star)                            | 2017      |                  |              |
| SwiftFormer         | SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2303.15446)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/Amshaker/SwiftFormer)![Star](https://img.shields.io/github/stars/Amshaker/SwiftFormer.svg?style=social&label=Star)                                                              | 2023      |                  |              |
| Swin Transformer    | Swin Transformer: Hierarchical Vision Transformer using Shifted Windows                              | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2103.14030)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/Swin-Transformer)![Star](https://img.shields.io/github/stars/microsoft/Swin-Transformer.svg?style=social&label=Star)                                                  | 2021      | microsoft        |              |
| Swin Transformer V2 | Swin Transformer V2: Scaling Up Capacity and Resolution                                              | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2111.09883)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/Swin-Transformer)![Star](https://img.shields.io/github/stars/microsoft/Swin-Transformer.svg?style=social&label=Star)                                                  | 2021      | microsoft        |              |
| Swin2SR             | Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration                    | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2209.11345)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mv-lab/swin2sr)![Star](https://img.shields.io/github/stars/mv-lab/swin2sr.svg?style=social&label=Star)                                                                          | 2022      |                  | ECCV 2022    |
| Table Transformer   | PubTables-1M: Towards comprehensive table extraction from unstructured documents                     | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2110.00061)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/table-transformer)![Star](https://img.shields.io/github/stars/microsoft/table-transformer.svg?style=social&label=Star)                                                | 2021      |                  |              |
| UPerNet             | Unified Perceptual Parsing for Scene Understanding                                                   | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/1807.10221)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/uper_head.py)![Star](https://img.shields.io/github/stars/open-mmlab/mmsegmentation.svg?style=social&label=Star) | 2018      |                  |              |
| VAN                 | Visual Attention Network                                                                             | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2202.09741)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/Visual-Attention-Network/VAN-Classification)![Star](https://img.shields.io/github/stars/Visual-Attention-Network/VAN-Classification.svg?style=social&label=Star)                | 2022      |                  |              |
| ViT                 | An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale                           | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2010.11929)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/vision_transformer)![Star](https://img.shields.io/github/stars/google-research/vision_transformer.svg?style=social&label=Star)                                  | 2020      | google-research  |              |
| ViT Hybrid          | An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale                           | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2010.11929)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/vision_transformer)![Star](https://img.shields.io/github/stars/google-research/vision_transformer.svg?style=social&label=Star)                                  | 2020      |                  |              |
| ViTDet              | Exploring Plain Vision Transformer Backbones for Object Detection                                    | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2203.16527)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/detectron2/tree/main/projects/ViTDet)![Star](https://img.shields.io/github/stars/facebookresearch/detectron2.svg?style=social&label=Star)                      | 2022      |                  |              |
| ViTMAE              | Masked Autoencoders Are Scalable Vision Learners                                                     | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2111.06377v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/mae)![Star](https://img.shields.io/github/stars/facebookresearch/mae.svg?style=social&label=Star)                                                              | 2021      |                  |              |
| ViTMatte            | Boosting Image Matting with Pretrained Plain Vision Transformers                                     | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2305.15272)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/hustvl/ViTMatte)![Star](https://img.shields.io/github/stars/hustvl/ViTMatte.svg?style=social&label=Star)                                                                        | 2023      |                  |              |
| ViTMSN              | Masked Siamese Networks for Label-Efficient Learning                                                 | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2204.07141)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/msn)![Star](https://img.shields.io/github/stars/facebookresearch/msn.svg?style=social&label=Star)                                                              | 2022      | facebookresearch |              |
| YOLOS               | You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection             | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2106.00666)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/hustvl/YOLOS)![Star](https://img.shields.io/github/stars/hustvl/YOLOS.svg?style=social&label=Star)                                                                              | 2021      |                  |              |
| ZoeDepth            | ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth                                  | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2302.12288)   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/Caojunxu/AC-FPN)![Star](https://img.shields.io/github/stars/Caojunxu/AC-FPN.svg?style=social&label=Star)                                                                        | 2023      |                  |              |
|                     |                                                                                                      |                                                                                                               |                                                                                                                                                                                                                                                                            |           |                  |              |
|                     |                                                                                                      |                                                                                                               |                                                                                                                                                                                                                                                                            |           |                  |              |