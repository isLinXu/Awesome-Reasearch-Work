### on-LVIS-v1.0-Benchmark

| Model / Methods                  | Title                                                                                                       | Paper Link                                                                                                                                                                                                                      | Code Link                                                                                                                                                                                                                                             | Published | AP(0.5⬆️)lvis | Keywords | Venue                                                          |
| -------------------------------- | ----------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | ------------- | -------- | -------------------------------------------------------------- |
| LaMI-DETR                        | LaMI-DETR: Open-Vocabulary Detection with Language Model Instruction                                        | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2407.11335v2)                                                                                                                   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/eternaldolphin/lami-detr)![Star](https://img.shields.io/github/stars/eternaldolphin/lami-detr.svg?style=social&label=Star)                                 | 2024      | 43.4          |          | ECCV2024                                                       |
| DITO                             | Region-centric Image-Language Pretraining for Open-Vocabulary Detection                                     | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2310.00161v2)                                                                                                                   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/google-research)![Star](https://img.shields.io/github/stars/google-research/google-research.svg?style=social&label=Star)                   | 2023      | 40.4          |          |                                                                |
| OV-DQUO (ViT-L/14)               | OV-DQUO: Open-Vocabulary DETR with Denoising Text Query Training and Open-World Unknown Objects Supervision | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2405.17913v1)                                                                                                                   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/xiaomoguhz/ov-dquo)![Star](https://img.shields.io/github/stars/xiaomoguhz/ov-dquo.svg?style=social&label=Star)                                             | 2024      | 39.3          |          |                                                                |
| CoDet (EVA02-L)                  | CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection                      | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2310.16667v1)                                                                                                                   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/cvmi-lab/codet)![Star](https://img.shields.io/github/stars/cvmi-lab/codet.svg?style=social&label=Star)                                                     | 2023      | 37.0          |          |                                                                |
| CLIPSelf                         | CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction                           | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2310.01403v2)                                                                                                                   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/wusize/clipself)![Star](https://img.shields.io/github/stars/wusize/clipself.svg?style=social&label=Star)                                                   | 2023      | 34.9          |          |                                                                |
| DE-ViT                           | Detect Everything with Few Examples                                                                         | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2309.12969v3)                                                                                                                   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mlzxy/devit)![Star](https://img.shields.io/github/stars/mlzxy/devit.svg?style=social&label=Star)                                                           | 2023      | 34.3          |          |                                                                |
| CFM-ViT                          | Contrastive Feature Masking Open-Vocabulary Vision Transformer                                              | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2309.00775v1)                                                                                                                   | ![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)                                                                                                                                                                                 | 2023      | 33.9          |          | [ICCV 2023](https://paperswithcode.com/conference/iccv-2023-1) |
| CLIM (RN50x64)                   | CLIM: Contrastive Language-Image Mosaic for Region Representation                                           | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2312.11376v2)                                                                                                                   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/wusize/clim)![Star](https://img.shields.io/github/stars/wusize/clim.svg?style=social&label=Star)                                                           | 2023      | 32.3          |          |                                                                |
| RO-ViT                           | Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers                      | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2305.07011v4)                                                                                                                   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mcahny/rovit)![Star](https://img.shields.io/github/stars/mcahny/rovit.svg?style=social&label=Star)                                                         | 2023      | 32.1          |          |                                                                |
| RTGen                            | Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers                      | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2405.19854v1)                                                                                                                   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/seermer/RTGen)![Star](https://img.shields.io/github/stars/seermer/RTGen.svg?style=social&label=Star)                                                       | 2024      | 30.2          |          |                                                                |
| OV-DQUO (ViT-B/16)               | OV-DQUO: Open-Vocabulary DETR with Denoising Text Query Training and Open-World Unknown Objects Supervision | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2405.17913v1)                                                                                                                   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/xiaomoguhz/ov-dquo)![Star](https://img.shields.io/github/stars/xiaomoguhz/ov-dquo.svg?style=social&label=Star)                                             | 2024      | 29.7          |          |                                                                |
| ViLD-ensemble w/ ALIGN (Eb7-FPN) | Open-vocabulary Object Detection via Vision and Language Knowledge Distillation                             | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2104.13921v3)                                                                                                                   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild)![Star](https://img.shields.io/github/stars/tensorflow/tpu.svg?style=social&label=Star) | 2021      | 26.3          |          |                                                                |
| OWL-ViT (CLIP-L/14)              | Simple Open-Vocabulary Object Detection with Vision Transformers                                            | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2205.06230v2)                                                                                                                   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit)![Star](https://img.shields.io/github/stars/google-research/scenic.svg?style=social&label=Star)   | 2022      | 25.6          |          |                                                                |
| POMP                             | Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition                     | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2205.06230v2)                                                                                                                   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit)![Star](https://img.shields.io/github/stars/google-research/scenic.svg?style=social&label=Star)   | 2023      | 25.2          |          |                                                                |
| BARON                            | Aligning Bag of Regions for Open-Vocabulary Object Detection                                                | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2302.13996v1)                                                                                                                   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/wusize/ovdet)![Star](https://img.shields.io/github/stars/wusize/ovdet.svg?style=social&label=Star)                                                         | 2023      | 22.6          |          | CVPR 2023                                                      |
| MEDet                            | Open Vocabulary Object Detection with Proposal Mining and Prediction Equalization                           | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2206.11134v4)                                                                                                                   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/peixianchen/medet)![Star](https://img.shields.io/github/stars/peixianchen/medet.svg?style=social&label=Star)                                               | 2022      | 22.4          |          |                                                                |
| Region-CLIP (RN50x4-C4)          | RegionCLIP: Region-based Language-Image Pretraining                                                         | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](http://openaccess.thecvf.com//content/CVPR2022/html/Zhong_RegionCLIP_Region-Based_Language-Image_Pretraining_CVPR_2022_paper.html)                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/regionclip)![Star](https://img.shields.io/github/stars/microsoft/regionclip.svg?style=social&label=Star)                                         | 2021      | 22.0          |          | CVPR 2022                                                      |
| RALF                             | Retrieval-Augmented Open-Vocabulary Object Detection                                                        | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](http://openaccess.thecvf.com//content/CVPR2024/html/Kim_Retrieval-Augmented_Open-Vocabulary_Object_Detection_CVPR_2024_paper.html)                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mlvlab/RALF)![Star](https://img.shields.io/github/stars/mlvlab/RALF.svg?style=social&label=Star)                                                           | 2024      | 21.9          |          | CVPR 2024                                                      |
| OADP                             | Object-Aware Distillation Pyramid for Open-Vocabulary Object Detection                                      | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](http://openaccess.thecvf.com//content/CVPR2023/html/Wang_Object-Aware_Distillation_Pyramid_for_Open-Vocabulary_Object_Detection_CVPR_2023_paper.html) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/lutingwang/oadp)![Star](https://img.shields.io/github/stars/lutingwang/oadp.svg?style=social&label=Star)                                                   | 2023      | 21.7          |          | CVPR 2023                                                      |
| X-Paste                          | X-Paste: Revisiting Scalable Copy-Paste for Instance Segmentation using CLIP and StableDiffusion            | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2212.03863v2)                                                                                                                   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/yoctta/xpaste)![Star](https://img.shields.io/github/stars/yoctta/xpaste.svg?style=social&label=Star)                                                       | 2022      | 21.4          |          |                                                                |
| Object-Centric-OVD               | Bridging the Gap between Object and Image-level Representations for Open-Vocabulary Detection               | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2207.03482v3)                                                                                                                   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mmaaz60/mvits_for_class_agnostic_od)![Star](https://img.shields.io/github/stars/mmaaz60/mvits_for_class_agnostic_od.svg?style=social&label=Star)           | 2022      | 21.1          |          |                                                                |
| ViLD-ensemble (R152-FPN)         | Open-vocabulary Object Detection via Vision and Language Knowledge Distillation                             | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2207.03482v3)                                                                                                                   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mmaaz60/mvits_for_class_agnostic_od)![Star](https://img.shields.io/github/stars/mmaaz60/mvits_for_class_agnostic_od.svg?style=social&label=Star)           | 2021      | 18.7          |          |                                                                |
| Detic                            | Detecting Twenty-thousand Classes using Image-level Supervision                                             | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://arxiv.org/abs/2201.02605v3)                                                                                                                   | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/Detic)![Star](https://img.shields.io/github/stars/facebookresearch/Detic.svg?style=social&label=Star)                                     | 2022      | 17.8          |          |                                                                |
| Region-CLIP (RN50-C4)            | Detecting Twenty-thousand Classes using Image-level Supervision                                             | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](http://openaccess.thecvf.com//content/CVPR2022/html/Zhong_RegionCLIP_Region-Based_Language-Image_Pretraining_CVPR_2022_paper.html)                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/regionclip)![Star](https://img.shields.io/github/stars/microsoft/regionclip.svg?style=social&label=Star)                                         | 2021      | 17.1          |          |                                                                |
| ViLD-ensemble (R50-FPN)          | Open-vocabulary Object Detection via Vision and Language Knowledge Distillation                             | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://openreview.net/forum?id=lL3lnMbR4WU)                                                                                                          | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/tensorflow/tpu)![Star](https://img.shields.io/github/stars/tensorflow/tpu.svg?style=social&label=Star)                                                     | 2021      | 16.6          |          | ICLR 2023                                                      |
| ViLD (R50-FPN)                   | Open-vocabulary Object Detection via Vision and Language Knowledge Distillation                             | [![Paper](https://img.shields.io/badge/Paper-ydd7e6?style=for-the-badge)](https://openreview.net/forum?id=lL3lnMbR4WU)                                                                                                          | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/tensorflow/tpu)![Star](https://img.shields.io/github/stars/tensorflow/tpu.svg?style=social&label=Star)                                                     | 2021      | 16.1          |          |                                                                |
|                                  |                                                                                                             |                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                       |           |               |          |                                                                |
|                                  |                                                                                                             |                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                       |           |               |          |                                                                |