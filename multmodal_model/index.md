# MultiModal-Models

| Model / Methods   | Title                                                                                                               | Paper Link                                                                                                                                                                                                                     | Code Link                                                                                                                                                                                                                                                                     | Published | Keywords         | Venue                                            |
| ----------------- | ------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | ---------------- | ------------------------------------------------ |
| ALIGN             | Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision                           | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2102.05918)                                                                                                                    | ![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)                                                                                                                                                                                                         | 2021      |                  |                                                  |
| AltCLIP           | AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities                                   | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2211.06679v2)                                                                                                                  | ![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)                                                                                                                                                                                                         | 2022      |                  |                                                  |
| BLIP              | BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation            | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2201.12086)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/salesforce/BLIP)![Star](https://img.shields.io/github/stars/salesforce/BLIP.svg?style=social&label=Star)                                                                           | 2022      | salesforce       |                                                  |
| BLIP-2            | BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models              | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2301.12597)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/salesforce/BLIP)![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star)                                                                          | 2023      | salesforce       |                                                  |
| BLIP-3            |                                                                                                                     |                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                               |           |                  |                                                  |
| BridgeTower       | BridgeTower: Building Bridges Between Encoders in Vision-Language Representative Learning                           | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2206.08657)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/BridgeTower)![Star](https://img.shields.io/github/stars/microsoft/BridgeTower.svg?style=social&label=Star)                                                               | 2023      | microsoft        | [AAAI’23](https://aaai.org/Conferences/AAAI-23/) |
| BROS              | BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2108.04539)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/BridgeTower)![Star](https://img.shields.io/github/stars/clovaai/bros.svg?style=social&label=Star)                                                                        | 2021      | clovaai          |                                                  |
| Chameleon         | Chameleon: Mixed-Modal Early-Fusion Foundation Models                                                               | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2405.09818v1)                                                                                                                  | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/chameleon)![Star](https://img.shields.io/github/stars/facebookresearch/chameleon.svg?style=social&label=Star)                                                     | 2024      | facebookresearch |                                                  |
| Chinese-CLIP      | Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese                                                    | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2211.01335)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://huggingface.co/OFA-Sys)                                                                                                                                                                       | 2023      | OFA-Sys          |                                                  |
| CLIP              | Learning Transferable Visual Models From Natural Language Supervision                                               | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2103.00020)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/openai/CLIP)![Star](https://img.shields.io/github/stars/openai/CLIP.svg?style=social&label=Star)                                                                                   | 2021      | openai           |                                                  |
| CLIPSeg           | Image Segmentation Using Text and Image Prompts                                                                     | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2112.10003)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/timojl/clipseg)![Star](https://img.shields.io/github/stars/timojl/clipseg.svg?style=social&label=Star)                                                                             | 2021      |                  | CVPR 2022                                        |
| CLVP              | Better speech synthesis through scaling                                                                             | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2305.07243)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/neonbjb/tortoise-tts)![Star](https://img.shields.io/github/stars/neonbjb/tortoise-tts.svg?style=social&label=Star)                                                                 | 2023      |                  |                                                  |
| Data2Vec          | data 2 vec：A General Framework for Self-supervised Learning in Speech，Vision and Language                           | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/pdf/2202.03555)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/data2vec_vision)![Star](https://img.shields.io/github/stars/facebookresearch/data2vec_vision.svg?style=social&label=Star)                                         | 2022      |                  |                                                  |
| DePlot            | DePlot: One-shot visual language reasoning by plot-to-table translation                                             | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2212.10505)                                                                                                                    | ![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)                                                                                                                                                                                                         | 2022      |                  |                                                  |
| Donut             | OCR-free Document Understanding Transformer                                                                         | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2111.15664)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/clovaai/donut)![Star](https://img.shields.io/github/stars/clovaai/donut.svg?style=social&label=Star)                                                                               | 2021      | clovaai          |                                                  |
| FLAVA             | FLAVA: A Foundational Language And Vision Alignment Model                                                           | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2112.04482)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/multimodal)![Star](https://img.shields.io/github/stars/facebookresearch/multimodal.svg?style=social&label=Star)                                                   | 2021      | facebookresearch |                                                  |
| GIT               | GIT: A Generative Image-to-text Transformer for Vision and Language                                                 | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2205.14100)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/GenerativeImage2Text)![Star](https://img.shields.io/github/stars/microsoft/GenerativeImage2Text.svg?style=social&label=Star)                                             | 2022      | microsoft        |                                                  |
| Grounding DINO    | Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection                              | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2303.05499)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/IDEA-Research/GroundingDINO)![Star](https://img.shields.io/github/stars/IDEA-Research/GroundingDINO.svg?style=social&label=Star)                                                   | 2023      | IDEA-Research    |                                                  |
| GroupViT          | GroupViT: Semantic Segmentation Emerges from Text Supervision                                                       | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2202.11094)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/NVlabs/GroupViT)![Star](https://img.shields.io/github/stars/NVlabs/GroupViT.svg?style=social&label=Star)                                                                           | 2022      | NVlabs           |                                                  |
| IDEFICS           | OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents                                     | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://huggingface.co/papers/2306.16527)                                                                                                            | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://huggingface.co/docs/transformers/model_doc/INSERT%20LINK%20TO%20GITHUB%20REPO%20HERE)                                                                                                         | 2023      | NVlabs           |                                                  |
| IDEFICS-2         | What matters when building vision-language models?                                                                  | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2405.02246)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/NVlabs/GroupViT)                                                                                                                                                                   | 2024      | NVlabs           |                                                  |
| InstructBLIP      | InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning                                | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2305.06500)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star)                                         | 2023      | salesforce       |                                                  |
| InstructBlipVideo | InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning                                | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2305.06500)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star)                                         | 2023      | salesforce       |                                                  |
| KOSMOS-2          | Kosmos-2: Grounding Multimodal Large Language Models to the World                                                   | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2306.14824)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/unilm/tree/master/kosmos-2)![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star)                                                      | 2023      | microsoft        |                                                  |
| LayoutLM          | LayoutLM: Pre-training of Text and Layout for Document Image Understanding                                          | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1912.13318)                                                                                                                    | ![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)                                                                                                                                                                                                         | 2019      | microsoft        |                                                  |
| LayoutLMV2        | LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding                                       | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2012.14740)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/NielsRogge/Transformers-Tutorials)![Star](https://img.shields.io/github/stars/NielsRogge/Transformers-Tutorials.svg?style=social&label=Star)                                       | 2020      | microsoft        |                                                  |
| LayoutLMv3        | LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking                                        | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2204.08387)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/NielsRogge/Transformers-Tutorials)![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star)                                                         | 2022      | microsoft        |                                                  |
| LayoutXLM         | LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding                            | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2104.08836)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/NielsRogge/microsoft/unilm)![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star)                                                                | 2021      | microsoft        |                                                  |
| LiLT              | LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding          | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2202.13669)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/jpwang/lilt)![Star](https://img.shields.io/github/stars/jpwang/lilt.svg?style=social&label=Star)                                                                                   | 2022      |                  |                                                  |
| LLaVa             | Visual Instruction Tuning                                                                                           | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2304.08485)[![Reading](https://img.shields.io/badge/Reading-yellow?style=for-the-badge)](https://papers.cool/arxiv/2102.05918) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/haotian-liu/LLaVA)![Star](https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&label=Star)                                                                       | 2023      |                  |                                                  |
| LLaVa-VL          | Improved Baselines with Visual Instruction Tuning                                                                   | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/pdf/2310.03744)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/haotian-liu/LLaVA)![Star](https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&label=Star)                                                                       | 2024      |                  |                                                  |
| LLaVA-NeXT        | LLaVA-NeXT: Improved reasoning, OCR, and world knowledge                                                            | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2310.03744)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/haotian-liu/LLaVA)![Star](https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&label=Star)                                                                       | 2024      |                  |                                                  |
| LLaVa-NeXT-Video  | LLaVA-NeXT: A Strong Zero-shot Video Understanding Model                                                            | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://llava-vl.github.io/blog/2024-04-30-llava-next-video/)                                                                                        | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/LLaVA-VL/LLaVA-NeXT)![Star](https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT.svg?style=social&label=Star)                                                                   | 2024      |                  |                                                  |
| Video-LLaVA       | Video-LLaVA: Learning United Visual Representation by Alignment Before Projection                                   | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2311.10122)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/LLaVA-VL/LLaVA-NeXT)![Star](https://img.shields.io/github/stars/PKU-YuanGroup/Video-LLaVA.svg?style=social&label=Star)                                                             | 2023      |                  |                                                  |
| LXMERT            | LXMERT: Learning Cross-Modality Encoder Representations from Transformers                                           | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1908.07490)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/airsplay/lxmert)![Star](https://img.shields.io/github/stars/airsplay/lxmert.svg?style=social&label=Star)                                                                           | 2019      |                  |                                                  |
| MatCha            | MatCha：Enhancing Visual Language Pretraining with Math Reasoning and Chart Derrendering                             | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2212.09662)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb)                                                                                                        | 2022      | google           |                                                  |
| MGP-STR           | Multi-Granularity Prediction for Scene Text Recognition                                                             | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2209.03592)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/MGP-STR)![Star](https://img.shields.io/github/stars/AlibabaResearch/AdvancedLiterateMachinery.svg?style=social&label=Star) | 2022      | AlibabaResearch  |                                                  |
| Mini-Omni         | Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming                                               | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2408.16725)[![Reading](https://img.shields.io/badge/Reading-yellow?style=for-the-badge)](https://papers.cool/arxiv/2408.16725) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/gpt-omni/mini-omni)![Star](https://img.shields.io/github/stars/gpt-omni/mini-omni.svg?style=social&label=Star)                                                                     | 2024      | gpt-omni         |                                                  |
| Nougat            | Nougat: Neural Optical Understanding for Academic Documents                                                         | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2308.13418)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/nougat)![Star](https://img.shields.io/github/stars/facebookresearch/nougat.svg?style=social&label=Star)                                                           | 2023      | facebookresearch |                                                  |
| OneFormer         | OneFormer: One Transformer to Rule Universal Image Segmentation                                                     | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2211.06220)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/SHI-Labs/OneFormer)![Star](https://img.shields.io/github/stars/SHI-Labs/OneFormer.svg?style=social&label=Star)                                                                     | 2022      | SHI-Labs         |                                                  |
| OWL-ViT           | Simple Open-Vocabulary Object Detection with Vision Transformers                                                    | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2205.06230)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit)![Star](https://img.shields.io/github/stars/google-research/scenic.svg?style=social&label=Star)                           | 2022      | google           |                                                  |
| OWLv2             | Scaling Open-Vocabulary Object Detection                                                                            | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2306.09683)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit)![Star](https://img.shields.io/github/stars/google-research/scenic.svg?style=social&label=Star)                           | 2023      | google           |                                                  |
| PaliGemma         | PaliGemma – Google’s Cutting-Edge Open Vision Language Model                                                        | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2306.09683)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://huggingface.co/google/paligemma-3b-pt-224)                                                                                                                                                    | 2024      |                  |                                                  |
| Perceiver         | Perceiver IO: A General Architecture for Structured Inputs & Outputs                                                | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2107.14795)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/deepmind/deepmind-research/tree/master/perceiver)![Star](https://img.shields.io/github/stars/deepmind/deepmind-research.svg?style=social&label=Star)                               | 2021      |                  |                                                  |
| Pix2Struct        | Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding                                     | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2210.03347)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/deepmind/deepmind-research/tree/master/perceiver)![Star](https://img.shields.io/github/stars/deepmind/deepmind-research.svg?style=social&label=Star)                               | 2022      |                  |                                                  |
| SAM               | Segment Anything                                                                                                    | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/pdf/2304.02643v1.pdf)                                                                                                              | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/segment-anything)![Star](https://img.shields.io/github/stars/facebookresearch/segment-anything.svg?style=social&label=Star)                                       | 2023      | meta             |                                                  |
| SAM v2            | SAM 2: Segment Anything in Images and Videos                                                                        | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/)                                                              | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/segment-anything-2)![Star](https://img.shields.io/github/stars/facebookresearch/segment-anything-2.svg?style=social&label=Star)                                   | 2024      | meta             |                                                  |
| SigLIP            | Sigmoid Loss for Language Image Pre-Training                                                                        | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2303.15343)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/big_vision)![Star](https://img.shields.io/github/stars/google-research/big_vision.svg?style=social&label=Star)                                                     | 2023      |                  |                                                  |
| TAPAS             | TAPAS: Weakly Supervised Table Parsing via Pre-training                                                             | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://www.aclweb.org/anthology/2020.acl-main.398)                                                                                                  | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/big_vision)![Star](https://img.shields.io/github/stars/google-research/tapas.svg?style=social&label=Star)                                                          | 2020      |                  |                                                  |
| TrOCR             | TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models                                      | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2109.10282)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/unilm/tree/6f60612e7cc86a2a1ae85c47231507a587ab4e01/trocr)![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star)                       | 2021      |                  |                                                  |
| TVLT              | TVLT: Textless Vision-Language Transformer                                                                          | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2209.14156)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/zinengtang/TVLT)![Star](https://img.shields.io/github/stars/zinengtang/TVLT.svg?style=social&label=Star)                                                                           | 2022      |                  |                                                  |
| TVP               | Text-Visual Prompting for Efficient 2D Temporal Video Grounding                                                     | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2303.04995)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/intel/TVP)![Star](https://img.shields.io/github/stars/intel/TVP.svg?style=social&label=Star)                                                                                       | 2023      | Intel            |                                                  |
| UDOP              | Unifying Vision, Text, and Layout for Universal Document Processing                                                 | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2212.02623)                                                                                                                    | ![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)                                                                                                                                                                                                         | 2022      |                  |                                                  |
| ViLT              | ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision                                     | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2102.03334)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/dandelin/ViLT)![Star](https://img.shields.io/github/stars/dandelin/ViLT.svg?style=social&label=Star)                                                                               | 2021      |                  |                                                  |
| VipLlava          | Making Large Multimodal Models Understand Arbitrary Visual Prompts                                                  | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2312.00784)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mu-cai/ViP-LLaVA)![Star](https://img.shields.io/github/stars/mu-cai/ViP-LLaVA.svg?style=social&label=Star)                                                                         | 2023      |                  |                                                  |
| VisualBERT        | VisualBERT: A Simple and Performant Baseline for Vision and Language                                                | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/pdf/1908.03557)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/uclanlp/visualbert)![Star](https://img.shields.io/github/stars/uclanlp/visualbert.svg?style=social&label=Star)                                                                     | 2019      |                  |                                                  |
| X-CLIP            | Expanding Language-Image Pretrained Models for General Video Recognition                                            | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2208.02816)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/VideoX)![Star](https://img.shields.io/github/stars/microsoft/VideoX.svg?style=social&label=Star)                                                                         | 2022      |                  |                                                  |
|                   |                                                                                                                     |                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                               |           |                  |                                                  |

- Vision Encoder Decoder Models
    
    - Vision Models
        
        - [ViT](https://huggingface.co/docs/transformers/model_doc/vit)
            
        - [BEiT](https://huggingface.co/docs/transformers/model_doc/beit)
            
        - [Swin](https://huggingface.co/docs/transformers/model_doc/swin)
            

- Language Models
    
    - [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)
        
    - [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)
        
    - [BERT](https://huggingface.co/docs/transformers/model_doc/bert)
        
    - [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)
        

- Vision TextDual Encoder
    
- Speech Encoder Decoder Models



Reading List

---
#### Section 1: LLMs and MLLMs

1. OpenAI, 2023,[**Introducing ChatGPT**](https://openai.com/blog/chatgpt)  
    
2. OpenAI, 2023,[**GPT-4 Technical Report**](https://arxiv.org/abs/2303.08774)  
    
3. Alayrac, et al., 2022,[**Flamingo: a Visual Language Model for Few-Shot Learning**](https://arxiv.org/abs/2204.14198)  
    
4. Li, et al., 2023,[**BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models**](https://arxiv.org/abs/2301.12597)  
    
5. Zhu, et al., 2023,[**MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models**](https://arxiv.org/abs/2304.10592)  
    
6. Wu, et al., 2023,[**Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models**](https://arxiv.org/abs/2303.04671)  
    
7. Shen, et al., 2023,[**HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face**](https://arxiv.org/abs/2303.17580)  
    
8. Tang, et al., 2023,[**Any-to-Any Generation via Composable Diffusion**](https://arxiv.org/abs/2305.11846)  
    
9. Girdhar, et al., 2023,[**ImageBind: One Embedding Space To Bind Them All**](https://arxiv.org/abs/2305.05665)  
    
10. Wu, et al., 2023,[**NExT-GPT: Any-to-Any Multimodal LLM**](https://arxiv.org/abs/2309.05519)  
    
11. Moon, et al., 2023,[**AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model**](https://arxiv.org/abs/2309.16058)  
    
12. Hu, et al., 2023,[**Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages**](https://arxiv.org/abs/2308.12038)  
    
13. Bai, et al., 2023,[**Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond**](https://arxiv.org/abs/2308.12966)  
    
14. Wang, et al., 2023,[**CogVLM: Visual Expert for Pretrained Language Models**](https://arxiv.org/abs/2311.03079)  
    
15. Peng, et al., 2023,[**Kosmos-2: Grounding Multimodal Large Language Models to the World**](https://arxiv.org/abs/2306.14824)  
    
16. Dong, et al., 2023,[**InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model**](https://arxiv.org/abs/2401.16420)  
    
17. Zhu, et al., 2023,[**LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment**](https://arxiv.org/abs/2310.01852)  
    
18. Ge, et al., 2023,[**Planting a SEED of Vision in Large Language Model**](https://arxiv.org/abs/2307.08041)  
    
19. Zhan, et al., 2024,[**AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling**](https://arxiv.org/abs/2402.12226)  
    
20. Kondratyuk, et al., 2023,[**VideoPoet: A Large Language Model for Zero-Shot Video Generation**](https://arxiv.org/abs/2312.14125)  
    
21. Zhang, et al., 2023,[**SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models**](https://arxiv.org/abs/2308.16692)  
    
22. Zeghidour, et al., 2021,[**SoundStream: An End-to-End Neural Audio Codec**](https://arxiv.org/abs/2107.03312)  
    
23. Liu, et al., 2023,[**Improved Baselines with Visual Instruction Tuning**](https://arxiv.org/abs/2310.03744)  
    
24. Wu, et al., 2023,[**Visual-ChatGPT: Talking, Drawing and Editing with Visual Foundation Models**](https://arxiv.org/abs/2303.04671)  
    
25. Wang, et al., 2023,[**ModaVerse: Efficiently Transforming Modalities with LLMs**](https://arxiv.org/abs/2401.06395)  
    
26. Fei, et al., 2024,[**VITRON: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing**](http://haofei.vip/downloads/papers/Skywork_Vitron_2024.pdf)  
    
27. Lu, et al., 2023,[**Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action**](https://arxiv.org/abs/2312.17172)  
    
28. Bai, et al., 2023,[**LVM: Sequential Modeling Enables Scalable Learning for Large Vision Models**](https://arxiv.org/abs/2312.00785)  
    
29. Huang, et al., 2023,[**Language Is Not All You Need: Aligning Perception with Language Models**](https://arxiv.org/abs/2302.14045)  
    
30. Li, et al., 2023, [**VideoChat: Chat-Centric Video Understanding**](https://arxiv.org/abs/2305.06355)  
    
31. Maaz, et al., 2023, [**Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models**](https://arxiv.org/abs/2306.05424)  
    
32. Zhang, et al., 2023, [**Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding**](https://arxiv.org/abs/2306.02858)  
    
33. Lin, et al., 2023, [**Video-LLaVA: Learning United Visual Representation by Alignment Before Projection**](https://arxiv.org/abs/2311.10122)  
    
34. Qian, et al., 2024, [**Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning**](https://arxiv.org/abs/2402.11435)  
    
35. Hong, et al., 2023, [**3D-LLM: Injecting the 3D World into Large Language Models**](https://arxiv.org/abs/2307.12981)  
    
36. Sun, et al., 2023, [**3D-GPT: Procedural 3D Modeling with Large Language Models**](https://arxiv.org/abs/2310.12945)  
    
37. Chen, et al., 2023, [**LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning**](https://arxiv.org/abs/2311.18651)  
    
38. Xu, et al., 2023, [**PointLLM: Empowering Large Language Models to Understand Point Clouds**](https://arxiv.org/abs/2308.16911)  
    
39. Chen, et al., 2024, [**SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities**](https://arxiv.org/abs/2401.12168)  
    
40. Huang, et al., 2023, [**AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head**](https://arxiv.org/abs/2304.12995)  
    
41. Zhang, et al., 2023, [**SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities**](https://arxiv.org/abs/2305.11000)  
    
42. Wang, et al., 2023, [**VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation**](https://arxiv.org/abs/2305.16107)  
    
43. Rubenstein, et al., 2023, [**AudioPaLM: A Large Language Model That Can Speak and Listen**](https://arxiv.org/abs/2306.12925)  
    
44. Tang, et al., 2023, [**SALMONN: Towards Generic Hearing Abilities for Large Language Models**](https://arxiv.org/abs/2310.13289)  
    
45. Latif, et al., 2023, [**Sparks of Large Audio Models: A Survey and Outlook**](https://arxiv.org/abs/2310.13289)  
    
46. Luo, et al., 2022, [**BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining**](https://arxiv.org/abs/2210.10341)  
    
47. Li, et al., 2023, [**DrugGPT: A GPT-based Strategy for Designing Potential Ligands Targeting Specific Proteins**](https://doi.org/10.1101/2023.06.29.543848)  
    
48. Chen, et al., 2023, [**MEDITRON-70B: Scaling Medical Pretraining for Large Language Models**](https://arxiv.org/abs/2311.16079)  
    
49. Wang, et al., 2023, [**HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge**](https://arxiv.org/abs/2304.06975)  
    
50. Zhang, et al., 2023, [**AlpaCare:Instruction-tuned Large Language Models for Medical Application**](https://arxiv.org/abs/2310.14558)  
    
51. Frey, et al., 2023, [**Neural Scaling of Deep Chemical Models**](https://www.nature.com/articles/s42256-023-00740-3)  
    
52. Zhang, et al., 2023, [**ChemLLM: A Chemical Large Language Model**](https://arxiv.org/abs/2402.06852)  
    
53. Liu, et al., 2023, [**MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter**](https://arxiv.org/abs/2310.12798)  
    
54. Jiang, et al., 2023, [**StructGPT: A General Framework for Large Language Model to Reason on Structured Data**](https://arxiv.org/abs/2305.09645)  
    
55. Chen, et al., 2024, [**LLaGA: Large Language and Graph Assistant**](https://arxiv.org/abs/2402.08170)  
    
56. Koh, et al., 2023, [**Generating Images with Multimodal Language Models**](https://arxiv.org/abs/2305.17216)  
    
57. Sun, et al., 2023, [**Generative Pretraining in Multimodality**](https://arxiv.org/abs/2307.05222)  
    
58. Zheng, et al., 2023, [**MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens**](https://arxiv.org/abs/2310.02239)  
    
59. Dong, et al., 2023, [**DreamLLM: Synergistic Multimodal Comprehension and Creation**](https://arxiv.org/abs/2309.11499)  
    
60. Liu, et al., 2023, [**LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents**](https://arxiv.org/abs/2311.05437)  
    
61. Wang, et al., 2023, [**GPT4Video: A Unified Multimodal Large Language Model for lnstruction-Followed Understanding and Safety-Aware Generation**](https://arxiv.org/abs/2311.16511)  
    
62. Jin, et al., 2024, [**Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization**](https://arxiv.org/abs/2402.03161)  
    
63. Jin, et al., 2023, [**Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding**](https://arxiv.org/abs/2311.08046)  
    
64. Li, et al., 2023, [**LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models**](https://arxiv.org/abs/2311.17043)  
    
65. Su, et al., 2023, [**PandaGPT: One Model to Instruction-Follow Them All**](https://arxiv.org/abs/2305.16355)  
    
66. Lyu, et al., 2023, [**Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration**](https://arxiv.org/abs/2306.09093)  
    
67. Tang, et al., 2023, [**CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation**](https://arxiv.org/abs/2311.18775)  
    
68. Zhang, et al., 2023, [**GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest**](https://arxiv.org/abs/2307.03601)  
    
69. Yuan, et al., 2023, [**Osprey: Pixel Understanding with Visual Instruction Tuning**](https://arxiv.org/abs/2312.10032)  
    
70. Rasheed, et al., 2023, [**GLaMM: Pixel Grounding Large Multimodal Model**](https://arxiv.org/abs/2311.03356)  
    
71. Pi, et al., 2023, [**DetGPT: Detect What You Need via Reasoning**](https://arxiv.org/abs/2305.14167)  
    
72. Ren, et al., 2023, [**PixelLM: Pixel Reasoning with Large Multimodal Model**](https://arxiv.org/abs/2312.02228)  
    
73. Lai, et al., 2023, [**Lisa: Reasoning segmentation via large language model**](https://arxiv.org/abs/2308.00692)  
    
74. Chen, et al., 2023, [**Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic**](https://arxiv.org/abs/2306.15195)  
    
75. Munasinghe, et al., 2023, [**PG-Video-LLaVA: Pixel Grounding in Large Multimodal Video Models**](https://arxiv.org/abs/2311.13435)  
    
76. Yu, et al., 2023, [**Merlin: Empowering Multimodal LLMs with Foresight Minds**](https://arxiv.org/abs/2312.00589)  
    
77. Fu, et al., 2023, [**MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models**](https://arxiv.org/abs/2306.13394)  
    
78. Xu, et al., 2023, [**LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models**](https://arxiv.org/abs/2306.09265)  
    
79. Ying, et al., 2024, [**MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI**](https://arxiv.org/abs/2404.16006)  
    
80. Pan, et al., 2024, [**Auto-Encoding Morph-Tokens for Multimodal LLM**](https://arxiv.org/abs/2405.01926)  
    
81. Thagard, et al., 1997, [**Abductive reasoning: Logic, visual thinking, and coherence**](https://link.springer.com/chapter/10.1007/978-94-017-0487-8_22)  
    
82. Bavishi, et al., 2023, [**Fuyu-8B: A Multimodal Architecture for AI Agents**](https://www.adept.ai/blog/fuyu-8b)  
    

  

#### Section 2: Instruction Tuning

1. Liu, et al., 2023, [**Visual Instruction Tuning**](https://arxiv.org/abs/2304.08485)  
    
2. Liu, et al., 2023, [**Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning**](https://arxiv.org/abs/2306.14565)  
    
3. Gao, et al., 2023, [**LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model**](https://arxiv.org/abs/2304.15010)  
    
4. Zhao, et al., 2023, [**SVIT: Scaling up Visual Instruction Tuning**](https://arxiv.org/abs/2307.04087)  
    
5. Ye, et al., 2023, [**mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality**](https://arxiv.org/abs/2304.14178)  
    
6. Yu, et al., 2023, [**RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback**](https://arxiv.org/abs/2312.00849)  
    
7. Liu, et al., 2023, [**MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning**](https://arxiv.org/abs/2311.10774)  
    
8. Zhao, et al., 2023, [**MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models**](https://arxiv.org/abs/2304.10592)  
    
9. Liu, et al., 2023, [**HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Models**](https://arxiv.org/abs/2310.14566)  
    
10. Li, et al., 2023, [**Evaluating Object Hallucination in Large Vision-Language Models**](https://arxiv.org/abs/2305.10355)  
    
11. Huang, et al., 2023, [**Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey**](https://arxiv.org/abs/2306.13549)  
    
12. Yin, et al., 2023, [**A Survey on Multimodal Large Language Models**](https://arxiv.org/abs/2312.16602)  
    
13. Yin, et al., 2023, [**Woodpecker: Hallucination Correction for Multimodal Large Language Models**](https://arxiv.org/abs/2312.16602)  
    

  

#### Section 3: Reasoning with LLM

1. Zhang, et al., 2023, [**Multimodal Chain-of-Thought Reasoning in Language Models**](https://arxiv.org/abs/2302.00923)  
    
2. Zhao, et al., 2023, [**MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning**](https://arxiv.org/abs/2309.07915)  
    
3. Lu, et al., 2023, [**Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models**](https://arxiv.org/abs/2304.09842)  
    
4. Zhang, et al., 2023, [**You Only Look at Screens: Multimodal Chain-of-Action Agents**](https://arxiv.org/abs/2309.11436)  
    
5. Sun, et al., 2023, [**Generative multimodal models are in-context learners**](https://arxiv.org/abs/2312.13286)  
    
6. Fei, et al., 2023, [**VITRON: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing**](https://vitron-llm.github.io/)  
    
7. Wei, et al., 2023, [**Enhancing Human-like Multi-Modal Reasoning: A New Challenging Dataset and Comprehensive Framework**](https://arxiv.org/abs/2307.12626)  
    
8. Zhang, et al., 2023, [**Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents**](https://arxiv.org/abs/2311.11797)  
    
9. Fei, et al., 2024, [**Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition**](http://haofei.vip/VoT/)  
    
10. Prystawski, et al., 2023, [**Why think step by step? Reasoning emerges from the locality of experience**](https://arxiv.org/abs/2304.03843)  
    
11. Gou, et al., 2023, [**CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing**](https://arxiv.org/abs/2305.11738)  
    
12. Tang, et al., 2024, [**Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science**](https://arxiv.org/abs/2402.04247)  
    
13. Yuan, et al., 2024, [**R-Judge: Benchmarking Safety Risk Awareness for LLM Agents**](https://arxiv.org/abs/2401.10019)  
    

  

#### Section 4: Efficient Learning

1. Hu, et al., 2021, [**LoRA: Low-Rank Adaptation of Large Language Models**](https://arxiv.org/abs/2106.09685)  
    
2. Dettmers, et al., 2023, [**QLoRA: Efficient Finetuning of Quantized LLMs**](https://arxiv.org/abs/2305.14314)  
    
3. Li, et al., 2023, [**BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models**](https://arxiv.org/abs/2301.12597)  
    
4. Luo, et al., 2023, [**Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models**](https://arxiv.org/abs/2305.15023)  
    
5. Yao, et al., 2024, [**MiniCPM-V**](https://github.com/OpenBMB/MiniCPM-V)  
    
6. DeepSpeed Team, 2020, [**DeepSpeed Blog**](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)  
    
7. Zhao, et al., 2023, [**PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel**](https://arxiv.org/abs/2304.11277)  
    
8. Zhu, et al., 2023, [**MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models**](https://arxiv.org/abs/2304.10592)  
    
9. Chen, et al., 2023, [**MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning**](https://arxiv.org/abs/2310.09478)  
    
10. Hong, et al., 2023, [**CogAgent: A Visual Language Model for GUI Agents**](https://arxiv.org/abs/2312.08914)  
    
11. Chen, et al., 2024, [**How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites**](https://arxiv.org/abs/2404.16821)  
    
12. Dehghani, et al., 2023, [**Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution**](https://arxiv.org/abs/2307.06304)  
    
13. Zhang, et al., 2023, [**VPGTrans: Transfer Visual Prompt Generator across LLMs**](https://arxiv.org/abs/2305.01278)  
    
14. Wu, et al., 2023, [**NExT-GPT: Any-to-Any Multimodal LLM**](https://arxiv.org/abs/2309.05519)  
    
15. Fei, et al., 2024, [**VITRON: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing**](http://haofei.vip/downloads/papers/Skywork_Vitron_2024.pdf)  
    
16. Zhang, et al., 2024, [**NExT-Chat: An LMM for Chat, Detection and Segmentation**](https://arxiv.org/abs/2311.04498)
