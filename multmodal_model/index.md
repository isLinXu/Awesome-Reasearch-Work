# MultiModal-Models

| Model / Methods   | Title                                                                                                               | Paper Link                                                                                                                                                                                                                     | Code Link                                                                                                                                                                                                                                                                     | Published | Keywords         | Venue                                            |
| ----------------- | ------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | ---------------- | ------------------------------------------------ |
| ALIGN             | Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision                           | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2102.05918)                                                                                                                    | ![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)                                                                                                                                                                                                         | 2021      |                  |                                                  |
| AltCLIP           | AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities                                   | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2211.06679v2)                                                                                                                  | ![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)                                                                                                                                                                                                         | 2022      |                  |                                                  |
| BLIP              | BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation            | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2201.12086)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/salesforce/BLIP)![Star](https://img.shields.io/github/stars/salesforce/BLIP.svg?style=social&label=Star)                                                                           | 2022      | salesforce       |                                                  |
| BLIP-2            | BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models              | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2301.12597)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/salesforce/BLIP)![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star)                                                                          | 2023      | salesforce       |                                                  |
| BLIP-3            |                                                                                                                     |                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                               |           |                  |                                                  |
| BridgeTower       | BridgeTower: Building Bridges Between Encoders in Vision-Language Representative Learning                           | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2206.08657)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/BridgeTower)![Star](https://img.shields.io/github/stars/microsoft/BridgeTower.svg?style=social&label=Star)                                                               | 2023      | microsoft        | [AAAI’23](https://aaai.org/Conferences/AAAI-23/) |
| BROS              | BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2108.04539)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/BridgeTower)![Star](https://img.shields.io/github/stars/clovaai/bros.svg?style=social&label=Star)                                                                        | 2021      | clovaai          |                                                  |
| Chameleon         | Chameleon: Mixed-Modal Early-Fusion Foundation Models                                                               | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2405.09818v1)                                                                                                                  | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/chameleon)![Star](https://img.shields.io/github/stars/facebookresearch/chameleon.svg?style=social&label=Star)                                                     | 2024      | facebookresearch |                                                  |
| Chinese-CLIP      | Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese                                                    | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2211.01335)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://huggingface.co/OFA-Sys)                                                                                                                                                                       | 2023      | OFA-Sys          |                                                  |
| CLIP              | Learning Transferable Visual Models From Natural Language Supervision                                               | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2103.00020)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/openai/CLIP)![Star](https://img.shields.io/github/stars/openai/CLIP.svg?style=social&label=Star)                                                                                   | 2021      | openai           |                                                  |
| CLIPSeg           | Image Segmentation Using Text and Image Prompts                                                                     | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2112.10003)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/timojl/clipseg)![Star](https://img.shields.io/github/stars/timojl/clipseg.svg?style=social&label=Star)                                                                             | 2021      |                  | CVPR 2022                                        |
| CLVP              | Better speech synthesis through scaling                                                                             | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2305.07243)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/neonbjb/tortoise-tts)![Star](https://img.shields.io/github/stars/neonbjb/tortoise-tts.svg?style=social&label=Star)                                                                 | 2023      |                  |                                                  |
| Data2Vec          | data 2 vec：A General Framework for Self-supervised Learning in Speech，Vision and Language                           | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/pdf/2202.03555)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/data2vec_vision)![Star](https://img.shields.io/github/stars/facebookresearch/data2vec_vision.svg?style=social&label=Star)                                         | 2022      |                  |                                                  |
| DePlot            | DePlot: One-shot visual language reasoning by plot-to-table translation                                             | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2212.10505)                                                                                                                    | ![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)                                                                                                                                                                                                         | 2022      |                  |                                                  |
| Donut             | OCR-free Document Understanding Transformer                                                                         | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2111.15664)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/clovaai/donut)![Star](https://img.shields.io/github/stars/clovaai/donut.svg?style=social&label=Star)                                                                               | 2021      | clovaai          |                                                  |
| FLAVA             | FLAVA: A Foundational Language And Vision Alignment Model                                                           | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2112.04482)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/multimodal)![Star](https://img.shields.io/github/stars/facebookresearch/multimodal.svg?style=social&label=Star)                                                   | 2021      | facebookresearch |                                                  |
| GIT               | GIT: A Generative Image-to-text Transformer for Vision and Language                                                 | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2205.14100)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/GenerativeImage2Text)![Star](https://img.shields.io/github/stars/microsoft/GenerativeImage2Text.svg?style=social&label=Star)                                             | 2022      | microsoft        |                                                  |
| Grounding DINO    | Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection                              | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2303.05499)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/IDEA-Research/GroundingDINO)![Star](https://img.shields.io/github/stars/IDEA-Research/GroundingDINO.svg?style=social&label=Star)                                                   | 2023      | IDEA-Research    |                                                  |
| GroupViT          | GroupViT: Semantic Segmentation Emerges from Text Supervision                                                       | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2202.11094)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/NVlabs/GroupViT)![Star](https://img.shields.io/github/stars/NVlabs/GroupViT.svg?style=social&label=Star)                                                                           | 2022      | NVlabs           |                                                  |
| IDEFICS           | OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents                                     | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://huggingface.co/papers/2306.16527)                                                                                                            | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://huggingface.co/docs/transformers/model_doc/INSERT%20LINK%20TO%20GITHUB%20REPO%20HERE)                                                                                                         | 2023      | NVlabs           |                                                  |
| IDEFICS-2         | What matters when building vision-language models?                                                                  | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2405.02246)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/NVlabs/GroupViT)                                                                                                                                                                   | 2024      | NVlabs           |                                                  |
| InstructBLIP      | InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning                                | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2305.06500)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star)                                         | 2023      | salesforce       |                                                  |
| InstructBlipVideo | InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning                                | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2305.06500)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star)                                         | 2023      | salesforce       |                                                  |
| KOSMOS-2          | Kosmos-2: Grounding Multimodal Large Language Models to the World                                                   | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2306.14824)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/unilm/tree/master/kosmos-2)![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star)                                                      | 2023      | microsoft        |                                                  |
| LayoutLM          | LayoutLM: Pre-training of Text and Layout for Document Image Understanding                                          | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1912.13318)                                                                                                                    | ![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)                                                                                                                                                                                                         | 2019      | microsoft        |                                                  |
| LayoutLMV2        | LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding                                       | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2012.14740)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/NielsRogge/Transformers-Tutorials)![Star](https://img.shields.io/github/stars/NielsRogge/Transformers-Tutorials.svg?style=social&label=Star)                                       | 2020      | microsoft        |                                                  |
| LayoutLMv3        | LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking                                        | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2204.08387)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/NielsRogge/Transformers-Tutorials)![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star)                                                         | 2022      | microsoft        |                                                  |
| LayoutXLM         | LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding                            | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2104.08836)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/NielsRogge/microsoft/unilm)![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star)                                                                | 2021      | microsoft        |                                                  |
| LiLT              | LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding          | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2202.13669)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/jpwang/lilt)![Star](https://img.shields.io/github/stars/jpwang/lilt.svg?style=social&label=Star)                                                                                   | 2022      |                  |                                                  |
| LLaVa             | Visual Instruction Tuning                                                                                           | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2304.08485)[![Reading](https://img.shields.io/badge/Reading-yellow?style=for-the-badge)](https://papers.cool/arxiv/2102.05918) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/haotian-liu/LLaVA)![Star](https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&label=Star)                                                                       | 2023      |                  |                                                  |
| LLaVa-VL          | Improved Baselines with Visual Instruction Tuning                                                                   | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/pdf/2310.03744)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/haotian-liu/LLaVA)![Star](https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&label=Star)                                                                       | 2024      |                  |                                                  |
| LLaVA-NeXT        | LLaVA-NeXT: Improved reasoning, OCR, and world knowledge                                                            | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2310.03744)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/haotian-liu/LLaVA)![Star](https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&label=Star)                                                                       | 2024      |                  |                                                  |
| LLaVa-NeXT-Video  | LLaVA-NeXT: A Strong Zero-shot Video Understanding Model                                                            | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://llava-vl.github.io/blog/2024-04-30-llava-next-video/)                                                                                        | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/LLaVA-VL/LLaVA-NeXT)![Star](https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT.svg?style=social&label=Star)                                                                   | 2024      |                  |                                                  |
| Video-LLaVA       | Video-LLaVA: Learning United Visual Representation by Alignment Before Projection                                   | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2311.10122)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/LLaVA-VL/LLaVA-NeXT)![Star](https://img.shields.io/github/stars/PKU-YuanGroup/Video-LLaVA.svg?style=social&label=Star)                                                             | 2023      |                  |                                                  |
| LXMERT            | LXMERT: Learning Cross-Modality Encoder Representations from Transformers                                           | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/1908.07490)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/airsplay/lxmert)![Star](https://img.shields.io/github/stars/airsplay/lxmert.svg?style=social&label=Star)                                                                           | 2019      |                  |                                                  |
| MatCha            | MatCha：Enhancing Visual Language Pretraining with Math Reasoning and Chart Derrendering                             | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2212.09662)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb)                                                                                                        | 2022      | google           |                                                  |
| MGP-STR           | Multi-Granularity Prediction for Scene Text Recognition                                                             | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2209.03592)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/MGP-STR)![Star](https://img.shields.io/github/stars/AlibabaResearch/AdvancedLiterateMachinery.svg?style=social&label=Star) | 2022      | AlibabaResearch  |                                                  |
| Mini-Omni         | Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming                                               | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2408.16725)[![Reading](https://img.shields.io/badge/Reading-yellow?style=for-the-badge)](https://papers.cool/arxiv/2408.16725) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/gpt-omni/mini-omni)![Star](https://img.shields.io/github/stars/gpt-omni/mini-omni.svg?style=social&label=Star)                                                                     | 2024      | gpt-omni         |                                                  |
| Nougat            | Nougat: Neural Optical Understanding for Academic Documents                                                         | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2308.13418)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/nougat)![Star](https://img.shields.io/github/stars/facebookresearch/nougat.svg?style=social&label=Star)                                                           | 2023      | facebookresearch |                                                  |
| OneFormer         | OneFormer: One Transformer to Rule Universal Image Segmentation                                                     | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2211.06220)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/SHI-Labs/OneFormer)![Star](https://img.shields.io/github/stars/SHI-Labs/OneFormer.svg?style=social&label=Star)                                                                     | 2022      | SHI-Labs         |                                                  |
| OWL-ViT           | Simple Open-Vocabulary Object Detection with Vision Transformers                                                    | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2205.06230)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit)![Star](https://img.shields.io/github/stars/google-research/scenic.svg?style=social&label=Star)                           | 2022      | google           |                                                  |
| OWLv2             | Scaling Open-Vocabulary Object Detection                                                                            | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2306.09683)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit)![Star](https://img.shields.io/github/stars/google-research/scenic.svg?style=social&label=Star)                           | 2023      | google           |                                                  |
| PaliGemma         | PaliGemma – Google’s Cutting-Edge Open Vision Language Model                                                        | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2306.09683)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://huggingface.co/google/paligemma-3b-pt-224)                                                                                                                                                    | 2024      |                  |                                                  |
| Perceiver         | Perceiver IO: A General Architecture for Structured Inputs & Outputs                                                | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2107.14795)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/deepmind/deepmind-research/tree/master/perceiver)![Star](https://img.shields.io/github/stars/deepmind/deepmind-research.svg?style=social&label=Star)                               | 2021      |                  |                                                  |
| Pix2Struct        | Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding                                     | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2210.03347)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/deepmind/deepmind-research/tree/master/perceiver)![Star](https://img.shields.io/github/stars/deepmind/deepmind-research.svg?style=social&label=Star)                               | 2022      |                  |                                                  |
| SAM               | Segment Anything                                                                                                    | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/pdf/2304.02643v1.pdf)                                                                                                              | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/segment-anything)![Star](https://img.shields.io/github/stars/facebookresearch/segment-anything.svg?style=social&label=Star)                                       | 2023      | meta             |                                                  |
| SAM v2            | SAM 2: Segment Anything in Images and Videos                                                                        | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/)                                                              | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/facebookresearch/segment-anything-2)![Star](https://img.shields.io/github/stars/facebookresearch/segment-anything-2.svg?style=social&label=Star)                                   | 2024      | meta             |                                                  |
| SigLIP            | Sigmoid Loss for Language Image Pre-Training                                                                        | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2303.15343)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/big_vision)![Star](https://img.shields.io/github/stars/google-research/big_vision.svg?style=social&label=Star)                                                     | 2023      |                  |                                                  |
| TAPAS             | TAPAS: Weakly Supervised Table Parsing via Pre-training                                                             | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://www.aclweb.org/anthology/2020.acl-main.398)                                                                                                  | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/google-research/big_vision)![Star](https://img.shields.io/github/stars/google-research/tapas.svg?style=social&label=Star)                                                          | 2020      |                  |                                                  |
| TrOCR             | TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models                                      | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2109.10282)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/unilm/tree/6f60612e7cc86a2a1ae85c47231507a587ab4e01/trocr)![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star)                       | 2021      |                  |                                                  |
| TVLT              | TVLT: Textless Vision-Language Transformer                                                                          | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2209.14156)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/zinengtang/TVLT)![Star](https://img.shields.io/github/stars/zinengtang/TVLT.svg?style=social&label=Star)                                                                           | 2022      |                  |                                                  |
| TVP               | Text-Visual Prompting for Efficient 2D Temporal Video Grounding                                                     | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2303.04995)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/intel/TVP)![Star](https://img.shields.io/github/stars/intel/TVP.svg?style=social&label=Star)                                                                                       | 2023      | Intel            |                                                  |
| UDOP              | Unifying Vision, Text, and Layout for Universal Document Processing                                                 | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2212.02623)                                                                                                                    | ![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)                                                                                                                                                                                                         | 2022      |                  |                                                  |
| ViLT              | ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision                                     | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2102.03334)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/dandelin/ViLT)![Star](https://img.shields.io/github/stars/dandelin/ViLT.svg?style=social&label=Star)                                                                               | 2021      |                  |                                                  |
| VipLlava          | Making Large Multimodal Models Understand Arbitrary Visual Prompts                                                  | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2312.00784)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/mu-cai/ViP-LLaVA)![Star](https://img.shields.io/github/stars/mu-cai/ViP-LLaVA.svg?style=social&label=Star)                                                                         | 2023      |                  |                                                  |
| VisualBERT        | VisualBERT: A Simple and Performant Baseline for Vision and Language                                                | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/pdf/1908.03557)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/uclanlp/visualbert)![Star](https://img.shields.io/github/stars/uclanlp/visualbert.svg?style=social&label=Star)                                                                     | 2019      |                  |                                                  |
| X-CLIP            | Expanding Language-Image Pretrained Models for General Video Recognition                                            | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2208.02816)                                                                                                                    | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/microsoft/VideoX)![Star](https://img.shields.io/github/stars/microsoft/VideoX.svg?style=social&label=Star)                                                                         | 2022      |                  |                                                  |
|                   |                                                                                                                     |                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                               |           |                  |                                                  |

- Vision Encoder Decoder Models
    
    - Vision Models
        
        - [ViT](https://huggingface.co/docs/transformers/model_doc/vit)
            
        - [BEiT](https://huggingface.co/docs/transformers/model_doc/beit)
            
        - [Swin](https://huggingface.co/docs/transformers/model_doc/swin)
            

- Language Models
    
    - [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)
        
    - [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)
        
    - [BERT](https://huggingface.co/docs/transformers/model_doc/bert)
        
    - [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)
        

- Vision TextDual Encoder
    
- Speech Encoder Decoder Models