# Audio-Modal-Models

| Model / Methods | Title                                                                                           | Paper Link                                                                                                                                                                                                                               | Code Link                                                                                                                                                                                                   | Published  | Keywords            | Venue |
| --------------- | ----------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------- | ------------------- | ----- |
| Whisper         | Robust Speech Recognition via Large-Scale Weak Supervision                                      | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2212.04356)<br />[![Reading](https://img.shields.io/badge/Reading-yellow?style=for-the-badge)](https://papers.cool/arxiv/2212.04356)     | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/openai/whisper)![Star](https://img.shields.io/github/stars/openai/whisper.svg?style=social&label=Star)           | 2022.12.06 | openai              |       |
| VALL-E          | Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers                          | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2301.02111v1)<br />[![Reading](https://img.shields.io/badge/Reading-yellow?style=for-the-badge)](https://papers.cool/arxiv/2301.02111v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/plachtaa/vall-e-x)![Star](https://img.shields.io/github/stars/plachtaa/vall-e-x.svg?style=social&label=Star)     | 2023.01.05 |                     |       |
| VALOR           | VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset                      | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2304.08345)<br />[![Reading](https://img.shields.io/badge/Reading-yellow?style=for-the-badge)](https://papers.cool/arxiv/2304.08345)     | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/TXH-mercury/VALOR)![Star](https://img.shields.io/github/stars/TXH-mercury/VALOR.svg?style=social&label=Star)     | 2023.04.17 |                     |       |
| VAST            | VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset                   | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2305.18500v2)<br />[![Reading](https://img.shields.io/badge/Reading-yellow?style=for-the-badge)](https://papers.cool/arxiv/2305.18500v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/txh-mercury/vast)![Star](https://img.shields.io/github/stars/txh-mercury/vast.svg?style=social&label=Star)       | 2023.05.29 |                     |       |
| AudioPaLM       | AudioPaLM: A Large Language Model That Can Speak and Listen                                     | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2306.12925v1)<br />[![Reading](https://img.shields.io/badge/Reading-yellow?style=for-the-badge)](https://papers.cool/arxiv/2306.12925v1) | -                                                                                                                                                                                                           | 2023.06.22 | google              |       |
| SALMONN         | SALMONN: Towards Generic Hearing Abilities for Large Language Models                            | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2310.13289v2)<br />[![Reading](https://img.shields.io/badge/Reading-yellow?style=for-the-badge)](https://papers.cool/arxiv/2310.13289v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/bytedance/salmonn)![Star](https://img.shields.io/github/stars/bytedance/salmonn.svg?style=social&label=Star)     | 2023.10.20 | bytedance           |       |
| SpeechGPT-Gen   | SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation                                   | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2401.13527v2)<br />[![Reading](https://img.shields.io/badge/Reading-yellow?style=for-the-badge)](https://papers.cool/arxiv/2401.13527v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/0nutation/speechgpt)![Star](https://img.shields.io/github/stars/0nutation/speechgpt.svg?style=social&label=Star) | 2024.01.24 |                     |       |
| SpeechVerse     | SpeechVerse: A Large-scale Generalizable Audio Language Model                                   | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2405.08295v2)<br />[![Reading](https://img.shields.io/badge/Reading-yellow?style=for-the-badge)](https://papers.cool/arxiv/2405.08295v2) |                                                                                                                                                                                                             | 2024.05.14 |                     |       |
| SpeechGPT       | SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2305.11000v2)<br />[![Reading](https://img.shields.io/badge/Reading-yellow?style=for-the-badge)](https://papers.cool/arxiv/2305.11000v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/gpt-omni/mini-omni)![Star](https://img.shields.io/github/stars/gpt-omni/mini-omni.svg?style=social&label=Star)   | 2024.05.18 | SpeechInstruct      |       |
| video-SALMONN   | video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models                               | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2406.15704v1)<br />[![Reading](https://img.shields.io/badge/Reading-yellow?style=for-the-badge)](https://papers.cool/arxiv/2406.15704v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/bytedance/salmonn)![Star](https://img.shields.io/github/stars/bytedance/salmonn.svg?style=social&label=Star)     | 2024.06.22 | bytedance           |       |
| Qwen2-Audio     | Qwen2-Audio Technical Report                                                                    | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2407.10759v1)<br />[![Reading](https://img.shields.io/badge/Reading-yellow?style=for-the-badge)](https://papers.cool/arxiv/2407.10759v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/qwenlm/qwen2-audio)![Star](https://img.shields.io/github/stars/qwenlm/qwen2-audio.svg?style=social&label=Star)   | 2024.07.15 | alibaba             |       |
| VITA            | Towards Open-Source Interactive Omni Multimodal LLM                                             | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2408.05211v2)<br />[![Reading](https://img.shields.io/badge/Reading-yellow?style=for-the-badge)](https://papers.cool/arxiv/2408.05211v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/VITA-MLLM/VITA)![Star](https://img.shields.io/github/stars/VITA-MLLM/VITA.svg?style=social&label=Star)           | 2024.08.09 |                     |       |
| Mini-Omni       | Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming                           | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://www.arxiv.org/abs/2408.16725)<br />[![Reading](https://img.shields.io/badge/Reading-yellow?style=for-the-badge)](https://papers.cool/arxiv/2408.16725) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/gpt-omni/mini-omni)![Star](https://img.shields.io/github/stars/gpt-omni/mini-omni.svg?style=social&label=Star)   | 2024.08.29 | VoiceAssistant-400K |       |
| LLaMA-Omni      | LLaMA-Omni: Seamless Speech Interaction with Large Language Models                              | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2409.06666v1)<br />[![Reading](https://img.shields.io/badge/Reading-yellow?style=for-the-badge)](https://papers.cool/arxiv/2409.06666v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/ictnlp/llama-omni)![Star](https://img.shields.io/github/stars/ictnlp/llama-omni.svg?style=social&label=Star)     | 2024.09.10 | InstructS2S-200K    |       |
|                 |                                                                                                 |                                                                                                                                                                                                                                          |                                                                                                                                                                                                             |            |                     |       |
|                 |                                                                                                 |                                                                                                                                                                                                                                          |                                                                                                                                                                                                             |            |                     |       |



## Zero-Shot Multi-Speaker TTS

| Model / Methods | Title                                                        | Paper Link                                                   | Code Link                                                    | Published  | Keywords | Venue |
| --------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ---------- | -------- | ----- |
| YourTTS         | YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2112.02418v4)<br />[![Reading](https://img.shields.io/badge/Reading-yellow?style=for-the-badge)](https://papers.cool/arxiv/2112.02418v4) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/coqui-ai/TTS)![Star](https://img.shields.io/github/stars/coqui-ai/TTS.svg?style=social&label=Star) | 2021.12.04 |          |       |
| MegaTTS         | Mega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2306.03509)<br />[![Reading](https://img.shields.io/badge/Reading-yellow?style=for-the-badge)](https://papers.cool/arxiv/2306.03509) | -                                                            | 2023.06.06 |          |       |
| MegaTTS2        | Mega-TTS 2: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2307.07218v4)<br />[![Reading](https://img.shields.io/badge/Reading-yellow?style=for-the-badge)](https://papers.cool/arxiv/2307.07218v4) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/LSimon95/megatts2)![Star](https://img.shields.io/github/stars/LSimon95/megatts2.svg?style=social&label=Star) | 2023.07.14 |          |       |
| XTTS            | XTTS: a Massively Multilingual Zero-Shot Text-to-Speech Model | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2406.04904v1)<br />[![Reading](https://img.shields.io/badge/Reading-yellow?style=for-the-badge)](https://papers.cool/arxiv/2406.04904v1) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/Edresson/ZS-TTS-Evaluation)![Star](https://img.shields.io/github/stars/Edresson/ZS-TTS-Evaluation.svg?style=social&label=Star) | 2023.06.07 |          |       |
|                 |                                                              |                                                              |                                                              |            |          |       |
|                 |                                                              |                                                              |                                                              |            |          |       |
|                 |                                                              |                                                              |                                                              |            |          |       |
|                 |                                                              |                                                              |                                                              |            |          |       |

## TTS

| Model / Methods | Title                                                        | Paper Link                                                   | Code Link                                                    | Published  | Keywords | Venue |
| --------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ---------- | -------- | ----- |
| InstructTTS     | InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt | [![Paper](https://img.shields.io/badge/Paper-cyd7e6?style=for-the-badge)](https://arxiv.org/abs/2301.13662v2)<br />[![Reading](https://img.shields.io/badge/Reading-yellow?style=for-the-badge)](https://papers.cool/arxiv/2301.13662v2) | [![Code](https://img.shields.io/badge/Code-add7e6?style=for-the-badge)](https://github.com/yangdongchao/academicodec)![Star](https://img.shields.io/github/stars/yangdongchao/academicodec.svg?style=social&label=Star) | 2023.01.31 |          |       |



##  datasets

https://huggingface.co/datasets/ICTNLP/ComSpeech_Datasets

https://github.com/2noise/chattts

https://github.com/suno-ai/bark

https://github.com/openai/whisper
